{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 64  # Set your batch size\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  22\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data1['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  15\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data2['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  30\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data3['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique subjects:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "#print all of the unique subjects\n",
    "print(\"Unique subjects: \", data3['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert subject_id to int\n",
    "data3['subject_id'] = data3['subject_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # join all data in one dataframe row-wise\n",
    "# data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)\n",
    "data = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values in subject_id column\n",
    "data['subject_id'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subjects:  [ 9  7 26 29  1 24]\n",
      "Train data shape:  (897667, 5)\n",
      "Test data shape:  (225105, 5)\n"
     ]
    }
   ],
   "source": [
    "#split train and test data\n",
    "#randomly select 20% of subjects for test data\n",
    "test_subjects = data['subject_id'].unique()\n",
    "test_subjects = np.random.choice(test_subjects, size=int(0.2*len(test_subjects)), replace=False)\n",
    "# test_subjects = [ 9  7 26 29  1 24]\n",
    "print(\"Test subjects: \", test_subjects)\n",
    "\n",
    "#split data into train and test\n",
    "train_data = data[~data['subject_id'].isin(test_subjects)]\n",
    "test_data = data[data['subject_id'].isin(test_subjects)]\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z normalization with respect to train data\n",
    "train_data_mean = train_data[['acc_x', 'acc_y', 'acc_z']].mean()\n",
    "train_data_std = train_data[['acc_x', 'acc_y', 'acc_z']].std()\n",
    "# Normalize Training Data\n",
    "train_data.loc[:, ['acc_x', 'acc_y', 'acc_z']] = (train_data[['acc_x', 'acc_y', 'acc_z']] - train_data_mean) / train_data_std\n",
    "\n",
    "# Normalize Test Data with Training Statistics\n",
    "test_data.loc[:, ['acc_x', 'acc_y', 'acc_z']] = (test_data[['acc_x', 'acc_y', 'acc_z']] - train_data_mean) / train_data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train window dataset (8 sec with 50% overlap): (4487, 400, 5)\n",
      "shape of test window dataset (8 sec with 50% overlap): (1124, 400, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 8\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 50\n",
    "\n",
    "train_window_data, _ = sliding_window_samples(train_data, window_size, overlap_ratio)\n",
    "print(f\"shape of train window dataset ({time_window} sec with {overlap_ratio}% overlap): {train_window_data.shape}\")\n",
    "\n",
    "test_window_data, _ = sliding_window_samples(test_data, window_size, overlap_ratio)\n",
    "print(f\"shape of test window dataset ({time_window} sec with {overlap_ratio}% overlap): {test_window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, -0.9355155521152527, 0.12009837894797139, 2.223044603166194,\n",
       "        'null_class'],\n",
       "       [0, -0.9424832899580327, 0.13119524723936618, 2.200308651740837,\n",
       "        'null_class'],\n",
       "       [0, -0.9146124605540366, 0.1348942033364978, 2.188940676028158,\n",
       "        'null_class'],\n",
       "       ...,\n",
       "       [0, 0.433641884812917, -0.841630287242125, -0.17559328706204944,\n",
       "        'standing'],\n",
       "       [0, 0.42319052198299445, -0.849028215623565, -0.17559328706204944,\n",
       "        'standing'],\n",
       "       [0, 0.42319052198299445, -0.849028215623565, -0.17559328706204944,\n",
       "        'standing']], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Only the Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "train_window_data = train_window_data[:, :, :-1]\n",
    "# train_window_data = train_window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "train_window_data = train_window_data[:, :, 1:]\n",
    "\n",
    "test_window_data = test_window_data[:, :, :-1]\n",
    "test_window_data = test_window_data[:, :, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_window_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jitter(data, noise_factor=0.05):\n",
    "    jitter = noise_factor * np.random.randn(*data.shape)\n",
    "    return data + jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, min_scale=0.5, max_scale=1.5):\n",
    "    scaling_factor = np.random.uniform(min_scale, max_scale)\n",
    "    return data * scaling_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(data):\n",
    "    # Invert the sign of the data to simulate sensor rotation\n",
    "    return -data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_data(data):\n",
    "    return -data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(data):\n",
    "    # This function now correctly handles 2D data arrays\n",
    "    return data[::-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(data, num_segments=4):\n",
    "    segment_length = data.shape[0] // num_segments  # Adjusted to the first dimension for 2D data\n",
    "    permuted_indices = np.random.permutation(num_segments)\n",
    "    return np.concatenate(\n",
    "        [data[segment_length * idx:segment_length * (idx + 1), :] for idx in permuted_indices], axis=0)  # Concatenating along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "\n",
    "def time_warp(data, warp_factor_range=(0.8, 1.2)):\n",
    "    sequence_length, num_channels = data.shape\n",
    "    original_time_points = np.linspace(0, 1, sequence_length)\n",
    "    warp_factor = np.random.uniform(*warp_factor_range)\n",
    "    \n",
    "    # Generate new time points based on the warp factor\n",
    "    warped_time_points = np.linspace(0, warp_factor, sequence_length)\n",
    "\n",
    "    warped_data = np.zeros_like(data)\n",
    "    for j in range(num_channels):\n",
    "        # Interpolate each channel\n",
    "        interpolation = interp1d(original_time_points, data[:, j], \n",
    "                                 kind='linear', fill_value=\"extrapolate\")\n",
    "        warped_data[:, j] = interpolation(warped_time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_channels(data):\n",
    "    # Assuming data is 2D with shape (sequence_length, num_channels)\n",
    "    shuffled_indices = np.random.permutation(data.shape[1])  # Shuffle along the second dimension\n",
    "    return data[:, shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store datasets\n",
    "train_dataset = [[] for _ in range(8)]\n",
    "\n",
    "# Loop over all training data\n",
    "for data in train_window_data:\n",
    "    # loop over all transformations\n",
    "    # print(f\"shape of data: {data.shape}\")\n",
    "    data_array = np.array(data, dtype=np.float32)\n",
    "    for j in range(8):\n",
    "        # Original data with label 0\n",
    "        train_dataset[j].append((data_array, 0))\n",
    "        # Apply transformation based on j and save it in the transformed_data variable\n",
    "        if j == 0:\n",
    "            transformed_data = add_jitter(data_array)\n",
    "        elif j == 1:\n",
    "            transformed_data = scale_data(data_array)\n",
    "        elif j == 2:\n",
    "            transformed_data = rotate_data(data_array)\n",
    "        elif j == 3:\n",
    "            transformed_data = negate_data(data_array)\n",
    "        elif j == 4:\n",
    "            transformed_data = horizontal_flip(data_array)\n",
    "        elif j == 5:\n",
    "            transformed_data = permute_data(data_array)\n",
    "        elif j == 6:\n",
    "            transformed_data = time_warp(data_array)\n",
    "        elif j == 7:\n",
    "            transformed_data = shuffle_channels(data_array)\n",
    "        # Append the transformed data with label 1\n",
    "        transformed_data_array = np.array(transformed_data, dtype=np.float32)\n",
    "        train_dataset[j].append((transformed_data_array, 1))\n",
    "\n",
    "for dataset in train_dataset:\n",
    "    shapes = set(tuple(d.shape) for d, _ in dataset)\n",
    "    if len(shapes) > 1:\n",
    "        print(\"Inconsistent shapes found in dataset:\", shapes)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for j in range(8):\n",
    "    data, labels = zip(*train_dataset[j])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    train_dataset[j] = (data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9355155 ,  0.12009838,  2.2230446 ],\n",
       "        [-0.9424833 ,  0.13119525,  2.2003086 ],\n",
       "        [-0.9146125 ,  0.1348942 ,  2.1889408 ],\n",
       "        ...,\n",
       "        [ 0.43364188, -0.8416303 , -0.17559329],\n",
       "        [ 0.42319053, -0.8490282 , -0.17559329],\n",
       "        [ 0.42319053, -0.8490282 , -0.17559329]],\n",
       "\n",
       "       [[-0.6096269 ,  0.07826187,  1.4486427 ],\n",
       "        [-0.6141674 ,  0.08549313,  1.4338268 ],\n",
       "        [-0.59600544,  0.08790354,  1.426419  ],\n",
       "        ...,\n",
       "        [ 0.2825819 , -0.5484467 , -0.11442502],\n",
       "        [ 0.2757713 , -0.5532676 , -0.11442502],\n",
       "        [ 0.2757713 , -0.5532676 , -0.11442502]],\n",
       "\n",
       "       [[ 0.4440935 , -0.81943655, -0.14906807],\n",
       "        [ 0.510287  , -0.8453292 , -0.06949241],\n",
       "        [ 0.68796384, -0.83793133, -0.11496421],\n",
       "        ...,\n",
       "        [ 0.44757736, -0.8490282 , -0.17559329],\n",
       "        [ 0.43712574, -0.83793133, -0.18696123],\n",
       "        [ 0.4266744 , -0.8416303 , -0.17559329]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.4319051 , -0.37680075, -0.05276383],\n",
       "        [ 0.4319051 , -0.44930378, -0.08828625],\n",
       "        [ 0.43487406, -0.4682176 , -0.07536901],\n",
       "        ...,\n",
       "        [ 0.642704  , -0.42408532, -0.16578974],\n",
       "        [ 0.69614583, -0.58170056, -0.38215363],\n",
       "        [ 0.41409105, -0.43669453, -0.36923638]],\n",
       "\n",
       "       [[ 0.4893838 , -0.59010124,  0.0214512 ],\n",
       "        [ 0.4893838 , -0.59010124,  0.0214512 ],\n",
       "        [ 0.5486093 , -0.5568106 ,  0.06692302],\n",
       "        ...,\n",
       "        [-1.1689343 ,  0.39382115,  2.3101988 ],\n",
       "        [-1.3222243 ,  0.34573472,  2.1396794 ],\n",
       "        [-1.4093207 ,  0.4567034 ,  1.8630594 ]],\n",
       "\n",
       "       [[ 0.43365556, -0.5229039 ,  0.01900846],\n",
       "        [ 0.43365556, -0.5229039 ,  0.01900846],\n",
       "        [ 0.48613682, -0.4934042 ,  0.05930221],\n",
       "        ...,\n",
       "        [-1.0358229 ,  0.34897506,  2.0471265 ],\n",
       "        [-1.171657  ,  0.30636442,  1.8960251 ],\n",
       "        [-1.2488353 ,  0.40469664,  1.6509049 ]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training dataset 0: (8974, 400, 3)\n",
      "shape of training dataset 1: (8974, 400, 3)\n",
      "shape of training dataset 2: (8974, 400, 3)\n",
      "shape of training dataset 3: (8974, 400, 3)\n",
      "shape of training dataset 4: (8974, 400, 3)\n",
      "shape of training dataset 5: (8974, 400, 3)\n",
      "shape of training dataset 6: (8974, 400, 3)\n",
      "shape of training dataset 7: (8974, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of all training datasets\n",
    "for j in range(8):\n",
    "    print(f\"shape of training dataset {j}: {train_dataset[j][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of training dataset 0: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 1: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 2: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 3: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 4: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 5: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 6: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 7: (array([0, 1]), array([4487, 4487], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# print the class distribution of all training datasets\n",
    "for j in range(8):\n",
    "    print(f\"Class distribution of training dataset {j}: {np.unique(train_dataset[j][1], return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to store datasets for test data\n",
    "test_dataset = [[] for _ in range(8)]\n",
    "\n",
    "# loop over all test data\n",
    "for data in test_window_data:\n",
    "    data_array = np.array(data, dtype=np.float32)\n",
    "    # loop over all transformations\n",
    "    for j in range(8):\n",
    "        # Original data with label 0\n",
    "        test_dataset[j].append((data_array, 0))\n",
    "        # Apply transformation based on j and save it in the transformed_data variable\n",
    "        if j == 0:\n",
    "            transformed_data = add_jitter(data_array)\n",
    "        elif j == 1:\n",
    "            transformed_data = scale_data(data_array)\n",
    "        elif j == 2:\n",
    "            transformed_data = rotate_data(data_array)\n",
    "        elif j == 3:\n",
    "            transformed_data = negate_data(data_array)\n",
    "        elif j == 4:\n",
    "            transformed_data = horizontal_flip(data_array)\n",
    "        elif j == 5:\n",
    "            transformed_data = permute_data(data_array)\n",
    "        elif j == 6:\n",
    "            transformed_data = time_warp(data_array)\n",
    "        elif j == 7:\n",
    "            transformed_data = shuffle_channels(data_array)\n",
    "        # Append the transformed data with label 1\n",
    "        transformed_data_array = np.array(transformed_data, dtype=np.float32)\n",
    "        test_dataset[j].append((transformed_data_array, 1))\n",
    "\n",
    "# check for inconsistent shapes\n",
    "for dataset in test_dataset:\n",
    "    shapes = set(tuple(d.shape) for d, _ in dataset)\n",
    "    if len(shapes) > 1:\n",
    "        print(\"Inconsistent shapes found in dataset:\", shapes)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for j in range(8):\n",
    "    data, labels = zip(*test_dataset[j])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    test_dataset[j] = (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test dataset 0: (2248, 400, 3)\n",
      "shape of test dataset 1: (2248, 400, 3)\n",
      "shape of test dataset 2: (2248, 400, 3)\n",
      "shape of test dataset 3: (2248, 400, 3)\n",
      "shape of test dataset 4: (2248, 400, 3)\n",
      "shape of test dataset 5: (2248, 400, 3)\n",
      "shape of test dataset 6: (2248, 400, 3)\n",
      "shape of test dataset 7: (2248, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of all test datasets\n",
    "for j in range(8):\n",
    "    print(f\"shape of test dataset {j}: {test_dataset[j][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of test dataset 0: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 1: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 2: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 3: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 4: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 5: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 6: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 7: (array([0, 1]), array([1124, 1124], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# print the class distribution of all test datasets\n",
    "for j in range(8):\n",
    "    print(f\"Class distribution of test dataset {j}: {np.unique(test_dataset[j][1], return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # Convert data and labels to PyTorch tensors\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Assuming labels are integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Initialize DataLoader for each dataset\n",
    "train_loaders = []\n",
    "\n",
    "for j in range(8):\n",
    "    # Assuming train_dataset[j][0] is the data and train_dataset[j][1] are the labels\n",
    "    data, labels = train_dataset[j]\n",
    "    transformed_dataset = CustomDataset(data, labels)\n",
    "    train_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loaders.append(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test loaders\n",
    "\n",
    "test_loaders = []\n",
    "for j in range(8):\n",
    "    test_data, test_labels = test_dataset[j]\n",
    "    test_transformed_dataset = CustomDataset(test_data, test_labels)\n",
    "    test_loader = DataLoader(test_transformed_dataset, batch_size=batch_size, shuffle=False)  # Usually, we don't shuffle test data\n",
    "    test_loaders.append(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# need to reshape and transpose the data to fit the input shape of the model\n",
    "\n",
    "class TPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TPN, self).__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(96, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(8)  # 8 heads for 8 different transformations\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.trunk(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully-connected layer\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
