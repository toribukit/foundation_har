{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import copy\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.stats import mode\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32  # Set your batch size\n",
    "learning_rate_client = 0.001\n",
    "local_epochs = 1\n",
    "subject_dir = 'FL_Data/windowed_data_refused/subject_'  # Set your directory to the subject data\n",
    "numclients = 54\n",
    "num_classes = 9\n",
    "\n",
    "#current timestamp\n",
    "current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_client(id, batch_size=batch_size, type='labelled_train'):\n",
    "    # Load the data\n",
    "    data = np.load(subject_dir + str(id) + '/windowed_' + type + '_x.npy')\n",
    "    labels = np.load(subject_dir + str(id) + '/windowed_' + type + '_y.npy')\n",
    "\n",
    "    # print shape of data\n",
    "    # print(data.shape)\n",
    "    # print(labels.shape)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    data = torch.from_numpy(data).float()\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "\n",
    "    # Create a dataloader\n",
    "    if type == 'labelled_train' or type == 'unlabelled_train':\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(54):\n",
    "#     data_label_train = load_data_client(i, batch_size, 'labelled_train')\n",
    "#     data_unlabel_train = load_data_client(i, batch_size, 'unlabelled_train')\n",
    "#     data_test = load_data_client(i, batch_size, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_autoencoder(model, train_loader, device, learning_rate=0.01, epochs=5):\n",
    "#     model.to(device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for epoch in range(epochs):\n",
    "#         for data, target in train_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             # print(data.shape)\n",
    "#             data = data.permute(0, 2, 1)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             # print(output.shape)\n",
    "#             loss = criterion(output, data)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         epoch_loss = total_loss / len(train_loader)\n",
    "#         # print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "#         total_loss = 0  # Reset total loss for the next epoch\n",
    "\n",
    "#     results = {\n",
    "#         'train_loss': epoch_loss\n",
    "#     }\n",
    "    \n",
    "#     return results  # Returns the average loss of the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_autoencoder(model, test_loader, device):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             data = data.permute(0, 2, 1)\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, data)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(test_loader)\n",
    "#     # print(f'Test Loss: {avg_loss}')\n",
    "    \n",
    "#     return avg_loss  # Returns the average loss for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train_model(model, train_loader, device, learning_rate=0.001, epochs=1):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        total_loss = 0  # Reset total loss for the next epoch\n",
    "\n",
    "    results = {\n",
    "        'train_loss': epoch_loss\n",
    "    }\n",
    "    \n",
    "    return results  # Returns the average loss of the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test the model\n",
    "# method to test the model and get the accuracy and f1 score\n",
    "def test_model(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # print(f'Accuracy: {accuracy}, F1 Score: {f1}')\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "  def __init__(self, client_config:dict):\n",
    "    # client config as dict to make configuration dynamic\n",
    "    self.id = client_config[\"id\"]\n",
    "    self.config = client_config\n",
    "    self.__model = None\n",
    "\n",
    "    self.labelled_loader = self.config[\"labelled\"]\n",
    "    self.unlabelled_loader = self.config[\"unlabelled\"]\n",
    "    self.test_loader = self.config[\"test\"]\n",
    "\n",
    "  @property\n",
    "  def model(self):\n",
    "    return self.__model\n",
    "\n",
    "  @model.setter\n",
    "  def model(self, model):\n",
    "    self.__model = model\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"Return a total size of the client's local data.\"\"\"\n",
    "    return len(self.unlabelled_loader.sampler)\n",
    "\n",
    "  def train_ssl(self):\n",
    "    results = train_model(model = self.model,\n",
    "                    train_loader = self.unlabelled_loader,\n",
    "                    device=device,\n",
    "                    learning_rate=learning_rate_client,\n",
    "                    epochs=local_epochs)\n",
    "    print(f\"Train result client {self.id}: {results}\")\n",
    "\n",
    "  def test_ssl(self):\n",
    "    loss = test_model(model = self.model,\n",
    "                    test_loader = self.unlabelled_loader)\n",
    "    print(f\"Test result client {self.id}: {loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg():\n",
    "  def __init__(self):\n",
    "    self.globalmodel = TCN(num_inputs, num_channels, kernel_size, num_classes=4).to(device)\n",
    "    self.rounds = 0\n",
    "    self.params = {}\n",
    "\n",
    "  def aggregate(self, round):\n",
    "    #v1:update the aggregate to save the model with round and date indicator\n",
    "    modelparams = []\n",
    "    for i in self.params.keys():\n",
    "      modelparams.append(self.params[i])\n",
    "\n",
    "    avg_weights = {}\n",
    "    for name in modelparams[0].keys():\n",
    "      avg_weights[name] = torch.mean(torch.stack([w[name] for w in modelparams]), dim = 0)\n",
    "\n",
    "    self.globalmodel.load_state_dict(avg_weights)\n",
    "\n",
    "    #save the model\n",
    "    name_path = f'Refused_FL/Model_Global_TCN/{current_time}'\n",
    "    if not os.path.exists(name_path):\n",
    "      os.makedirs(name_path)\n",
    "\n",
    "    torch.save(self.globalmodel.state_dict(), f\"{name_path}/global_model_round_{round}.pth\")\n",
    "    \n",
    "    # filename = f\"{path_glob_m}/global_model_round_{round}_{current_time}.pth\"\n",
    "    # torch.save(self.globalmodel.state_dict(), filename)\n",
    "\n",
    "  def clientstrain(self, clientconfig):\n",
    "    clients = clientconfig\n",
    "    for i in clients.keys():\n",
    "      test_client = Client(clients[i])\n",
    "      test_client.model = copy.deepcopy(self.globalmodel)\n",
    "      test_client.train_ssl()\n",
    "      test_client.test_ssl()\n",
    "      self.params[i] = test_client.model.state_dict()\n",
    "\n",
    "  def initiate_FL(self, clientconfig, serverdata):\n",
    "    clients = clientconfig\n",
    "    print(\"Round: {}\".format(self.rounds))\n",
    "\n",
    "    print(\"Obtaining Weights!!\")\n",
    "    self.clientstrain(clients)\n",
    "\n",
    "    #### Aggregate model\n",
    "    print(\"Aggregating Model!!\")\n",
    "    self.aggregate(self.rounds)\n",
    "\n",
    "    #### Replace parameters with global model parameters\n",
    "    for i in self.params.keys():\n",
    "        self.params[i] = self.globalmodel.state_dict()\n",
    "\n",
    "\n",
    "    servertest = serverdata\n",
    "    result = test_model(model = self.globalmodel,\n",
    "                    test_loader = servertest)\n",
    "    print(\"Round {} metrics:\".format(self.rounds))\n",
    "    print(\"Server Result = {}\".format(result))\n",
    "    print(\"Round {} finished!\".format(self.rounds))\n",
    "    self.rounds += 1\n",
    "    return clients, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client: 0\n",
      "labelled: 12\n",
      "unlabelled: 45\n",
      "test: 14\n",
      "client: 1\n",
      "labelled: 11\n",
      "unlabelled: 43\n",
      "test: 14\n",
      "client: 2\n",
      "labelled: 12\n",
      "unlabelled: 45\n",
      "test: 15\n",
      "client: 3\n",
      "labelled: 10\n",
      "unlabelled: 41\n",
      "test: 13\n",
      "client: 4\n",
      "labelled: 13\n",
      "unlabelled: 51\n",
      "test: 16\n",
      "client: 5\n",
      "labelled: 12\n",
      "unlabelled: 49\n",
      "test: 16\n",
      "client: 6\n",
      "labelled: 3\n",
      "unlabelled: 12\n",
      "test: 4\n",
      "client: 7\n",
      "labelled: 4\n",
      "unlabelled: 13\n",
      "test: 4\n",
      "client: 8\n",
      "labelled: 2\n",
      "unlabelled: 8\n",
      "test: 3\n",
      "client: 9\n",
      "labelled: 11\n",
      "unlabelled: 43\n",
      "test: 14\n",
      "client: 10\n",
      "labelled: 9\n",
      "unlabelled: 34\n",
      "test: 11\n",
      "client: 11\n",
      "labelled: 12\n",
      "unlabelled: 46\n",
      "test: 15\n",
      "client: 12\n",
      "labelled: 11\n",
      "unlabelled: 42\n",
      "test: 13\n",
      "client: 13\n",
      "labelled: 13\n",
      "unlabelled: 50\n",
      "test: 16\n",
      "client: 14\n",
      "labelled: 11\n",
      "unlabelled: 44\n",
      "test: 14\n",
      "client: 15\n",
      "labelled: 10\n",
      "unlabelled: 38\n",
      "test: 12\n",
      "client: 16\n",
      "labelled: 13\n",
      "unlabelled: 49\n",
      "test: 16\n",
      "client: 17\n",
      "labelled: 12\n",
      "unlabelled: 45\n",
      "test: 14\n",
      "client: 18\n",
      "labelled: 11\n",
      "unlabelled: 43\n",
      "test: 14\n",
      "client: 19\n",
      "labelled: 11\n",
      "unlabelled: 45\n",
      "test: 14\n",
      "client: 20\n",
      "labelled: 11\n",
      "unlabelled: 43\n",
      "test: 14\n",
      "client: 21\n",
      "labelled: 11\n",
      "unlabelled: 44\n",
      "test: 14\n",
      "client: 22\n",
      "labelled: 9\n",
      "unlabelled: 36\n",
      "test: 12\n",
      "client: 23\n",
      "labelled: 12\n",
      "unlabelled: 45\n",
      "test: 14\n",
      "client: 24\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 25\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 26\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 27\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 28\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 29\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 30\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 31\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 32\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 33\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 34\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 35\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 36\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 37\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 38\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 39\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 40\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 41\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 42\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 43\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 44\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 45\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 46\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 47\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 48\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 49\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 50\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 51\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n",
      "client: 52\n",
      "labelled: 2\n",
      "unlabelled: 5\n",
      "test: 2\n",
      "client: 53\n",
      "labelled: 2\n",
      "unlabelled: 6\n",
      "test: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined test: 341\n",
      "combined unlabelled: 1092\n"
     ]
    }
   ],
   "source": [
    "clients = {}\n",
    "\n",
    "for i in range(numclients):\n",
    "    clients[i] = {\"id\": i, \"batch_size\": batch_size, \"local_epoch\": 1}\n",
    "    clients[i]['labelled'] = load_data_client(i, batch_size, 'labelled_train')\n",
    "    clients[i]['unlabelled'] = load_data_client(i, batch_size, 'unlabelled_train')\n",
    "    clients[i]['test'] = load_data_client(i, batch_size, 'test')\n",
    "\n",
    "    print(f\"client: {i}\")\n",
    "    print(f\"labelled: {len(clients[i]['labelled'])}\")\n",
    "    print(f\"unlabelled: {len(clients[i]['unlabelled'])}\")\n",
    "    print(f\"test: {len(clients[i]['test'])}\")\n",
    "\n",
    "# combine all client test data into one\n",
    "combined_test_data = []\n",
    "combined_test_labels = []\n",
    "for i in range(numclients):\n",
    "    for data, labels in clients[i]['test']:\n",
    "        combined_test_data.append(data)\n",
    "        combined_test_labels.append(labels)\n",
    "combined_test_data = torch.cat(combined_test_data, dim=0)\n",
    "combined_test_labels = torch.cat(combined_test_labels, dim=0)\n",
    "# create dataset and dataloader\n",
    "combined_test_dataset = torch.utils.data.TensorDataset(combined_test_data, combined_test_labels)\n",
    "combined_test_dataloader = torch.utils.data.DataLoader(combined_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"combined test: {len(combined_test_dataloader)}\")\n",
    "\n",
    "# combine all client unlabelled data into one\n",
    "combined_unlabelled_data = []\n",
    "combined_unlabelled_labels = []\n",
    "for i in range(numclients):\n",
    "    for data, labels in clients[i]['unlabelled']:\n",
    "        combined_unlabelled_data.append(data)\n",
    "        combined_unlabelled_labels.append(labels)\n",
    "combined_unlabelled_data = torch.cat(combined_unlabelled_data, dim=0)\n",
    "combined_unlabelled_labels = torch.cat(combined_unlabelled_labels, dim=0)\n",
    "# create dataset and dataloader\n",
    "combined_unlabelled_dataset = torch.utils.data.TensorDataset(combined_unlabelled_data, combined_unlabelled_labels)\n",
    "combined_unlabelled_dataloader = torch.utils.data.DataLoader(combined_unlabelled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"combined unlabelled: {len(combined_unlabelled_dataloader)}\")\n",
    "\n",
    "# server test_data\n",
    "# server_test_data = combined_unlabelled_dataloader\n",
    "\n",
    "# server = FedAvg()\n",
    "\n",
    "# loss_rounds = []\n",
    "# for i in range(num_epochs):\n",
    "#     clients, loss = server.initiate_FL(clientconfig=clients, serverdata=server_test_data)\n",
    "#     loss_rounds.append(loss)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"-\" * 50)\n",
    "# print(\"Loss values all rounds: \", loss_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined labelled: 273\n"
     ]
    }
   ],
   "source": [
    "# combine all client labelled data into one\n",
    "combined_labelled_data = []\n",
    "combined_labelled_labels = []\n",
    "for i in range(numclients):\n",
    "    for data, labels in clients[i]['labelled']:\n",
    "        combined_labelled_data.append(data)\n",
    "        combined_labelled_labels.append(labels)\n",
    "combined_labelled_data = torch.cat(combined_labelled_data, dim=0)\n",
    "combined_labelled_labels = torch.cat(combined_labelled_labels, dim=0)\n",
    "# create dataset and dataloader\n",
    "combined_labelled_dataset = torch.utils.data.TensorDataset(combined_labelled_data, combined_labelled_labels)\n",
    "combined_labelled_dataloader = torch.utils.data.DataLoader(combined_labelled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"combined labelled: {len(combined_labelled_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each class\n",
    "class_counts = torch.zeros(num_classes)  # num_classes should be defined based on your dataset\n",
    "for _, target in combined_labelled_dataloader:\n",
    "    class_counts += torch.bincount(target, minlength=num_classes)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts += 1  # Add 1 to each class count to avoid division by zero\n",
    "c_weight = 1. / class_counts\n",
    "c_weight = c_weight / c_weight.sum() * num_classes\n",
    "c_weight = c_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0120e+03, 1.2410e+03, 1.3840e+03, 1.5050e+03, 8.8300e+02, 3.7800e+02,\n",
       "        1.0000e+00, 9.6700e+02, 1.3440e+03])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.8163e-03, 7.1894e-03, 6.4466e-03, 5.9283e-03, 1.0104e-02, 2.3603e-02,\n",
       "        8.9220e+00, 9.2265e-03, 6.6384e-03], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCN(\n",
       "  (tcn): Sequential(\n",
       "    (0): TemporalBlock(\n",
       "      (conv1): Conv1d(3, 64, kernel_size=(8,), stride=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(8,), stride=(1,))\n",
       "      (relu2): ReLU()\n",
       "      (downsample): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): TemporalBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(8,), stride=(1,), dilation=(2,))\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(8,), stride=(1,), dilation=(2,))\n",
       "      (relu2): ReLU()\n",
       "      (downsample): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): TemporalBlock(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(8,), stride=(1,), dilation=(4,))\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(8,), stride=(1,), dilation=(4,))\n",
       "      (relu2): ReLU()\n",
       "      (downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_path = 'Refused_FL/Model_Global_TCN/2024-02-12_00-55-24/global_model_round_199.pth' # model with 200 epochs, 1 local epoch\n",
    "model = TCN(num_inputs, num_channels, kernel_size, num_classes=4)\n",
    "\n",
    "#load pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=num_classes)\n",
    "\n",
    "# # Freezing all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze layers from the last TemporalBlock's conv2 onwards\n",
    "# num_levels = len(model.tcn)  # Number of TemporalBlocks in your TCN\n",
    "# for i, block in enumerate(model.tcn):\n",
    "#     if i == num_levels - 1:  # Check if it's the last TemporalBlock\n",
    "#         # Unfreeze the conv2 layer and any subsequent layers within this block\n",
    "#         unfreeze = False\n",
    "#         for name, param in block.named_parameters():\n",
    "#             if 'conv2' in name:\n",
    "#                 unfreeze = True\n",
    "#             if unfreeze:\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the classification layer\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TCN(num_inputs, num_channels, kernel_size, num_classes=num_classes)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to test the model and get the accuracy and f1 score\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f'Accuracy: {accuracy}, F1 Score: {f1}')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, test_loader, num_epochs=200):\n",
    "    # Assuming class weights are calculated and provided as `class_weights`\n",
    "    class_weights = torch.tensor(c_weight).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc, f1 = test_model(model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Accuracy: {acc}, F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IME-LAB\\AppData\\Local\\Temp\\ipykernel_3396\\2508305535.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_weights = torch.tensor(c_weight).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23849756635136377, F1 Score: 0.19411374901500106\n",
      "Epoch 1/100, Loss: 2.5419280529022217, Accuracy: 0.23849756635136377, F1 Score: 0.19411374901500106\n",
      "Accuracy: 0.25089539902654057, F1 Score: 0.20385712118891358\n",
      "Epoch 2/100, Loss: 1.50894033908844, Accuracy: 0.25089539902654057, F1 Score: 0.20385712118891358\n",
      "Accuracy: 0.267334006795849, F1 Score: 0.20880919424037359\n",
      "Epoch 3/100, Loss: 1.8318653106689453, Accuracy: 0.267334006795849, F1 Score: 0.20880919424037359\n",
      "Accuracy: 0.2706400955092295, F1 Score: 0.2234770316885124\n",
      "Epoch 4/100, Loss: 2.4398069381713867, Accuracy: 0.2706400955092295, F1 Score: 0.2234770316885124\n",
      "Accuracy: 0.26880337955735145, F1 Score: 0.21370415639447118\n",
      "Epoch 5/100, Loss: 1.5360980033874512, Accuracy: 0.26880337955735145, F1 Score: 0.21370415639447118\n",
      "Accuracy: 0.2775277803287722, F1 Score: 0.2207597285233303\n",
      "Epoch 6/100, Loss: 1.9258979558944702, Accuracy: 0.2775277803287722, F1 Score: 0.2207597285233303\n",
      "Accuracy: 0.2609055009642759, F1 Score: 0.23011791036541177\n",
      "Epoch 7/100, Loss: 2.3425207138061523, Accuracy: 0.2609055009642759, F1 Score: 0.23011791036541177\n",
      "Accuracy: 0.27679309394802093, F1 Score: 0.2360461163479452\n",
      "Epoch 8/100, Loss: 1.9896982908248901, Accuracy: 0.27679309394802093, F1 Score: 0.2360461163479452\n",
      "Accuracy: 0.28276242079162456, F1 Score: 0.24580805933726685\n",
      "Epoch 9/100, Loss: 2.137963056564331, Accuracy: 0.28276242079162456, F1 Score: 0.24580805933726685\n",
      "Accuracy: 0.2838644503627514, F1 Score: 0.25544804711268226\n",
      "Epoch 10/100, Loss: 1.8552546501159668, Accuracy: 0.2838644503627514, F1 Score: 0.25544804711268226\n",
      "Accuracy: 0.28230324180365507, F1 Score: 0.2538073895733039\n",
      "Epoch 11/100, Loss: 1.2131078243255615, Accuracy: 0.28230324180365507, F1 Score: 0.2538073895733039\n",
      "Accuracy: 0.2869868674809441, F1 Score: 0.2554825475046701\n",
      "Epoch 12/100, Loss: 1.6627423763275146, Accuracy: 0.2869868674809441, F1 Score: 0.2554825475046701\n",
      "Accuracy: 0.2856093305170355, F1 Score: 0.2529616542829064\n",
      "Epoch 13/100, Loss: 2.073185920715332, Accuracy: 0.2856093305170355, F1 Score: 0.2529616542829064\n",
      "Accuracy: 0.2912113141702636, F1 Score: 0.25642003796723467\n",
      "Epoch 14/100, Loss: 1.4422847032546997, Accuracy: 0.2912113141702636, F1 Score: 0.25642003796723467\n",
      "Accuracy: 0.28634401689778677, F1 Score: 0.2664230408380374\n",
      "Epoch 15/100, Loss: 1.9349110126495361, Accuracy: 0.28634401689778677, F1 Score: 0.2664230408380374\n",
      "Accuracy: 0.2853338231242538, F1 Score: 0.2639224658350889\n",
      "Epoch 16/100, Loss: 1.8144311904907227, Accuracy: 0.2853338231242538, F1 Score: 0.2639224658350889\n",
      "Accuracy: 0.2875378822665075, F1 Score: 0.26665406190455543\n",
      "Epoch 17/100, Loss: 1.450976014137268, Accuracy: 0.2875378822665075, F1 Score: 0.26665406190455543\n",
      "Accuracy: 0.28616034530259893, F1 Score: 0.24908494832665012\n",
      "Epoch 18/100, Loss: 1.5871778726577759, Accuracy: 0.28616034530259893, F1 Score: 0.24908494832665012\n",
      "Accuracy: 0.2848746441362843, F1 Score: 0.25168877814604684\n",
      "Epoch 19/100, Loss: 1.8445303440093994, Accuracy: 0.2848746441362843, F1 Score: 0.25168877814604684\n",
      "Accuracy: 0.2922215079437965, F1 Score: 0.25549219734749073\n",
      "Epoch 20/100, Loss: 2.513685464859009, Accuracy: 0.2922215079437965, F1 Score: 0.25549219734749073\n",
      "Accuracy: 0.29488474607401965, F1 Score: 0.2660128673903377\n",
      "Epoch 21/100, Loss: 2.0928077697753906, Accuracy: 0.29488474607401965, F1 Score: 0.2660128673903377\n",
      "Accuracy: 0.292680686931766, F1 Score: 0.2509271172117538\n",
      "Epoch 22/100, Loss: 1.6304564476013184, Accuracy: 0.292680686931766, F1 Score: 0.2509271172117538\n",
      "Accuracy: 0.2845991367435026, F1 Score: 0.260055091127706\n",
      "Epoch 23/100, Loss: 2.05668306350708, Accuracy: 0.2845991367435026, F1 Score: 0.260055091127706\n",
      "Accuracy: 0.2866195242905685, F1 Score: 0.24786121424769292\n",
      "Epoch 24/100, Loss: 1.5672686100006104, Accuracy: 0.2866195242905685, F1 Score: 0.24786121424769292\n",
      "Accuracy: 0.28625218110019285, F1 Score: 0.26065973737165526\n",
      "Epoch 25/100, Loss: 1.6323801279067993, Accuracy: 0.28625218110019285, F1 Score: 0.26065973737165526\n",
      "Accuracy: 0.2949765818716136, F1 Score: 0.24913100195719123\n",
      "Epoch 26/100, Loss: 1.3073762655258179, Accuracy: 0.2949765818716136, F1 Score: 0.24913100195719123\n",
      "Accuracy: 0.2942418954908623, F1 Score: 0.267402548322374\n",
      "Epoch 27/100, Loss: 1.2464599609375, Accuracy: 0.2942418954908623, F1 Score: 0.267402548322374\n",
      "Accuracy: 0.2952520892643953, F1 Score: 0.2671989855620478\n",
      "Epoch 28/100, Loss: 2.3911333084106445, Accuracy: 0.2952520892643953, F1 Score: 0.2671989855620478\n",
      "Accuracy: 0.29038479199191847, F1 Score: 0.25603047254991546\n",
      "Epoch 29/100, Loss: 1.8341965675354004, Accuracy: 0.29038479199191847, F1 Score: 0.25603047254991546\n",
      "Accuracy: 0.2924051795389843, F1 Score: 0.2750239603969932\n",
      "Epoch 30/100, Loss: 1.737985372543335, Accuracy: 0.2924051795389843, F1 Score: 0.2750239603969932\n",
      "Accuracy: 0.2927725227293599, F1 Score: 0.2625289400585954\n",
      "Epoch 31/100, Loss: 1.7960623502731323, Accuracy: 0.2927725227293599, F1 Score: 0.2625289400585954\n",
      "Accuracy: 0.293599044907705, F1 Score: 0.26064085611079146\n",
      "Epoch 32/100, Loss: 1.4823909997940063, Accuracy: 0.293599044907705, F1 Score: 0.26064085611079146\n",
      "Accuracy: 0.296445954633116, F1 Score: 0.2703703205069595\n",
      "Epoch 33/100, Loss: 1.8062961101531982, Accuracy: 0.296445954633116, F1 Score: 0.2703703205069595\n",
      "Accuracy: 0.29727247681146113, F1 Score: 0.2684472244889457\n",
      "Epoch 34/100, Loss: 1.6072993278503418, Accuracy: 0.29727247681146113, F1 Score: 0.2684472244889457\n",
      "Accuracy: 0.29488474607401965, F1 Score: 0.26313159576013995\n",
      "Epoch 35/100, Loss: 2.0446975231170654, Accuracy: 0.29488474607401965, F1 Score: 0.26313159576013995\n",
      "Accuracy: 0.29571126825236477, F1 Score: 0.2574772812953129\n",
      "Epoch 36/100, Loss: 1.4160963296890259, Accuracy: 0.29571126825236477, F1 Score: 0.2574772812953129\n",
      "Accuracy: 0.2924970153365782, F1 Score: 0.2705460102182931\n",
      "Epoch 37/100, Loss: 1.898535132408142, Accuracy: 0.2924970153365782, F1 Score: 0.2705460102182931\n",
      "Accuracy: 0.2967214620258977, F1 Score: 0.27923870768519776\n",
      "Epoch 38/100, Loss: 1.9169882535934448, Accuracy: 0.2967214620258977, F1 Score: 0.27923870768519776\n",
      "Accuracy: 0.29782349159702454, F1 Score: 0.27030524702653724\n",
      "Epoch 39/100, Loss: 1.801263451576233, Accuracy: 0.29782349159702454, F1 Score: 0.27030524702653724\n",
      "Accuracy: 0.30186426669115624, F1 Score: 0.2637197361973724\n",
      "Epoch 40/100, Loss: 1.3531620502471924, Accuracy: 0.30186426669115624, F1 Score: 0.2637197361973724\n",
      "Accuracy: 0.29809899898980624, F1 Score: 0.28304065454764443\n",
      "Epoch 41/100, Loss: 2.0867576599121094, Accuracy: 0.29809899898980624, F1 Score: 0.28304065454764443\n",
      "Accuracy: 0.2998438791440904, F1 Score: 0.277917381693039\n",
      "Epoch 42/100, Loss: 2.6739325523376465, Accuracy: 0.2998438791440904, F1 Score: 0.277917381693039\n",
      "Accuracy: 0.29910919276333914, F1 Score: 0.27799207950234606\n",
      "Epoch 43/100, Loss: 2.0421571731567383, Accuracy: 0.29910919276333914, F1 Score: 0.27799207950234606\n",
      "Accuracy: 0.29754798420424283, F1 Score: 0.2664542192488671\n",
      "Epoch 44/100, Loss: 1.8695123195648193, Accuracy: 0.29754798420424283, F1 Score: 0.2664542192488671\n",
      "Accuracy: 0.30048672972724766, F1 Score: 0.2827635874120939\n",
      "Epoch 45/100, Loss: 1.8790946006774902, Accuracy: 0.30048672972724766, F1 Score: 0.2827635874120939\n",
      "Accuracy: 0.3001193865368721, F1 Score: 0.2783359742987299\n",
      "Epoch 46/100, Loss: 1.6069878339767456, Accuracy: 0.3001193865368721, F1 Score: 0.2783359742987299\n",
      "Accuracy: 0.29910919276333914, F1 Score: 0.27474982053282143\n",
      "Epoch 47/100, Loss: 1.4462110996246338, Accuracy: 0.29910919276333914, F1 Score: 0.27474982053282143\n",
      "Accuracy: 0.297364312609055, F1 Score: 0.27075634362216416\n",
      "Epoch 48/100, Loss: 1.6202964782714844, Accuracy: 0.297364312609055, F1 Score: 0.27075634362216416\n",
      "Accuracy: 0.2977316557994306, F1 Score: 0.2779837627222074\n",
      "Epoch 49/100, Loss: 1.778970718383789, Accuracy: 0.2977316557994306, F1 Score: 0.2779837627222074\n",
      "Accuracy: 0.2963541188355221, F1 Score: 0.2675668231872958\n",
      "Epoch 50/100, Loss: 1.6161367893218994, Accuracy: 0.2963541188355221, F1 Score: 0.2675668231872958\n",
      "Accuracy: 0.299292864358527, F1 Score: 0.27140391460451135\n",
      "Epoch 51/100, Loss: 1.348989725112915, Accuracy: 0.299292864358527, F1 Score: 0.27140391460451135\n",
      "Accuracy: 0.2984663421801818, F1 Score: 0.2778983505876601\n",
      "Epoch 52/100, Loss: 2.172525644302368, Accuracy: 0.2984663421801818, F1 Score: 0.2778983505876601\n",
      "Accuracy: 0.29470107447883187, F1 Score: 0.27673776228441244\n",
      "Epoch 53/100, Loss: 1.989017128944397, Accuracy: 0.29470107447883187, F1 Score: 0.27673776228441244\n",
      "Accuracy: 0.2949765818716136, F1 Score: 0.278011442218744\n",
      "Epoch 54/100, Loss: 1.1875625848770142, Accuracy: 0.2949765818716136, F1 Score: 0.278011442218744\n",
      "Accuracy: 0.3012214161079989, F1 Score: 0.28581932004021177\n",
      "Epoch 55/100, Loss: 1.5315847396850586, Accuracy: 0.3012214161079989, F1 Score: 0.28581932004021177\n",
      "Accuracy: 0.29561943245477085, F1 Score: 0.2665067594959029\n",
      "Epoch 56/100, Loss: 1.6310691833496094, Accuracy: 0.29561943245477085, F1 Score: 0.2665067594959029\n",
      "Accuracy: 0.3025071172743135, F1 Score: 0.27422312082860867\n",
      "Epoch 57/100, Loss: 1.289142370223999, Accuracy: 0.3025071172743135, F1 Score: 0.27422312082860867\n",
      "Accuracy: 0.2979153273946184, F1 Score: 0.27919586361009957\n",
      "Epoch 58/100, Loss: 2.0249316692352295, Accuracy: 0.2979153273946184, F1 Score: 0.27919586361009957\n",
      "Accuracy: 0.3009459087152172, F1 Score: 0.2774027824602085\n",
      "Epoch 59/100, Loss: 1.3888534307479858, Accuracy: 0.3009459087152172, F1 Score: 0.2774027824602085\n",
      "Accuracy: 0.3022316098815318, F1 Score: 0.2713335276189602\n",
      "Epoch 60/100, Loss: 0.6333685517311096, Accuracy: 0.3022316098815318, F1 Score: 0.2713335276189602\n",
      "Accuracy: 0.3001193865368721, F1 Score: 0.2773634599488312\n",
      "Epoch 61/100, Loss: 1.7768280506134033, Accuracy: 0.3001193865368721, F1 Score: 0.2773634599488312\n",
      "Accuracy: 0.2960786114427404, F1 Score: 0.27337329143744177\n",
      "Epoch 62/100, Loss: 1.6057296991348267, Accuracy: 0.2960786114427404, F1 Score: 0.27337329143744177\n",
      "Accuracy: 0.2979153273946184, F1 Score: 0.2783494567744772\n",
      "Epoch 63/100, Loss: 1.2919437885284424, Accuracy: 0.2979153273946184, F1 Score: 0.2783494567744772\n",
      "Accuracy: 0.29084397097988796, F1 Score: 0.2626809602645904\n",
      "Epoch 64/100, Loss: 1.7950782775878906, Accuracy: 0.29084397097988796, F1 Score: 0.2626809602645904\n",
      "Accuracy: 0.2934153733125172, F1 Score: 0.2725788050590509\n",
      "Epoch 65/100, Loss: 1.8933159112930298, Accuracy: 0.2934153733125172, F1 Score: 0.2725788050590509\n",
      "Accuracy: 0.3009459087152172, F1 Score: 0.2822561183582366\n",
      "Epoch 66/100, Loss: 1.521864891052246, Accuracy: 0.3009459087152172, F1 Score: 0.2822561183582366\n",
      "Accuracy: 0.30342547525025254, F1 Score: 0.2749023706036646\n",
      "Epoch 67/100, Loss: 2.1001622676849365, Accuracy: 0.30342547525025254, F1 Score: 0.2749023706036646\n",
      "Accuracy: 0.29589493984755255, F1 Score: 0.28208818990641704\n",
      "Epoch 68/100, Loss: 2.211883544921875, Accuracy: 0.29589493984755255, F1 Score: 0.28208818990641704\n",
      "Accuracy: 0.29056846358710625, F1 Score: 0.2783537405035536\n",
      "Epoch 69/100, Loss: 1.930119514465332, Accuracy: 0.29056846358710625, F1 Score: 0.2783537405035536\n",
      "Accuracy: 0.29883368537055743, F1 Score: 0.29095591340840427\n",
      "Epoch 70/100, Loss: 1.8610509634017944, Accuracy: 0.29883368537055743, F1 Score: 0.29095591340840427\n",
      "Accuracy: 0.3005785655248416, F1 Score: 0.2766521116772129\n",
      "Epoch 71/100, Loss: 2.2499008178710938, Accuracy: 0.3005785655248416, F1 Score: 0.2766521116772129\n",
      "Accuracy: 0.3023234456791257, F1 Score: 0.27461621600187053\n",
      "Epoch 72/100, Loss: 1.7132593393325806, Accuracy: 0.3023234456791257, F1 Score: 0.27461621600187053\n",
      "Accuracy: 0.30397649003581595, F1 Score: 0.27721717074522956\n",
      "Epoch 73/100, Loss: 1.841318964958191, Accuracy: 0.30397649003581595, F1 Score: 0.27721717074522956\n",
      "Accuracy: 0.2960786114427404, F1 Score: 0.2718607975909777\n",
      "Epoch 74/100, Loss: 2.230458974838257, Accuracy: 0.2960786114427404, F1 Score: 0.2718607975909777\n",
      "Accuracy: 0.3016805950959684, F1 Score: 0.27668224927913987\n",
      "Epoch 75/100, Loss: 2.7214865684509277, Accuracy: 0.3016805950959684, F1 Score: 0.27668224927913987\n",
      "Accuracy: 0.29809899898980624, F1 Score: 0.2708883249617284\n",
      "Epoch 76/100, Loss: 1.3288531303405762, Accuracy: 0.29809899898980624, F1 Score: 0.2708883249617284\n",
      "Accuracy: 0.297364312609055, F1 Score: 0.2769741741197776\n",
      "Epoch 77/100, Loss: 1.9745128154754639, Accuracy: 0.297364312609055, F1 Score: 0.2769741741197776\n",
      "Accuracy: 0.2979153273946184, F1 Score: 0.27241506234340346\n",
      "Epoch 78/100, Loss: 1.3068337440490723, Accuracy: 0.2979153273946184, F1 Score: 0.27241506234340346\n",
      "Accuracy: 0.3003948939296538, F1 Score: 0.2790008611750062\n",
      "Epoch 79/100, Loss: 2.0204429626464844, Accuracy: 0.3003948939296538, F1 Score: 0.2790008611750062\n",
      "Accuracy: 0.29910919276333914, F1 Score: 0.26095526004906094\n",
      "Epoch 80/100, Loss: 3.338027000427246, Accuracy: 0.29910919276333914, F1 Score: 0.26095526004906094\n",
      "Accuracy: 0.2990173569657453, F1 Score: 0.27787855784618803\n",
      "Epoch 81/100, Loss: 1.8571338653564453, Accuracy: 0.2990173569657453, F1 Score: 0.27787855784618803\n",
      "Accuracy: 0.299292864358527, F1 Score: 0.2787725008033799\n",
      "Epoch 82/100, Loss: 1.7142086029052734, Accuracy: 0.299292864358527, F1 Score: 0.2787725008033799\n",
      "Accuracy: 0.292680686931766, F1 Score: 0.28392736805921337\n",
      "Epoch 83/100, Loss: 1.385181188583374, Accuracy: 0.292680686931766, F1 Score: 0.28392736805921337\n",
      "Accuracy: 0.30370098264303425, F1 Score: 0.27591648036691424\n",
      "Epoch 84/100, Loss: 1.9017263650894165, Accuracy: 0.30370098264303425, F1 Score: 0.27591648036691424\n",
      "Accuracy: 0.2966296262283038, F1 Score: 0.26623607004603217\n",
      "Epoch 85/100, Loss: 1.4826116561889648, Accuracy: 0.2966296262283038, F1 Score: 0.26623607004603217\n",
      "Accuracy: 0.301129580310405, F1 Score: 0.28261179396230923\n",
      "Epoch 86/100, Loss: 1.5526952743530273, Accuracy: 0.301129580310405, F1 Score: 0.28261179396230923\n",
      "Accuracy: 0.292680686931766, F1 Score: 0.2808475036582478\n",
      "Epoch 87/100, Loss: 1.9529223442077637, Accuracy: 0.292680686931766, F1 Score: 0.2808475036582478\n",
      "Accuracy: 0.29598677564514647, F1 Score: 0.28471307490681774\n",
      "Epoch 88/100, Loss: 1.6112040281295776, Accuracy: 0.29598677564514647, F1 Score: 0.28471307490681774\n",
      "Accuracy: 0.29415005969326846, F1 Score: 0.2683399075158664\n",
      "Epoch 89/100, Loss: 1.3665730953216553, Accuracy: 0.29415005969326846, F1 Score: 0.2683399075158664\n",
      "Accuracy: 0.2977316557994306, F1 Score: 0.27259172677089083\n",
      "Epoch 90/100, Loss: 1.2053087949752808, Accuracy: 0.2977316557994306, F1 Score: 0.27259172677089083\n",
      "Accuracy: 0.29754798420424283, F1 Score: 0.2693746780549381\n",
      "Epoch 91/100, Loss: 0.7461795210838318, Accuracy: 0.29754798420424283, F1 Score: 0.2693746780549381\n",
      "Accuracy: 0.3054458627973184, F1 Score: 0.27115679924547503\n",
      "Epoch 92/100, Loss: 1.5873218774795532, Accuracy: 0.3054458627973184, F1 Score: 0.27115679924547503\n",
      "Accuracy: 0.3012214161079989, F1 Score: 0.2794311086548143\n",
      "Epoch 93/100, Loss: 1.6264817714691162, Accuracy: 0.3012214161079989, F1 Score: 0.2794311086548143\n",
      "Accuracy: 0.3003030581320599, F1 Score: 0.27440190670405284\n",
      "Epoch 94/100, Loss: 1.9771859645843506, Accuracy: 0.3003030581320599, F1 Score: 0.27440190670405284\n",
      "Accuracy: 0.2921296721462026, F1 Score: 0.27992834837477537\n",
      "Epoch 95/100, Loss: 1.6416850090026855, Accuracy: 0.2921296721462026, F1 Score: 0.27992834837477537\n",
      "Accuracy: 0.3005785655248416, F1 Score: 0.2795078192018631\n",
      "Epoch 96/100, Loss: 1.293161392211914, Accuracy: 0.3005785655248416, F1 Score: 0.2795078192018631\n",
      "Accuracy: 0.2995683717513087, F1 Score: 0.2777181454506805\n",
      "Epoch 97/100, Loss: 1.5682294368743896, Accuracy: 0.2995683717513087, F1 Score: 0.2777181454506805\n",
      "Accuracy: 0.292680686931766, F1 Score: 0.26405934722118135\n",
      "Epoch 98/100, Loss: 2.224989891052246, Accuracy: 0.292680686931766, F1 Score: 0.26405934722118135\n",
      "Accuracy: 0.29910919276333914, F1 Score: 0.2809855735182845\n",
      "Epoch 99/100, Loss: 0.8120028972625732, Accuracy: 0.29910919276333914, F1 Score: 0.2809855735182845\n",
      "Accuracy: 0.2981908347874001, F1 Score: 0.28216599496398603\n",
      "Epoch 100/100, Loss: 1.7408747673034668, Accuracy: 0.2981908347874001, F1 Score: 0.28216599496398603\n"
     ]
    }
   ],
   "source": [
    "fine_tune_model(model.to(device), combined_labelled_dataloader, combined_test_dataloader,num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2981908347874001, F1 Score: 0.28216599496398603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2981908347874001, 0.28216599496398603)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, combined_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
