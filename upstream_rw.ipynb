{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject        x        y        z     activity\n",
       "0        0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1        0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2        0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3        0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4        0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data.columns = ['subject', 'x', 'y', 'z', 'activity']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.200803e+06</td>\n",
       "      <td>3.200803e+06</td>\n",
       "      <td>3.200803e+06</td>\n",
       "      <td>3.200803e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.011755e+00</td>\n",
       "      <td>-5.071222e+00</td>\n",
       "      <td>-4.468575e+00</td>\n",
       "      <td>1.885688e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.261132e+00</td>\n",
       "      <td>6.336688e+00</td>\n",
       "      <td>4.661046e+00</td>\n",
       "      <td>4.155098e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.957740e+01</td>\n",
       "      <td>-1.991230e+01</td>\n",
       "      <td>-1.957880e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>-9.394270e+00</td>\n",
       "      <td>-6.599290e+00</td>\n",
       "      <td>-2.520825e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>-6.812620e+00</td>\n",
       "      <td>-3.481320e+00</td>\n",
       "      <td>1.441360e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>-4.284745e-01</td>\n",
       "      <td>-2.027360e+00</td>\n",
       "      <td>4.166750e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.942200e+01</td>\n",
       "      <td>1.908710e+01</td>\n",
       "      <td>1.942060e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject             x             y             z\n",
       "count  3.200803e+06  3.200803e+06  3.200803e+06  3.200803e+06\n",
       "mean   7.011755e+00 -5.071222e+00 -4.468575e+00  1.885688e+00\n",
       "std    4.261132e+00  6.336688e+00  4.661046e+00  4.155098e+00\n",
       "min    0.000000e+00 -1.957740e+01 -1.991230e+01 -1.957880e+01\n",
       "25%    3.000000e+00 -9.394270e+00 -6.599290e+00 -2.520825e-01\n",
       "50%    7.000000e+00 -6.812620e+00 -3.481320e+00  1.441360e+00\n",
       "75%    1.100000e+01 -4.284745e-01 -2.027360e+00  4.166750e+00\n",
       "max    1.400000e+01  1.942200e+01  1.908710e+01  1.942060e+01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject     0\n",
       "x           0\n",
       "y           0\n",
       "z           0\n",
       "activity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of window dataset (2 sec with 0% overlap): (32008, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 2\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 0\n",
    "\n",
    "window_data, _ = sliding_window_samples(data, window_size, overlap_ratio)\n",
    "print(f\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, -9.57434, -2.02733, 1.34506, 'climbing_up'],\n",
       "       [0, -9.56479, -1.99597, 1.39345, 'climbing_up'],\n",
       "       [0, -9.55122, -1.98445, 1.41139, 'climbing_up'],\n",
       "       [0, -9.51335, -1.97557, 1.42615, 'climbing_up'],\n",
       "       [0, -9.52959, -1.98187, 1.45395, 'climbing_up'],\n",
       "       [0, -9.55446, -2.00818, 1.40735, 'climbing_up'],\n",
       "       [0, -9.53834, -2.00737, 1.37628, 'climbing_up'],\n",
       "       [0, -9.53804, -2.01022, 1.39708, 'climbing_up'],\n",
       "       [0, -9.54524, -2.00552, 1.33041, 'climbing_up'],\n",
       "       [0, -9.52776, -2.00702, 1.34845, 'climbing_up'],\n",
       "       [0, -9.55386, -2.02527, 1.35837, 'climbing_up'],\n",
       "       [0, -9.52835, -2.03238, 1.38365, 'climbing_up'],\n",
       "       [0, -9.56699, -1.99843, 1.39401, 'climbing_up'],\n",
       "       [0, -9.56685, -2.00259, 1.39221, 'climbing_up'],\n",
       "       [0, -9.52701, -2.00899, 1.43456, 'climbing_up'],\n",
       "       [0, -9.5379, -1.98004, 1.41574, 'climbing_up'],\n",
       "       [0, -9.5269, -2.00275, 1.39914, 'climbing_up'],\n",
       "       [0, -9.53383, -2.01756, 1.39552, 'climbing_up'],\n",
       "       [0, -9.52927, -2.04524, 1.39505, 'climbing_up'],\n",
       "       [0, -9.54939, -2.06349, 1.35515, 'climbing_up'],\n",
       "       [0, -9.57919, -2.05342, 1.30473, 'climbing_up'],\n",
       "       [0, -9.54581, -2.00975, 1.40678, 'climbing_up'],\n",
       "       [0, -9.55437, -1.97728, 1.37785, 'climbing_up'],\n",
       "       [0, -9.53853, -1.91187, 1.42242, 'climbing_up'],\n",
       "       [0, -9.54269, -1.93948, 1.38406, 'climbing_up'],\n",
       "       [0, -9.55089, -1.94292, 1.35092, 'climbing_up'],\n",
       "       [0, -9.55858, -1.92409, 1.37088, 'climbing_up'],\n",
       "       [0, -9.58437, -1.9384, 1.30769, 'climbing_up'],\n",
       "       [0, -9.60376, -1.9664, 1.30066, 'climbing_up'],\n",
       "       [0, -9.57201, -1.937, 1.37991, 'climbing_up'],\n",
       "       [0, -9.48825, -1.94637, 1.41388, 'climbing_up'],\n",
       "       [0, -9.50738, -1.98032, 1.39027, 'climbing_up'],\n",
       "       [0, -9.50343, -1.9859, 1.43124, 'climbing_up'],\n",
       "       [0, -9.51437, -2.00862, 1.39946, 'climbing_up'],\n",
       "       [0, -9.60202, -2.0014, 1.37727, 'climbing_up'],\n",
       "       [0, -9.59247, -1.99593, 1.33881, 'climbing_up'],\n",
       "       [0, -9.54808, -2.01442, 1.33626, 'climbing_up'],\n",
       "       [0, -9.53618, -1.96252, 1.39854, 'climbing_up'],\n",
       "       [0, -9.49696, -1.96182, 1.40451, 'climbing_up'],\n",
       "       [0, -9.54164, -1.99272, 1.42386, 'climbing_up'],\n",
       "       [0, -9.54819, -2.03534, 1.40292, 'climbing_up'],\n",
       "       [0, -9.53156, -2.03748, 1.4025, 'climbing_up'],\n",
       "       [0, -9.56239, -2.03786, 1.37531, 'climbing_up'],\n",
       "       [0, -9.53401, -1.9895, 1.36824, 'climbing_up'],\n",
       "       [0, -9.54245, -2.00624, 1.36913, 'climbing_up'],\n",
       "       [0, -9.53153, -2.01468, 1.35768, 'climbing_up'],\n",
       "       [0, -9.52769, -2.04039, 1.38901, 'climbing_up'],\n",
       "       [0, -9.53714, -1.98903, 1.3676, 'climbing_up'],\n",
       "       [0, -9.53729, -2.00072, 1.38795, 'climbing_up'],\n",
       "       [0, -9.56065, -2.00671, 1.36797, 'climbing_up'],\n",
       "       [0, -9.53923, -1.98451, 1.40778, 'climbing_up'],\n",
       "       [0, -9.54686, -1.99846, 1.39595, 'climbing_up'],\n",
       "       [0, -9.54067, -1.9949, 1.38991, 'climbing_up'],\n",
       "       [0, -9.55211, -2.03433, 1.37581, 'climbing_up'],\n",
       "       [0, -9.5605, -2.02248, 1.34822, 'climbing_up'],\n",
       "       [0, -9.5572, -1.98334, 1.36485, 'climbing_up'],\n",
       "       [0, -9.53024, -1.99268, 1.38704, 'climbing_up'],\n",
       "       [0, -9.52623, -1.9971, 1.35849, 'climbing_up'],\n",
       "       [0, -9.5197, -1.99419, 1.41997, 'climbing_up'],\n",
       "       [0, -9.56703, -2.01167, 1.36169, 'climbing_up'],\n",
       "       [0, -9.57712, -2.0105, 1.37529, 'climbing_up'],\n",
       "       [0, -9.53651, -1.99554, 1.28723, 'climbing_up'],\n",
       "       [0, -9.57671, -1.98624, 1.3004, 'climbing_up'],\n",
       "       [0, -9.59233, -1.97415, 1.3421, 'climbing_up'],\n",
       "       [0, -9.51952, -1.96303, 1.4254, 'climbing_up'],\n",
       "       [0, -9.51927, -1.96407, 1.40817, 'climbing_up'],\n",
       "       [0, -9.52806, -1.99628, 1.37447, 'climbing_up'],\n",
       "       [0, -9.51862, -2.03751, 1.39439, 'climbing_up'],\n",
       "       [0, -9.55586, -2.04478, 1.3385, 'climbing_up'],\n",
       "       [0, -9.56255, -2.04832, 1.31299, 'climbing_up'],\n",
       "       [0, -9.56233, -1.98579, 1.30225, 'climbing_up'],\n",
       "       [0, -9.55365, -1.97885, 1.37724, 'climbing_up'],\n",
       "       [0, -9.53552, -1.98027, 1.39236, 'climbing_up'],\n",
       "       [0, -9.58125, -1.95444, 1.44051, 'climbing_up'],\n",
       "       [0, -9.57291, -2.01328, 1.39828, 'climbing_up'],\n",
       "       [0, -9.55646, -2.01436, 1.3744, 'climbing_up'],\n",
       "       [0, -9.52835, -2.01096, 1.34441, 'climbing_up'],\n",
       "       [0, -9.5558, -2.00215, 1.32759, 'climbing_up'],\n",
       "       [0, -9.51975, -1.99797, 1.37598, 'climbing_up'],\n",
       "       [0, -9.52168, -2.02998, 1.34213, 'climbing_up'],\n",
       "       [0, -9.55077, -2.00726, 1.34149, 'climbing_up'],\n",
       "       [0, -9.56644, -2.04855, 1.3988, 'climbing_up'],\n",
       "       [0, -9.54649, -2.03125, 1.41132, 'climbing_up'],\n",
       "       [0, -9.55216, -1.98485, 1.44388, 'climbing_up'],\n",
       "       [0, -9.54489, -1.97496, 1.45764, 'climbing_up'],\n",
       "       [0, -9.55424, -1.97469, 1.43362, 'climbing_up'],\n",
       "       [0, -9.53337, -2.00058, 1.42389, 'climbing_up'],\n",
       "       [0, -9.55183, -2.00632, 1.37601, 'climbing_up'],\n",
       "       [0, -9.55583, -2.0061, 1.37352, 'climbing_up'],\n",
       "       [0, -9.55884, -2.03387, 1.36136, 'climbing_up'],\n",
       "       [0, -9.54465, -1.98581, 1.42012, 'climbing_up'],\n",
       "       [0, -9.50615, -1.9633, 1.38219, 'climbing_up'],\n",
       "       [0, -9.54141, -1.94768, 1.36845, 'climbing_up'],\n",
       "       [0, -9.55809, -2.01511, 1.31859, 'climbing_up'],\n",
       "       [0, -9.5717, -1.98106, 1.38014, 'climbing_up'],\n",
       "       [0, -9.57861, -1.94254, 1.39775, 'climbing_up'],\n",
       "       [0, -9.52849, -1.92653, 1.32771, 'climbing_up'],\n",
       "       [0, -9.55745, -1.96399, 1.37877, 'climbing_up'],\n",
       "       [0, -9.51877, -1.95482, 1.36098, 'climbing_up'],\n",
       "       [0, -9.4998, -1.95235, 1.38393, 'climbing_up']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "window_data = window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "window_data = window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_noise_single_window(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param noise_level: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy data.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_single_window(data, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly crop a single window of data and pad it to maintain original shape.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param crop_ratio: Ratio of the original window size to keep.\n",
    "    :return: Cropped and padded data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    new_size = int(window_size * crop_ratio)\n",
    "    start = np.random.randint(0, window_size - new_size)\n",
    "    end = start + new_size\n",
    "    cropped_data = data[start:end, :]\n",
    "\n",
    "    # Pad the cropped data to maintain original window size\n",
    "    padding_size = window_size - new_size\n",
    "    padding = np.zeros((padding_size, data.shape[1]))\n",
    "    padded_data = np.vstack((cropped_data, padding))\n",
    "\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of warping.\n",
    "    :return: Warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.copy(data)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        time_points = np.linspace(0, 1, window_size)\n",
    "        random_points = np.linspace(0, 1, np.random.randint(4, 10))\n",
    "        warp_values = 1 + np.random.normal(0, warp_factor, random_points.size)\n",
    "        interpolator = CubicSpline(random_points, warp_values)\n",
    "        warped_data[:, j] *= interpolator(time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply time warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of time warping.\n",
    "    :return: Time-warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_points = np.linspace(0, 1, window_size)\n",
    "    random_points = np.sort(np.random.rand(np.random.randint(3, 6)))\n",
    "    warp_values = np.random.normal(1, warp_factor, random_points.size)\n",
    "    interpolator = CubicSpline(random_points, warp_values)\n",
    "    warped_time = interpolator(time_points)\n",
    "    warped_time -= warped_time.min()\n",
    "    warped_time /= warped_time.max()\n",
    "    warped_time *= (window_size - 1)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        interpolator = CubicSpline(np.arange(window_size), data[:, j])\n",
    "        warped_data[:, j] = interpolator(warped_time)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of noisy data: (100, 3)\n",
      "shape of cropped data: (100, 3)\n",
      "shape of warped data: (100, 3)\n",
      "shape of time warped data: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# add random noise\n",
    "noisy_data = add_random_noise_single_window(window_data[0], noise_level=0.1)\n",
    "print(f\"shape of noisy data: {noisy_data.shape}\")\n",
    "# random cropping\n",
    "cropped_data = random_cropping_single_window(window_data[0], crop_ratio=0.8)\n",
    "print(f\"shape of cropped data: {cropped_data.shape}\")\n",
    "# magnitude warping\n",
    "warped_data = magnitude_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of warped data: {warped_data.shape}\")\n",
    "# time warping\n",
    "time_warped_data = time_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of time warped data: {time_warped_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the original data\n",
    "augmented_data = np.copy(window_data)\n",
    "\n",
    "# create labels for augmented data\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "# loop over all windows\n",
    "for i in range(window_data.shape[0]):\n",
    "    # choose one number from 0 to 3\n",
    "    random_number = np.random.randint(0, 4)\n",
    "\n",
    "    if random_number == 0:\n",
    "        augmented_data[i] = add_random_noise_single_window(window_data[i], noise_level=0.1)\n",
    "        augmented_labels.append(0)\n",
    "    elif random_number == 1:\n",
    "        augmented_data[i] = random_cropping_single_window(window_data[i], crop_ratio=0.8)\n",
    "        augmented_labels.append(1)\n",
    "    elif random_number == 2:\n",
    "        augmented_data[i] = magnitude_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(2)\n",
    "    else:\n",
    "        augmented_data[i] = time_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7922, 1: 7953, 2: 8035, 3: 8098}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of augmented data: (32008, 100, 3)\n",
      "shape of augmented labels: 32008\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of augmented data: {augmented_data.shape}\")\n",
    "print(f\"shape of augmented labels: {len(augmented_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented data and labels\n",
    "# np.save('./data_processing/augmented_data.npy', augmented_data)\n",
    "# np.save('./data_processing/augmented_labels.npy', augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmented data and labels\n",
    "augmented_data = np.load('./data_processing/augmented_data.npy', allow_pickle=True)\n",
    "augmented_labels = np.load('./data_processing/augmented_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32008, 100, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32008,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7930, 1: 8078, 2: 7986, 3: 8014}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of arrays to a single numpy array\n",
    "# if isinstance(augmented_data, list):\n",
    "#     augmented_data = np.stack(augmented_data)\n",
    "\n",
    "# # Ensure the data type is float32\n",
    "# augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "# augmented_labels_tensor = torch.from_numpy(np.array(augmented_labels)).long()\n",
    "\n",
    "# # split data into train and test set\n",
    "# train_size = int(0.8 * augmented_data.shape[0])\n",
    "# test_size = augmented_data.shape[0] - train_size\n",
    "# train_data, test_data = torch.utils.data.random_split(augmented_data, [train_size, test_size])\n",
    "# train_labels, test_labels = torch.utils.data.random_split(augmented_labels, [train_size, test_size])\n",
    "\n",
    "# # create train and test dataset\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # create train and test dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # check if GPU is available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming augmented_data and augmented_labels are numpy arrays\n",
    "augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "augmented_labels_tensor = torch.from_numpy(augmented_labels)\n",
    "#convert labels to long\n",
    "augmented_labels_tensor = augmented_labels_tensor.long()\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(0.8 * len(augmented_data_tensor))\n",
    "test_size = len(augmented_data_tensor) - train_size\n",
    "\n",
    "# Creating datasets\n",
    "dataset = TensorDataset(augmented_data_tensor, augmented_labels_tensor)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Function to extract tensors from Subset\n",
    "def extract_subset_data(subset, dataset):\n",
    "    return dataset.tensors[0][subset.indices], dataset.tensors[1][subset.indices]\n",
    "\n",
    "# Extract data and labels from train and test sets\n",
    "train_data, train_labels = extract_subset_data(train_dataset, dataset)\n",
    "test_data, test_labels = extract_subset_data(test_dataset, dataset)\n",
    "\n",
    "# create train and test TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_loader: 201\n",
      "shape of test_loader: 51\n"
     ]
    }
   ],
   "source": [
    "#  print the shape of train_loader and test_loader\n",
    "print(f\"shape of train_loader: {len(train_loader)}\")\n",
    "print(f\"shape of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    #calculate accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return running_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. Test Loss: {test_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}\")\n",
    "    return train_losses, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 4\n",
    "\n",
    "# model = CNNFeatureExtractor(num_classes=4)\n",
    "\n",
    "# # move model to GPU if available\n",
    "# model.to(device)\n",
    "\n",
    "# # define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# # train and test model\n",
    "# num_epochs = 20\n",
    "# train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"cnn_feature_extractor_{timestamp}.pt\"\n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20.. Train Loss: 1.408.. Test Loss: 1.392.. Test Accuracy: 0.261\n",
      "Epoch: 2/20.. Train Loss: 1.399.. Test Loss: 1.387.. Test Accuracy: 0.256\n",
      "Epoch: 3/20.. Train Loss: 1.394.. Test Loss: 1.386.. Test Accuracy: 0.271\n",
      "Epoch: 4/20.. Train Loss: 1.392.. Test Loss: 1.384.. Test Accuracy: 0.256\n",
      "Epoch: 5/20.. Train Loss: 1.390.. Test Loss: 1.384.. Test Accuracy: 0.266\n",
      "Epoch: 6/20.. Train Loss: 1.388.. Test Loss: 1.382.. Test Accuracy: 0.263\n",
      "Epoch: 7/20.. Train Loss: 1.388.. Test Loss: 1.382.. Test Accuracy: 0.283\n",
      "Epoch: 8/20.. Train Loss: 1.384.. Test Loss: 1.380.. Test Accuracy: 0.279\n",
      "Epoch: 9/20.. Train Loss: 1.386.. Test Loss: 1.380.. Test Accuracy: 0.281\n",
      "Epoch: 10/20.. Train Loss: 1.383.. Test Loss: 1.379.. Test Accuracy: 0.259\n",
      "Epoch: 11/20.. Train Loss: 1.382.. Test Loss: 1.379.. Test Accuracy: 0.295\n",
      "Epoch: 12/20.. Train Loss: 1.382.. Test Loss: 1.378.. Test Accuracy: 0.291\n",
      "Epoch: 13/20.. Train Loss: 1.380.. Test Loss: 1.377.. Test Accuracy: 0.286\n",
      "Epoch: 14/20.. Train Loss: 1.377.. Test Loss: 1.376.. Test Accuracy: 0.271\n",
      "Epoch: 15/20.. Train Loss: 1.376.. Test Loss: 1.375.. Test Accuracy: 0.286\n",
      "Epoch: 16/20.. Train Loss: 1.375.. Test Loss: 1.373.. Test Accuracy: 0.301\n",
      "Epoch: 17/20.. Train Loss: 1.374.. Test Loss: 1.373.. Test Accuracy: 0.297\n",
      "Epoch: 18/20.. Train Loss: 1.373.. Test Loss: 1.372.. Test Accuracy: 0.299\n",
      "Epoch: 19/20.. Train Loss: 1.372.. Test Loss: 1.370.. Test Accuracy: 0.311\n",
      "Epoch: 20/20.. Train Loss: 1.370.. Test Loss: 1.369.. Test Accuracy: 0.284\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "model = TCN(num_inputs, num_channels, kernel_size, num_classes=4).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00003)  # Replace lr with your learning rate\n",
    "\n",
    "# train and test model\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "model_name = f\"tcn_{timestamp}.pt\"\n",
    "torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
