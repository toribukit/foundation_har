{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import copy\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import Optimizer\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.stats import mode\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "num_epochs = 20\n",
    "batch_size = 32  # Set your batch size\n",
    "learning_rate_client = 0.001\n",
    "local_epochs = 1\n",
    "subject_dir = 'FL_Data/windowed_data_refused/subject_'  # Set your directory to the subject data\n",
    "num_clients = 54\n",
    "num_classes = 4 # number of transformations\n",
    "local_learning_rate = 0.01\n",
    "local_steps= 1\n",
    "learning_rate_decay= False\n",
    "future_test= False\n",
    "learning_rate_decay_gamma= 0.99\n",
    "n_clusters= 6\n",
    "mu= 1\n",
    "global_rounds= 100\n",
    "join_ratio= 1.0\n",
    "eval_gap= 1\n",
    "detailed_info= False\n",
    "partition= \"nature\"\n",
    "initial_rounds= 10\n",
    "metric= 'cosine'\n",
    "linkage= 'complete'\n",
    "\n",
    "#current timestamp\n",
    "current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_client(id, batch_size=batch_size, type='labelled_train'):\n",
    "    # Load the data\n",
    "    data = np.load(subject_dir + str(id) + '/windowed_' + type + '_x.npy')\n",
    "    labels = np.load(subject_dir + str(id) + '/windowed_' + type + '_y.npy')\n",
    "\n",
    "    # print shape of data\n",
    "    # print(data.shape)\n",
    "    # print(labels.shape)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    data = torch.from_numpy(data).float()\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "\n",
    "    # Create a dataloader\n",
    "    if type == 'labelled_train' or type == 'unlabelled_train':\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    else:\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedGradientDescent(Optimizer):\n",
    "  def __init__(self, params, lr=0.01, mu=0.0):\n",
    "    default = dict(lr=lr, mu=mu)\n",
    "    super().__init__(params, default)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def step(self, global_params, device):\n",
    "    for group in self.param_groups:\n",
    "      for p, g in zip(group['params'], global_params):\n",
    "        g = g.to(device)\n",
    "        d_p = p.grad.data + group['mu'] * (p.data - g.data)\n",
    "        p.data.add_(d_p, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_autoencoder(model, train_loader, device, learning_rate=0.01, epochs=5):\n",
    "#     model.to(device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for epoch in range(epochs):\n",
    "#         for data, target in train_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             # print(data.shape)\n",
    "#             data = data.permute(0, 2, 1)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             # print(output.shape)\n",
    "#             loss = criterion(output, data)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         epoch_loss = total_loss / len(train_loader)\n",
    "#         # print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "#         total_loss = 0  # Reset total loss for the next epoch\n",
    "\n",
    "#     results = {\n",
    "#         'train_loss': epoch_loss\n",
    "#     }\n",
    "    \n",
    "#     return results  # Returns the average loss of the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_autoencoder(model, test_loader, device):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             data = data.permute(0, 2, 1)\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, data)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(test_loader)\n",
    "#     # print(f'Test Loss: {avg_loss}')\n",
    "    \n",
    "#     return avg_loss  # Returns the average loss for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train_model(model, train_loader, device, learning_rate=0.001, epochs=1):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        total_loss = 0  # Reset total loss for the next epoch\n",
    "\n",
    "    results = {\n",
    "        'train_loss': epoch_loss\n",
    "    }\n",
    "    \n",
    "    return results  # Returns the average loss of the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test the model\n",
    "# method to test the model and get the accuracy and f1 score\n",
    "def test_model(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_num = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            #calculate test_num\n",
    "            test_num += len(target)\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # print(f'Accuracy: {accuracy}, F1 Score: {f1}')\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(object):\n",
    "  \"\"\"\n",
    "  Base class for clients in federated learning.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model, id, **kwargs):\n",
    "    self.model = copy.deepcopy(model)\n",
    "    self.device = device\n",
    "    self.id = id\n",
    "    self.num_classes = num_classes\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = local_learning_rate\n",
    "    self.local_steps = local_steps\n",
    "    self.data_path = subject_dir\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.future_test = future_test\n",
    "\n",
    "\n",
    "    # check BatchNorm\n",
    "    self.has_BatchNorm = False\n",
    "    for layer in self.model.children():\n",
    "      if isinstance(layer, nn.BatchNorm2d):\n",
    "        self.has_BatchNorm = True\n",
    "        break\n",
    "\n",
    "    self.loss = nn.CrossEntropyLoss()\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate) # momentum=0.9, weight_decay=1e-4\n",
    "    self.learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "      optimizer=self.optimizer,\n",
    "      gamma=learning_rate_decay_gamma\n",
    "    )\n",
    "\n",
    "  def load_train_data(self, batch_size=batch_size):\n",
    "    train_data = load_data_client(self.id, batch_size, 'unlabelled_train')\n",
    "    # get length of train data\n",
    "    self.train_samples = len(train_data)\n",
    "\n",
    "    return train_data\n",
    "    \n",
    "    # if batch_size == None:\n",
    "    #   batch_size = self.batch_size\n",
    "    # train_data = read_client_data(self.dataset, self.data_path, self.id, is_train=True)\n",
    "\n",
    "    # # label poison attack\n",
    "    # if self.malicious and self.attack_type == 'A1':\n",
    "    #   for idx in range(len(train_data)):\n",
    "    #     train_data[idx][1] = self.num_classes - train_data[idx][1] - 1\n",
    "    # self.train_samples = len(train_data)\n",
    "    # return DataLoader(train_data, batch_size, drop_last=True, shuffle=False)\n",
    "\n",
    "  def load_test_data(self, batch_size=batch_size):\n",
    "    test_data = load_data_client(self.id, batch_size, 'unlabelled_train')\n",
    "\n",
    "    return test_data\n",
    "    # \"\"\"\n",
    "    # fine-tunes the model using the loaded training data\n",
    "    # \"\"\"\n",
    "    # if batch_size == None:\n",
    "    #   batch_size = self.batch_size\n",
    "    # test_data = read_client_data(self.dataset, self.data_path, self.id, is_train=False)\n",
    "    # return DataLoader(test_data, batch_size, drop_last=False, shuffle=False)\n",
    "\n",
    "  def set_parameters(self, model):\n",
    "    for new_param, old_param in zip(model.parameters(), self.model.parameters()):\n",
    "      old_param.data = new_param.data.clone()\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    testloaderfull = self.load_test_data()\n",
    "\n",
    "    test_acc, test_num = test_model(self.model, testloaderfull)\n",
    "\n",
    "    # self.model.eval()\n",
    "\n",
    "    # test_acc = 0\n",
    "    # test_num = 0\n",
    "    # y_prob = []\n",
    "    # y_true = []\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #   for x, y in testloaderfull:\n",
    "    #     if type(x) == type([]):\n",
    "    #       x[0] = x[0].to(self.device)\n",
    "    #     else:\n",
    "    #       x = x.to(self.device)\n",
    "    #     y = y.to(self.device)\n",
    "    #     output = self.model(x)\n",
    "\n",
    "    #     test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "    #     test_num += y.shape[0]\n",
    "\n",
    "    #     y_prob.append(output.detach().cpu().numpy())\n",
    "    #     nc = self.num_classes\n",
    "    #     if self.num_classes == 2:\n",
    "    #       nc += 1\n",
    "    #     lb = label_binarize(y.detach().cpu().numpy(), classes=np.arange(nc))\n",
    "    #     if self.num_classes == 2:\n",
    "    #       lb = lb[:, :2]\n",
    "    #     y_true.append(lb)\n",
    "\n",
    "    # y_prob = np.concatenate(y_prob, axis=0)\n",
    "    # y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    return test_acc, test_num\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    trainloader = self.load_train_data()\n",
    "\n",
    "    self.model.eval()\n",
    "\n",
    "    train_num = 0\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "      for x, y in trainloader:\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        output = self.model(x)\n",
    "        loss = self.loss(output, y)\n",
    "        train_num += y.shape[0]\n",
    "        losses += loss.item() * y.shape[0]\n",
    "\n",
    "    return losses, train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clientCHAR(Client):\n",
    "  def __init__(self, model, id, **kwargs):\n",
    "    super().__init__(model, id, **kwargs)\n",
    "    self.mu = mu\n",
    "    self.model_per = copy.deepcopy(self.model)\n",
    "    self.optimizer_per = PerturbedGradientDescent(self.model_per.parameters(), lr=self.learning_rate, mu=self.mu)\n",
    "    self.learning_rate_scheduler_per = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=self.optimizer_per,\n",
    "        gamma=learning_rate_decay_gamma\n",
    "        )\n",
    "\n",
    "  def dtrain(self):\n",
    "    trainloader = self.load_train_data()\n",
    "    model = copy.deepcopy(self.model)\n",
    "    self.model.train()\n",
    "    self.model_per.train()\n",
    "\n",
    "    max_local_steps = self.local_steps\n",
    "\n",
    "    for step in range(max_local_steps):\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out_p = self.model_per(x)\n",
    "        loss = self.loss(out_p, y)\n",
    "        self.optimizer_per.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_per.step(model.parameters(), self.device)\n",
    "\n",
    "        out_g = self.model(x)\n",
    "        loss = self.loss(out_g, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    if self.learning_rate_decay:\n",
    "      self.learning_rate_scheduler.step()\n",
    "      self.learning_rate_scheduler_per.step()\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    testloaderfull = self.load_test_data()\n",
    "\n",
    "    test_acc, test_num = test_model(self.model_per, testloaderfull)\n",
    "    # self.model_per.eval()\n",
    "\n",
    "    # test_acc = 0\n",
    "    # test_num = 0\n",
    "    # y_prob = []\n",
    "    # y_true = []\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #   for x, y in testloaderfull:\n",
    "    #     if type(x) == type([]):\n",
    "    #       x[0] = x[0].to(self.device)\n",
    "    #     else:\n",
    "    #       x = x.to(self.device)\n",
    "    #     y = y.to(self.device)\n",
    "    #     output = self.model_per(x)\n",
    "\n",
    "    #     test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "    #     test_num += y.shape[0]\n",
    "\n",
    "    #     y_prob.append(F.softmax(output).detach().cpu().numpy())\n",
    "    #     y_true.append(label_binarize(y.detach().cpu().numpy(), classes=np.arange(self.num_classes)))\n",
    "\n",
    "    # y_prob = np.concatenate(y_prob, axis=0)\n",
    "    # y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    return test_acc, test_num\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    trainloader = self.load_train_data()\n",
    "    self.model_per.eval()\n",
    "\n",
    "    train_num = 0\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model_per(x)\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        #add a regularization term to the loss\n",
    "        # ensure that the personalized model doesn't deviate too far from the global model.\n",
    "        # The strength of this regularization is controlled by the parameter self.mu\n",
    "        gm = torch.cat([p.data.view(-1) for p in self.model.parameters()], dim=0)\n",
    "        pm = torch.cat([p.data.view(-1) for p in self.model_per.parameters()], dim=0)\n",
    "        loss += 0.5 * self.mu * torch.norm(pm-gm, p=2) #element-wise difference using L2 norm\n",
    "\n",
    "        train_num += y.shape[0]\n",
    "        losses += loss.item() * y.shape[0]\n",
    "\n",
    "    return losses, train_num\n",
    "\n",
    "  def get_update(self, global_model):\n",
    "    trainloader = self.load_train_data()\n",
    "    model = copy.deepcopy(self.model) #old model\n",
    "    self.set_parameters(global_model)\n",
    "    self.model.train()\n",
    "\n",
    "    max_local_steps = self.local_steps\n",
    "\n",
    "    for step in range(max_local_steps):\n",
    "      for i, (x, y) in enumerate(trainloader):\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model(x)\n",
    "        loss = self.loss(output, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    model_update = [c_param.data - s_param.data for c_param, s_param in zip(self.model.parameters(), global_model.parameters())]\n",
    "    self.set_parameters(model)\n",
    "    return model_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(object):\n",
    "  def __init__(self, model):\n",
    "    # Set up the main attributes\n",
    "    self.device = device\n",
    "    self.num_classes = num_classes\n",
    "    self.global_rounds = global_rounds\n",
    "    self.local_steps = local_steps\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = local_learning_rate\n",
    "    self.global_model = copy.deepcopy(model)\n",
    "    self.num_clients = num_clients\n",
    "    self.join_ratio = join_ratio\n",
    "    # self.attack_ratio = attack_ratio\n",
    "    # self.attack_type = attack_type\n",
    "    self.seed = seed\n",
    "    # self.algorithm = algorithm\n",
    "    self.current_round = -1\n",
    "    self.future_test = future_test\n",
    "    # self.future_ratio = future_ratio\n",
    "    self.num_training_clients = num_clients\n",
    "    self.join_clients = num_clients\n",
    "    # self.finetune_rounds = finetune_rounds\n",
    "    self.eval_gap = eval_gap\n",
    "    self.detailed_info = detailed_info\n",
    "    self.partition = partition\n",
    "    self.data_path = subject_dir\n",
    "\n",
    "    self.clients = []\n",
    "    self.training_clients = []\n",
    "    self.selected_clients = []\n",
    "\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_models = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    self.rs_test_acc_g = []\n",
    "    self.rs_train_loss_g = []\n",
    "    self.rs_test_accs_g = []\n",
    "    self.rs_test_acc_p = []\n",
    "    self.rs_train_loss_p = []\n",
    "    self.rs_test_accs_p = []\n",
    "    self.ft_train_loss = []\n",
    "    self.ft_test_acc = []\n",
    "    self.ft_std_acc = []\n",
    "\n",
    "  def set_clients(self, model, clientObj):\n",
    "\n",
    "    if self.future_test == False:\n",
    "      for i in range(self.num_clients):\n",
    "        client = clientObj(model=model, id=i)\n",
    "        self.clients.append(client)\n",
    "\n",
    "      self.training_clients = self.clients\n",
    "      self.training_clients_ids = np.arange(self.num_clients)\n",
    "\n",
    "  def select_clients(self):\n",
    "    selected_clients = list(np.random.choice(self.training_clients, self.join_clients, replace=False))\n",
    "    return selected_clients\n",
    "\n",
    "  def send_models(self):\n",
    "    for client in self.selected_clients:\n",
    "      client.set_parameters(self.global_model)\n",
    "\n",
    "  def receive_models(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = [] #weight based on the fraction of client's data\n",
    "    self.uploaded_models = []\n",
    "\n",
    "    tot_samples = 0\n",
    "    for client in self.selected_clients:\n",
    "      tot_samples += client.train_samples\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      self.uploaded_models.append(client.model)\n",
    "\n",
    "    for i, w in enumerate(self.uploaded_weights):\n",
    "      self.uploaded_weights[i] = w / tot_samples\n",
    "\n",
    "\n",
    "  def evaluate_personalized(self, acc=None, loss=None):\n",
    "    stats = self.test_metrics_personalized()\n",
    "    stats_train = self.train_metrics_personalized()\n",
    "\n",
    "    # if self.malicious_ids != []:\n",
    "    #   relative_malicious_ids = np.array([stats[0].index(i) for i in self.malicious_ids])\n",
    "\n",
    "    #   stats_A = np.array(stats)[:, relative_malicious_ids].tolist()\n",
    "    #   stats_train_A = np.array(stats_train)[:, relative_malicious_ids].tolist()\n",
    "\n",
    "    #   test_acc_A = sum(stats_A[2])*1.0 / sum(stats_A[1])\n",
    "    #   train_loss_A = sum(stats_train_A[2])*1.0 / sum(stats_train_A[1])\n",
    "    #   accs_A = [a / n for a, n in zip(stats_A[2], stats_A[1])]\n",
    "    #   losses_A = [a / n for a, n in zip(stats_train_A[2], stats_train_A[1])]\n",
    "\n",
    "    # else:\n",
    "    #   test_acc_A = -1\n",
    "    #   train_loss_A = -1\n",
    "    #   accs_A = []\n",
    "    #   losses_A = []\n",
    "\n",
    "    # benign_ids = np.sort(np.setdiff1d(self.training_clients_ids, self.malicious_ids))\n",
    "    benign_ids = np.sort(self.training_clients_ids)\n",
    "    relative_benign_ids = np.array([stats[0].index(i) for i in benign_ids])\n",
    "\n",
    "    stats_B = np.array(stats)[:, relative_benign_ids].tolist()\n",
    "    stats_train_B = np.array(stats_train)[:, relative_benign_ids].tolist()\n",
    "\n",
    "    stats = None\n",
    "    stats_train = None\n",
    "\n",
    "    test_acc = sum(stats_B[2])*1.0 / sum(stats_B[1])\n",
    "    train_loss = sum(stats_train_B[2])*1.0 / sum(stats_train_B[1])\n",
    "    accs = [a / n for a, n in zip(stats_B[2], stats_B[1])]\n",
    "    losses = [a / n for a, n in zip(stats_train_B[2], stats_train_B[1])]\n",
    "\n",
    "    if acc == None:\n",
    "      self.rs_test_acc_p.append(test_acc)\n",
    "    else:\n",
    "      acc.append(test_acc)\n",
    "\n",
    "    if loss == None:\n",
    "      self.rs_train_loss_p.append(train_loss)\n",
    "    else:\n",
    "      loss.append(train_loss)\n",
    "\n",
    "    self.rs_test_accs_p.append(accs)\n",
    "\n",
    "    print(\"Averaged Train Loss: {:.2f}\".format(train_loss))\n",
    "    print(\"Averaged Test Accurancy: {:.2f}%\".format(test_acc*100))\n",
    "    print(\"Std Test Accurancy: {:.2f}%\".format(np.std(accs)*100))\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    num_samples = []\n",
    "    tot_correct = []\n",
    "\n",
    "    for c in self.training_clients:\n",
    "      ct, ns = c.test_metrics_personalized()\n",
    "      tot_correct.append(ct*1.0)\n",
    "\n",
    "      num_samples.append(ns)\n",
    "\n",
    "    ids = [c.id for c in self.training_clients]\n",
    "    return ids, num_samples, tot_correct\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    num_samples = []\n",
    "    losses = []\n",
    "    for c in self.training_clients:\n",
    "      cl, ns = c.train_metrics_personalized()\n",
    "      num_samples.append(ns)\n",
    "      losses.append(cl*1.0)\n",
    "\n",
    "    ids = [c.id for c in self.training_clients]\n",
    "    return ids, num_samples, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedCHAR(Server):\n",
    "  def __init__(self, model):\n",
    "    super().__init__(model)\n",
    "\n",
    "    self.set_clients(model, clientCHAR)\n",
    "\n",
    "    print(f\"\\nJoin ratio / total clients: {self.join_ratio} / {self.num_training_clients}\")\n",
    "    print(\"Finished creating server and clients.\")\n",
    "\n",
    "    self.initial_rounds = initial_rounds\n",
    "    self.n_clusters = n_clusters\n",
    "    self.metric = metric\n",
    "    self.linkage = linkage\n",
    "\n",
    "  def train(self):\n",
    "    # initial Stage\n",
    "    for i in range(self.initial_rounds):\n",
    "      self.selected_clients = self.select_clients()\n",
    "      self.send_models()\n",
    "\n",
    "      for client in self.selected_clients:\n",
    "        client.dtrain()\n",
    "\n",
    "      if i%self.eval_gap == 0:\n",
    "        print(f\"\\n-------------Round number: {i}-------------\")\n",
    "        print(\"\\nEvaluate personalized models for training clients.\")\n",
    "        self.evaluate_personalized()\n",
    "\n",
    "      self.receive_models()\n",
    "      self.aggregate_parameters()\n",
    "\n",
    "    # Clustering Stage\n",
    "    print(f\"\\n-------------Clustering-------------\")\n",
    "    clients_updates = self.collect()\n",
    "    self.cluster_identity = self.cluster(clients_updates)\n",
    "    cluster_info = [[('Client', idx) for idx, g_id in enumerate(self.cluster_identity) if g_id == i] for i in range(max(self.cluster_identity)+1)]\n",
    "    for idx, info in enumerate(cluster_info):\n",
    "      print('Cluster {}: {}'.format(idx, info))\n",
    "\n",
    "    self.group_models = [copy.deepcopy(self.global_model)] * (max(self.cluster_identity) + 1)\n",
    "\n",
    "    # Remaining Stage\n",
    "    for i in range(self.global_rounds - self.initial_rounds):\n",
    "      self.selected_clients = self.select_clients()\n",
    "      self.send_models_g()\n",
    "\n",
    "      for client in self.selected_clients:\n",
    "        client.dtrain()\n",
    "\n",
    "      if i%self.eval_gap == 0:\n",
    "        print(f\"\\n-------------Round number: {i+self.initial_rounds}-------------\")\n",
    "        print(\"\\nEvaluate personalized models for training clients.\")\n",
    "        self.evaluate_personalized()\n",
    "\n",
    "      self.receive_models_g()\n",
    "      self.aggregate_parameters_g(g_epochs=(i+self.initial_rounds))\n",
    "\n",
    "    print(\"\\nFinal Average Personalized Accuracy: {}\\n\".format(self.rs_test_acc_p[-1]))\n",
    "\n",
    "    return self.cluster_identity\n",
    "\n",
    "  def receive_models(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    tot_samples = 0\n",
    "    for client in self.selected_clients:\n",
    "      tot_samples += client.train_samples\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      self.uploaded_updates.append([c_param.data - s_param.data for c_param, s_param in zip(client.model.parameters(), self.global_model.parameters())])\n",
    "\n",
    "    for i, w in enumerate(self.uploaded_weights):\n",
    "      self.uploaded_weights[i] = w / tot_samples\n",
    "\n",
    "  def add_parameters(self, w, client_update):\n",
    "    for server_param, client_param in zip(self.global_update, client_update):\n",
    "      server_param.data += client_param.data.clone() * w\n",
    "\n",
    "  def aggregate_parameters(self):\n",
    "    self.global_update = copy.deepcopy(self.uploaded_updates[0])\n",
    "    for param in self.global_update:\n",
    "      param.data.zero_()\n",
    "\n",
    "    for w, client_update in zip(self.uploaded_weights, self.uploaded_updates):\n",
    "      self.add_parameters(w, client_update)\n",
    "\n",
    "    for model_param, update_param in zip(self.global_model.parameters(), self.global_update):\n",
    "      model_param.data += update_param.data.clone()\n",
    "\n",
    "  def collect(self):\n",
    "    clients_updates = []\n",
    "    for client in self.training_clients:\n",
    "      clients_updates.append(client.get_update(self.global_model))\n",
    "\n",
    "    clients_updates = [torch.cat([uu.reshape(-1, 1) for uu in u], axis=0).detach().cpu().numpy().squeeze() for u in clients_updates]\n",
    "    return clients_updates\n",
    "\n",
    "  def cluster(self, clients_updates):\n",
    "    clustering = AgglomerativeClustering(n_clusters=self.n_clusters, metric=self.metric, linkage=self.linkage).fit(clients_updates)\n",
    "    return clustering.labels_\n",
    "\n",
    "  def send_models_g(self):\n",
    "    for client in self.selected_clients:\n",
    "      c_idx = list(self.training_clients_ids).index(client.id)\n",
    "      client.set_parameters(self.group_models[self.cluster_identity[c_idx]])\n",
    "\n",
    "  def receive_models_g(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    for client in self.selected_clients:\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      c_idx = list(self.training_clients_ids).index(client.id)\n",
    "      self.uploaded_updates.append([c_param.data - s_param.data for c_param, s_param in zip(client.model.parameters(), self.group_models[self.cluster_identity[c_idx]].parameters())])\n",
    "\n",
    "  def aggregate_parameters_g(self, g_epochs=0):\n",
    "    for i in range(len(self.group_models)):\n",
    "      self.global_update = copy.deepcopy(self.uploaded_updates[0])\n",
    "      for param in self.global_update:\n",
    "        param.data.zero_()\n",
    "\n",
    "      user_idx_in_same_group = np.array([r_id for r_id, c_id in enumerate(self.uploaded_ids) if self.cluster_identity[list(self.training_clients_ids).index(c_id)] == i])\n",
    "      uploaded_weights = [self.uploaded_weights[u_id] for u_id in range(len(self.uploaded_weights)) if u_id in user_idx_in_same_group]\n",
    "      uploaded_weights = [weight / sum(uploaded_weights) for weight in uploaded_weights]\n",
    "      uploaded_updates = [self.uploaded_updates[u_id] for u_id in range(len(self.uploaded_updates)) if u_id in user_idx_in_same_group]\n",
    "\n",
    "      for w, client_update in zip(uploaded_weights, uploaded_updates):\n",
    "        self.add_parameters(w, client_update)\n",
    "\n",
    "      for model_param, update_param in zip(self.group_models[i].parameters(), self.global_update):\n",
    "        model_param.data += update_param.data.clone()\n",
    "\n",
    "      # save the group_models parameters\n",
    "      name_path = f'Refused_FL/Model_Global_CHAR/{current_time}/Group_{i}'\n",
    "      # create if name_path is not exist\n",
    "      if not os.path.exists(name_path):\n",
    "        os.makedirs(name_path)\n",
    "      torch.save(self.group_models[i].state_dict(), f\"{name_path}/cluster_model_round_{g_epochs}.pth\")\n",
    "      # torch.save(self.group_models[i].state_dict(), f\"{name_path}/cluster_model_round_{g_epochs}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating server and clients ...\n",
      "CNNFeatureExtractor(\n",
      "  (conv1): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3072, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Join ratio / total clients: 1.0 / 54\n",
      "Finished creating server and clients.\n",
      "\n",
      "-------------Round number: 0-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.40\n",
      "Averaged Test Accurancy: 26.71%\n",
      "Std Test Accurancy: 2.55%\n",
      "\n",
      "-------------Round number: 1-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.41\n",
      "Averaged Test Accurancy: 27.44%\n",
      "Std Test Accurancy: 2.67%\n",
      "\n",
      "-------------Round number: 2-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.42\n",
      "Averaged Test Accurancy: 27.66%\n",
      "Std Test Accurancy: 2.98%\n",
      "\n",
      "-------------Round number: 3-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.42\n",
      "Averaged Test Accurancy: 28.17%\n",
      "Std Test Accurancy: 3.31%\n",
      "\n",
      "-------------Round number: 4-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.42\n",
      "Averaged Test Accurancy: 28.65%\n",
      "Std Test Accurancy: 3.37%\n",
      "\n",
      "-------------Round number: 5-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.42\n",
      "Averaged Test Accurancy: 28.99%\n",
      "Std Test Accurancy: 3.53%\n",
      "\n",
      "-------------Round number: 6-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 29.51%\n",
      "Std Test Accurancy: 3.65%\n",
      "\n",
      "-------------Round number: 7-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 29.21%\n",
      "Std Test Accurancy: 3.88%\n",
      "\n",
      "-------------Round number: 8-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 29.43%\n",
      "Std Test Accurancy: 4.06%\n",
      "\n",
      "-------------Round number: 9-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 29.87%\n",
      "Std Test Accurancy: 4.31%\n",
      "\n",
      "-------------Clustering-------------\n",
      "Cluster 0: [('Client', 3), ('Client', 6), ('Client', 7), ('Client', 24), ('Client', 25), ('Client', 27), ('Client', 35), ('Client', 43), ('Client', 45), ('Client', 48)]\n",
      "Cluster 1: [('Client', 1), ('Client', 2), ('Client', 16), ('Client', 20), ('Client', 33), ('Client', 37), ('Client', 39), ('Client', 41), ('Client', 46), ('Client', 47)]\n",
      "Cluster 2: [('Client', 5), ('Client', 9), ('Client', 10), ('Client', 11), ('Client', 13), ('Client', 15), ('Client', 19), ('Client', 21), ('Client', 26), ('Client', 29), ('Client', 30), ('Client', 38), ('Client', 40), ('Client', 44)]\n",
      "Cluster 3: [('Client', 4), ('Client', 14), ('Client', 23), ('Client', 28), ('Client', 31), ('Client', 32), ('Client', 36), ('Client', 42), ('Client', 49), ('Client', 50), ('Client', 53)]\n",
      "Cluster 4: [('Client', 12), ('Client', 18), ('Client', 22), ('Client', 34), ('Client', 51), ('Client', 52)]\n",
      "Cluster 5: [('Client', 0), ('Client', 8), ('Client', 17)]\n",
      "\n",
      "-------------Round number: 10-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 29.78%\n",
      "Std Test Accurancy: 4.35%\n",
      "\n",
      "-------------Round number: 11-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 30.40%\n",
      "Std Test Accurancy: 4.85%\n",
      "\n",
      "-------------Round number: 12-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.43\n",
      "Averaged Test Accurancy: 30.30%\n",
      "Std Test Accurancy: 4.68%\n",
      "\n",
      "-------------Round number: 13-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 30.79%\n",
      "Std Test Accurancy: 4.94%\n",
      "\n",
      "-------------Round number: 14-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.45\n",
      "Averaged Test Accurancy: 31.04%\n",
      "Std Test Accurancy: 4.86%\n",
      "\n",
      "-------------Round number: 15-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.45\n",
      "Averaged Test Accurancy: 31.37%\n",
      "Std Test Accurancy: 4.92%\n",
      "\n",
      "-------------Round number: 16-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.27%\n",
      "Std Test Accurancy: 5.18%\n",
      "\n",
      "-------------Round number: 17-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.10%\n",
      "Std Test Accurancy: 5.44%\n",
      "\n",
      "-------------Round number: 18-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.61%\n",
      "Std Test Accurancy: 5.53%\n",
      "\n",
      "-------------Round number: 19-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.24%\n",
      "Std Test Accurancy: 5.74%\n",
      "\n",
      "-------------Round number: 20-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.99%\n",
      "Std Test Accurancy: 6.33%\n",
      "\n",
      "-------------Round number: 21-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.62%\n",
      "Std Test Accurancy: 6.40%\n",
      "\n",
      "-------------Round number: 22-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 32.19%\n",
      "Std Test Accurancy: 6.23%\n",
      "\n",
      "-------------Round number: 23-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.77%\n",
      "Std Test Accurancy: 6.11%\n",
      "\n",
      "-------------Round number: 24-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 32.40%\n",
      "Std Test Accurancy: 6.22%\n",
      "\n",
      "-------------Round number: 25-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.44\n",
      "Averaged Test Accurancy: 31.89%\n",
      "Std Test Accurancy: 6.14%\n",
      "\n",
      "-------------Round number: 26-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Averaged Train Loss: 1.45\n",
      "Averaged Test Accurancy: 32.95%\n",
      "Std Test Accurancy: 6.84%\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "print(\"Creating server and clients ...\")\n",
    "start = time.time()\n",
    "model = CNNFeatureExtractor(num_classes=4).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "server = FedCHAR(model)\n",
    "cluster_member = server.train()\n",
    "print(f\"\\nTime cost: {round((time.time()-start)/60, 2)}min.\")\n",
    "print(f\"Cluster members: {cluster_member}\")\n",
    "print(f\"len cluster member: {len(cluster_member)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cluster member\n",
    "np.save(f'Refused_FL/Model_Global_CHAR/{current_time}/cluster_member.npy', cluster_member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fine-Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject id: 0, len: 11\n",
      "subject id: 1, len: 10\n",
      "subject id: 2, len: 11\n",
      "subject id: 3, len: 10\n",
      "subject id: 4, len: 12\n",
      "subject id: 5, len: 12\n",
      "subject id: 6, len: 2\n",
      "subject id: 7, len: 3\n",
      "subject id: 8, len: 1\n",
      "subject id: 9, len: 10\n",
      "subject id: 10, len: 8\n",
      "subject id: 11, len: 11\n",
      "subject id: 12, len: 10\n",
      "subject id: 13, len: 12\n",
      "subject id: 14, len: 10\n",
      "subject id: 15, len: 9\n",
      "subject id: 16, len: 12\n",
      "subject id: 17, len: 11\n",
      "subject id: 18, len: 10\n",
      "subject id: 19, len: 11\n",
      "subject id: 20, len: 10\n",
      "subject id: 21, len: 10\n",
      "subject id: 22, len: 8\n",
      "subject id: 23, len: 11\n",
      "subject id: 24, len: 1\n",
      "subject id: 25, len: 1\n",
      "subject id: 26, len: 1\n",
      "subject id: 27, len: 1\n",
      "subject id: 28, len: 1\n",
      "subject id: 29, len: 1\n",
      "subject id: 30, len: 1\n",
      "subject id: 31, len: 1\n",
      "subject id: 32, len: 1\n",
      "subject id: 33, len: 1\n",
      "subject id: 34, len: 1\n",
      "subject id: 35, len: 1\n",
      "subject id: 36, len: 1\n",
      "subject id: 37, len: 1\n",
      "subject id: 38, len: 1\n",
      "subject id: 39, len: 1\n",
      "subject id: 40, len: 1\n",
      "subject id: 41, len: 1\n",
      "subject id: 42, len: 1\n",
      "subject id: 43, len: 1\n",
      "subject id: 44, len: 1\n",
      "subject id: 45, len: 1\n",
      "subject id: 46, len: 1\n",
      "subject id: 47, len: 1\n",
      "subject id: 48, len: 1\n",
      "subject id: 49, len: 1\n",
      "subject id: 50, len: 1\n",
      "subject id: 51, len: 1\n",
      "subject id: 52, len: 1\n",
      "subject id: 53, len: 1\n"
     ]
    }
   ],
   "source": [
    "labelled_data = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    data_label = load_data_client(id= i, batch_size=batch_size, type='labelled_train')\n",
    "    print(f\"subject id: {i}, len: {len(data_label)}\")\n",
    "    labelled_data.append(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined labelled: 255\n"
     ]
    }
   ],
   "source": [
    "# combine all client labelled data into one\n",
    "combined_labelled_data = []\n",
    "combined_labelled_labels = []\n",
    "for i in range(num_clients):\n",
    "    for data, labels in labelled_data[i]:\n",
    "        combined_labelled_data.append(data)\n",
    "        combined_labelled_labels.append(labels)\n",
    "combined_labelled_data = torch.cat(combined_labelled_data, dim=0)\n",
    "combined_labelled_labels = torch.cat(combined_labelled_labels, dim=0)\n",
    "# create dataset and dataloader\n",
    "combined_labelled_dataset = torch.utils.data.TensorDataset(combined_labelled_data, combined_labelled_labels)\n",
    "combined_labelled_dataloader = torch.utils.data.DataLoader(combined_labelled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"combined labelled: {len(combined_labelled_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject id: 0, len: 14\n",
      "subject id: 1, len: 14\n",
      "subject id: 2, len: 15\n",
      "subject id: 3, len: 13\n",
      "subject id: 4, len: 16\n",
      "subject id: 5, len: 16\n",
      "subject id: 6, len: 4\n",
      "subject id: 7, len: 4\n",
      "subject id: 8, len: 3\n",
      "subject id: 9, len: 14\n",
      "subject id: 10, len: 11\n",
      "subject id: 11, len: 15\n",
      "subject id: 12, len: 13\n",
      "subject id: 13, len: 16\n",
      "subject id: 14, len: 14\n",
      "subject id: 15, len: 12\n",
      "subject id: 16, len: 16\n",
      "subject id: 17, len: 14\n",
      "subject id: 18, len: 14\n",
      "subject id: 19, len: 14\n",
      "subject id: 20, len: 14\n",
      "subject id: 21, len: 14\n",
      "subject id: 22, len: 12\n",
      "subject id: 23, len: 14\n",
      "subject id: 24, len: 2\n",
      "subject id: 25, len: 2\n",
      "subject id: 26, len: 2\n",
      "subject id: 27, len: 2\n",
      "subject id: 28, len: 2\n",
      "subject id: 29, len: 2\n",
      "subject id: 30, len: 2\n",
      "subject id: 31, len: 2\n",
      "subject id: 32, len: 2\n",
      "subject id: 33, len: 2\n",
      "subject id: 34, len: 2\n",
      "subject id: 35, len: 2\n",
      "subject id: 36, len: 2\n",
      "subject id: 37, len: 2\n",
      "subject id: 38, len: 2\n",
      "subject id: 39, len: 2\n",
      "subject id: 40, len: 2\n",
      "subject id: 41, len: 2\n",
      "subject id: 42, len: 2\n",
      "subject id: 43, len: 2\n",
      "subject id: 44, len: 2\n",
      "subject id: 45, len: 2\n",
      "subject id: 46, len: 2\n",
      "subject id: 47, len: 2\n",
      "subject id: 48, len: 2\n",
      "subject id: 49, len: 2\n",
      "subject id: 50, len: 2\n",
      "subject id: 51, len: 2\n",
      "subject id: 52, len: 2\n",
      "subject id: 53, len: 2\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    test = load_data_client(id= i, batch_size=batch_size, type='test')\n",
    "    print(f\"subject id: {i}, len: {len(test)}\")\n",
    "    test_data.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 0, 1, 0, 2, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2,\n",
       "       1, 1, 2, 2, 1, 0, 0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cluster member\n",
    "# cluster_member = np.load(f'Refused_FL/Model_Global_CHAR/2024-02-12_18-10-23/cluster_member.npy')\n",
    "\n",
    "cluster_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined test for cluster 0: 136 batches\n",
      "Combined test for cluster 1: 80 batches\n",
      "Combined test for cluster 2: 125 batches\n"
     ]
    }
   ],
   "source": [
    "test_loader_clusters = [None for _ in range(n_clusters)]\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    combined_test_data = []\n",
    "    combined_test_labels = []\n",
    "    for i in range(num_clients):\n",
    "        if c == cluster_member[i]:\n",
    "            # Assuming 'test_data[i]' is a DataLoader or similar iterable\n",
    "            for data, labels in test_data[i]:\n",
    "                combined_test_data.append(data)\n",
    "                combined_test_labels.append(labels)\n",
    "    # Combine the data and labels tensors\n",
    "    combined_test_data = torch.cat(combined_test_data, dim=0)\n",
    "    combined_test_labels = torch.cat(combined_test_labels, dim=0)\n",
    "    # Create dataset and dataloader for the combined data\n",
    "    combined_test_dataset = torch.utils.data.TensorDataset(combined_test_data, combined_test_labels)\n",
    "    combined_test_dataloader = torch.utils.data.DataLoader(combined_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Store the combined dataloader for the current cluster\n",
    "    test_loader_clusters[c] = combined_test_dataloader\n",
    "\n",
    "    print(f\"Combined test for cluster {c}: {len(combined_test_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all client labelled data into one\n",
    "# combined_test_data = []\n",
    "# combined_test_labels = []\n",
    "# for i in range(num_clients):\n",
    "#     for data, labels in test_data[i]:\n",
    "#         combined_test_data.append(data)\n",
    "#         combined_test_labels.append(labels)\n",
    "# combined_test_data = torch.cat(combined_test_data, dim=0)\n",
    "# combined_test_labels = torch.cat(combined_test_labels, dim=0)\n",
    "# # create dataset and dataloader\n",
    "# combined_test_dataset = torch.utils.data.TensorDataset(combined_test_data, combined_test_labels)\n",
    "# combined_test_dataloader = torch.utils.data.DataLoader(combined_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# print(f\"combined test: {len(combined_test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each class\n",
    "class_counts = torch.zeros(9)  # num_classes should be defined based on your dataset\n",
    "for _, target in combined_labelled_dataloader:\n",
    "    class_counts += torch.bincount(target, minlength=9)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts += 1  # Add 1 to each class count to avoid division by zero\n",
    "c_weight = 1. / class_counts\n",
    "c_weight = c_weight / c_weight.sum() * num_classes\n",
    "c_weight = c_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.5700e+02, 1.1640e+03, 1.2580e+03, 1.4310e+03, 8.5600e+02, 3.2600e+02,\n",
       "        1.0000e+00, 8.7100e+02, 1.3050e+03])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1402e-03, 3.4039e-03, 3.1496e-03, 2.7688e-03, 4.6287e-03, 1.2154e-02,\n",
       "        3.9622e+00, 4.5490e-03, 3.0361e-03], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model_path = 'Refused_FL/Model_Global/2024-02-11_22-01-36/global_model_round_199.pth' # model with 200 epochs, 1 local epoch\n",
    "# model = CNNFeatureExtractor(num_classes=4)\n",
    "\n",
    "# #load pretrained model\n",
    "# model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "# # # Freezing layers up to conv3\n",
    "# # for name, param in model.named_parameters():\n",
    "# #     if 'conv3' in name:\n",
    "# #         break\n",
    "# #     param.requires_grad = False\n",
    "\n",
    "# # # Unfreeze layers from conv3 onwards\n",
    "# # unfreeze = False\n",
    "# # for name, param in model.named_parameters():\n",
    "# #     if 'conv3' in name:\n",
    "# #         unfreeze = True\n",
    "# #     if unfreeze:\n",
    "# #         param.requires_grad = True\n",
    "\n",
    "# model.fc2 = nn.Linear(in_features=model.fc2.in_features, out_features=num_classes)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNNFeatureExtractor(num_classes=num_classes)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to test the model and get the accuracy and f1 score\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f'Accuracy: {accuracy}, F1 Score: {f1}')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, test_loader, num_epochs=200):\n",
    "    # Assuming class weights are calculated and provided as `class_weights`\n",
    "    class_weights = torch.tensor(c_weight).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.Adam(model.fc2.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc, f1 = test_model(model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Accuracy: {acc}, F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune_model(model.to(device), combined_labelled_dataloader, combined_test_dataloader,num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0\n",
      "**************************************************\n",
      "Accuracy: 0.30974264705882354, F1 Score: 0.241695078433261\n",
      "Epoch 1/20, Loss: 1.7096428871154785, Accuracy: 0.30974264705882354, F1 Score: 0.241695078433261\n",
      "Accuracy: 0.35110294117647056, F1 Score: 0.30333319129214387\n",
      "Epoch 2/20, Loss: 1.662732481956482, Accuracy: 0.35110294117647056, F1 Score: 0.30333319129214387\n",
      "Accuracy: 0.3538602941176471, F1 Score: 0.2956520925607204\n",
      "Epoch 3/20, Loss: 1.5077474117279053, Accuracy: 0.3538602941176471, F1 Score: 0.2956520925607204\n",
      "Accuracy: 0.3616727941176471, F1 Score: 0.3307271900536811\n",
      "Epoch 4/20, Loss: 1.7691786289215088, Accuracy: 0.3616727941176471, F1 Score: 0.3307271900536811\n",
      "Accuracy: 0.35592830882352944, F1 Score: 0.32118130670725203\n",
      "Epoch 5/20, Loss: 1.400025486946106, Accuracy: 0.35592830882352944, F1 Score: 0.32118130670725203\n",
      "Accuracy: 0.3336397058823529, F1 Score: 0.2944887123044251\n",
      "Epoch 6/20, Loss: 1.692155122756958, Accuracy: 0.3336397058823529, F1 Score: 0.2944887123044251\n",
      "Accuracy: 0.36833639705882354, F1 Score: 0.3403810172193278\n",
      "Epoch 7/20, Loss: 1.6675642728805542, Accuracy: 0.36833639705882354, F1 Score: 0.3403810172193278\n",
      "Accuracy: 0.37362132352941174, F1 Score: 0.340249862359342\n",
      "Epoch 8/20, Loss: 1.4162379503250122, Accuracy: 0.37362132352941174, F1 Score: 0.340249862359342\n",
      "Accuracy: 0.35661764705882354, F1 Score: 0.3423154948336722\n",
      "Epoch 9/20, Loss: 1.400783896446228, Accuracy: 0.35661764705882354, F1 Score: 0.3423154948336722\n",
      "Accuracy: 0.34420955882352944, F1 Score: 0.3305058734869976\n",
      "Epoch 10/20, Loss: 1.4803075790405273, Accuracy: 0.34420955882352944, F1 Score: 0.3305058734869976\n",
      "Accuracy: 0.3396139705882353, F1 Score: 0.3134830352785738\n",
      "Epoch 11/20, Loss: 1.2144103050231934, Accuracy: 0.3396139705882353, F1 Score: 0.3134830352785738\n",
      "Accuracy: 0.33938419117647056, F1 Score: 0.3205001295446167\n",
      "Epoch 12/20, Loss: 1.111554741859436, Accuracy: 0.33938419117647056, F1 Score: 0.3205001295446167\n",
      "Accuracy: 0.3396139705882353, F1 Score: 0.31874203977502363\n",
      "Epoch 13/20, Loss: 1.125741720199585, Accuracy: 0.3396139705882353, F1 Score: 0.31874203977502363\n",
      "Accuracy: 0.3311121323529412, F1 Score: 0.3190120252733363\n",
      "Epoch 14/20, Loss: 1.0131090879440308, Accuracy: 0.3311121323529412, F1 Score: 0.3190120252733363\n",
      "Accuracy: 0.3352481617647059, F1 Score: 0.3270187532703063\n",
      "Epoch 15/20, Loss: 0.5853954553604126, Accuracy: 0.3352481617647059, F1 Score: 0.3270187532703063\n",
      "Accuracy: 0.33318014705882354, F1 Score: 0.3107556278724896\n",
      "Epoch 16/20, Loss: 0.43671315908432007, Accuracy: 0.33318014705882354, F1 Score: 0.3107556278724896\n",
      "Accuracy: 0.31824448529411764, F1 Score: 0.3089138409495172\n",
      "Epoch 17/20, Loss: 0.670244574546814, Accuracy: 0.31824448529411764, F1 Score: 0.3089138409495172\n",
      "Accuracy: 0.3219209558823529, F1 Score: 0.3113621563395222\n",
      "Epoch 18/20, Loss: 0.4173767566680908, Accuracy: 0.3219209558823529, F1 Score: 0.3113621563395222\n",
      "Accuracy: 0.32306985294117646, F1 Score: 0.3030099154557453\n",
      "Epoch 19/20, Loss: 0.6232060194015503, Accuracy: 0.32306985294117646, F1 Score: 0.3030099154557453\n",
      "Accuracy: 0.3102022058823529, F1 Score: 0.3120005279501669\n",
      "Epoch 20/20, Loss: 0.36695095896720886, Accuracy: 0.3102022058823529, F1 Score: 0.3120005279501669\n",
      "**************************************************\n",
      "Cluster: 1\n",
      "**************************************************\n",
      "Accuracy: 0.2170846394984326, F1 Score: 0.19044017564233617\n",
      "Epoch 1/20, Loss: 1.8995532989501953, Accuracy: 0.2170846394984326, F1 Score: 0.19044017564233617\n",
      "Accuracy: 0.21669278996865204, F1 Score: 0.1980830235612961\n",
      "Epoch 2/20, Loss: 1.543095350265503, Accuracy: 0.21669278996865204, F1 Score: 0.1980830235612961\n",
      "Accuracy: 0.25156739811912227, F1 Score: 0.2159354579542308\n",
      "Epoch 3/20, Loss: 1.5203436613082886, Accuracy: 0.25156739811912227, F1 Score: 0.2159354579542308\n",
      "Accuracy: 0.24960815047021945, F1 Score: 0.20689609656462057\n",
      "Epoch 4/20, Loss: 1.4441747665405273, Accuracy: 0.24960815047021945, F1 Score: 0.20689609656462057\n",
      "Accuracy: 0.26959247648902823, F1 Score: 0.23435372965753054\n",
      "Epoch 5/20, Loss: 1.3693206310272217, Accuracy: 0.26959247648902823, F1 Score: 0.23435372965753054\n",
      "Accuracy: 0.2629310344827586, F1 Score: 0.24221582974733874\n",
      "Epoch 6/20, Loss: 1.6606553792953491, Accuracy: 0.2629310344827586, F1 Score: 0.24221582974733874\n",
      "Accuracy: 0.265282131661442, F1 Score: 0.23176698731968984\n",
      "Epoch 7/20, Loss: 1.353556513786316, Accuracy: 0.265282131661442, F1 Score: 0.23176698731968984\n",
      "Accuracy: 0.2582288401253918, F1 Score: 0.24057371743467584\n",
      "Epoch 8/20, Loss: 1.225877046585083, Accuracy: 0.2582288401253918, F1 Score: 0.24057371743467584\n",
      "Accuracy: 0.24960815047021945, F1 Score: 0.2212399410771181\n",
      "Epoch 9/20, Loss: 1.448707938194275, Accuracy: 0.24960815047021945, F1 Score: 0.2212399410771181\n",
      "Accuracy: 0.27037617554858934, F1 Score: 0.2391826309475981\n",
      "Epoch 10/20, Loss: 1.2051926851272583, Accuracy: 0.27037617554858934, F1 Score: 0.2391826309475981\n",
      "Accuracy: 0.2644984326018809, F1 Score: 0.25220293992029247\n",
      "Epoch 11/20, Loss: 1.0882762670516968, Accuracy: 0.2644984326018809, F1 Score: 0.25220293992029247\n",
      "Accuracy: 0.2582288401253918, F1 Score: 0.2427325137396033\n",
      "Epoch 12/20, Loss: 1.021801471710205, Accuracy: 0.2582288401253918, F1 Score: 0.2427325137396033\n",
      "Accuracy: 0.27037617554858934, F1 Score: 0.25637846951726145\n",
      "Epoch 13/20, Loss: 1.045379877090454, Accuracy: 0.27037617554858934, F1 Score: 0.25637846951726145\n",
      "Accuracy: 0.2554858934169279, F1 Score: 0.2355764023782998\n",
      "Epoch 14/20, Loss: 0.8286210894584656, Accuracy: 0.2554858934169279, F1 Score: 0.2355764023782998\n",
      "Accuracy: 0.2566614420062696, F1 Score: 0.24381030306053686\n",
      "Epoch 15/20, Loss: 0.9612871408462524, Accuracy: 0.2566614420062696, F1 Score: 0.24381030306053686\n",
      "Accuracy: 0.2617554858934169, F1 Score: 0.24356807720814078\n",
      "Epoch 16/20, Loss: 0.8977862596511841, Accuracy: 0.2617554858934169, F1 Score: 0.24356807720814078\n",
      "Accuracy: 0.26880877742946707, F1 Score: 0.24635521685366946\n",
      "Epoch 17/20, Loss: 0.6687601804733276, Accuracy: 0.26880877742946707, F1 Score: 0.24635521685366946\n",
      "Accuracy: 0.27155172413793105, F1 Score: 0.2540475097789478\n",
      "Epoch 18/20, Loss: 0.6201788783073425, Accuracy: 0.27155172413793105, F1 Score: 0.2540475097789478\n",
      "Accuracy: 0.2664576802507837, F1 Score: 0.2537032938606242\n",
      "Epoch 19/20, Loss: 0.7331121563911438, Accuracy: 0.2664576802507837, F1 Score: 0.2537032938606242\n",
      "Accuracy: 0.2358934169278997, F1 Score: 0.23387425568129244\n",
      "Epoch 20/20, Loss: 0.3853503465652466, Accuracy: 0.2358934169278997, F1 Score: 0.23387425568129244\n",
      "**************************************************\n",
      "Cluster: 2\n",
      "**************************************************\n",
      "Accuracy: 0.42383939774153073, F1 Score: 0.383639845379104\n",
      "Epoch 1/20, Loss: 1.7160016298294067, Accuracy: 0.42383939774153073, F1 Score: 0.383639845379104\n",
      "Accuracy: 0.465495608531995, F1 Score: 0.4285479599312116\n",
      "Epoch 2/20, Loss: 1.5778462886810303, Accuracy: 0.465495608531995, F1 Score: 0.4285479599312116\n",
      "Accuracy: 0.4215809284818068, F1 Score: 0.404841303302034\n",
      "Epoch 3/20, Loss: 1.5891287326812744, Accuracy: 0.4215809284818068, F1 Score: 0.404841303302034\n",
      "Accuracy: 0.4619824341279799, F1 Score: 0.4424024094515306\n",
      "Epoch 4/20, Loss: 1.6236211061477661, Accuracy: 0.4619824341279799, F1 Score: 0.4424024094515306\n",
      "Accuracy: 0.4740276035131744, F1 Score: 0.45022363075777005\n",
      "Epoch 5/20, Loss: 1.3326067924499512, Accuracy: 0.4740276035131744, F1 Score: 0.45022363075777005\n",
      "Accuracy: 0.4757841907151819, F1 Score: 0.4528009861677776\n",
      "Epoch 6/20, Loss: 1.5752218961715698, Accuracy: 0.4757841907151819, F1 Score: 0.4528009861677776\n",
      "Accuracy: 0.45395232120451695, F1 Score: 0.44401344303555484\n",
      "Epoch 7/20, Loss: 1.2865862846374512, Accuracy: 0.45395232120451695, F1 Score: 0.44401344303555484\n",
      "Accuracy: 0.490840652446675, F1 Score: 0.47443534862694947\n",
      "Epoch 8/20, Loss: 0.9412851929664612, Accuracy: 0.490840652446675, F1 Score: 0.47443534862694947\n",
      "Accuracy: 0.48306148055207027, F1 Score: 0.4580219119587399\n",
      "Epoch 9/20, Loss: 1.218238353729248, Accuracy: 0.48306148055207027, F1 Score: 0.4580219119587399\n",
      "Accuracy: 0.42383939774153073, F1 Score: 0.4086708615750344\n",
      "Epoch 10/20, Loss: 1.2793281078338623, Accuracy: 0.42383939774153073, F1 Score: 0.4086708615750344\n",
      "Accuracy: 0.4258469259723965, F1 Score: 0.41085679864312386\n",
      "Epoch 11/20, Loss: 1.0549386739730835, Accuracy: 0.4258469259723965, F1 Score: 0.41085679864312386\n",
      "Accuracy: 0.4283563362609787, F1 Score: 0.4248305794299252\n",
      "Epoch 12/20, Loss: 1.0579053163528442, Accuracy: 0.4283563362609787, F1 Score: 0.4248305794299252\n",
      "Accuracy: 0.42785445420326224, F1 Score: 0.420226890204763\n",
      "Epoch 13/20, Loss: 1.1587047576904297, Accuracy: 0.42785445420326224, F1 Score: 0.420226890204763\n",
      "Accuracy: 0.4062735257214555, F1 Score: 0.41953401105050836\n",
      "Epoch 14/20, Loss: 0.718822717666626, Accuracy: 0.4062735257214555, F1 Score: 0.41953401105050836\n",
      "Accuracy: 0.416060225846926, F1 Score: 0.416647112760597\n",
      "Epoch 15/20, Loss: 0.6677860617637634, Accuracy: 0.416060225846926, F1 Score: 0.416647112760597\n",
      "Accuracy: 0.4035131744040151, F1 Score: 0.40541577291912545\n",
      "Epoch 16/20, Loss: 0.586542010307312, Accuracy: 0.4035131744040151, F1 Score: 0.40541577291912545\n",
      "Accuracy: 0.3987452948557089, F1 Score: 0.40926979169751404\n",
      "Epoch 17/20, Loss: 0.5069521069526672, Accuracy: 0.3987452948557089, F1 Score: 0.40926979169751404\n",
      "Accuracy: 0.4163111668757842, F1 Score: 0.42337629318446657\n",
      "Epoch 18/20, Loss: 0.53758704662323, Accuracy: 0.4163111668757842, F1 Score: 0.42337629318446657\n",
      "Accuracy: 0.4, F1 Score: 0.39906331077292373\n",
      "Epoch 19/20, Loss: 0.3645066022872925, Accuracy: 0.4, F1 Score: 0.39906331077292373\n",
      "Accuracy: 0.4145545796737767, F1 Score: 0.41851399961450875\n",
      "Epoch 20/20, Loss: 0.3900998830795288, Accuracy: 0.4145545796737767, F1 Score: 0.41851399961450875\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    pretrained_model_path = f'Refused_FL/Model_Global_CHAR/2024-02-12_23-26-27/Group_{i}/cluster_model_round_49.pth'\n",
    "    model = CNNFeatureExtractor(num_classes=4)\n",
    "    #load pretrained model\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "    # Freezing layers up to conv3\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv3' in name:\n",
    "            break\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze layers from conv3 onwards\n",
    "    unfreeze = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv3' in name:\n",
    "            unfreeze = True\n",
    "        if unfreeze:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.fc2 = nn.Linear(in_features=model.fc2.in_features, out_features=9)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Cluster: {i}\")\n",
    "    print(\"*\" * 50)\n",
    "    fine_tune_model(model.to(device), combined_labelled_dataloader, test_loader_clusters[i],num_epochs=20)\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3629350720911011, F1 Score: 0.35672745069023415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3629350720911011, 0.35672745069023415)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_model(model, combined_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
