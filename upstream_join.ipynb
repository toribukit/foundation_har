{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data in one dataframe row-wise\n",
    "data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7487254, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.487254e+06</td>\n",
       "      <td>7.487254e+06</td>\n",
       "      <td>7.487254e+06</td>\n",
       "      <td>7.487254e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.618116e+00</td>\n",
       "      <td>-3.966821e+00</td>\n",
       "      <td>-2.498674e+00</td>\n",
       "      <td>2.437833e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.590949e+00</td>\n",
       "      <td>5.534619e+00</td>\n",
       "      <td>4.317720e+00</td>\n",
       "      <td>3.961563e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>-8.414867e+00</td>\n",
       "      <td>-4.598438e+00</td>\n",
       "      <td>-1.527778e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>-4.905000e+00</td>\n",
       "      <td>-1.962570e+00</td>\n",
       "      <td>1.839375e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>-1.250000e-02</td>\n",
       "      <td>5.518125e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "      <td>3.310875e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id         acc_x         acc_y         acc_z\n",
       "count  7.487254e+06  7.487254e+06  7.487254e+06  7.487254e+06\n",
       "mean   9.618116e+00 -3.966821e+00 -2.498674e+00  2.437833e+00\n",
       "std    6.590949e+00  5.534619e+00  4.317720e+00  3.961563e+00\n",
       "min    0.000000e+00 -3.924000e+01 -3.924000e+01 -3.924000e+01\n",
       "25%    4.000000e+00 -8.414867e+00 -4.598438e+00 -1.527778e-02\n",
       "50%    9.000000e+00 -4.905000e+00 -1.962570e+00  1.839375e+00\n",
       "75%    1.400000e+01  8.000000e-01 -1.250000e-02  5.518125e+00\n",
       "max    2.900000e+01  3.893344e+01  3.310875e+01  3.893344e+01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id    0\n",
       "acc_x         0\n",
       "acc_y         0\n",
       "acc_z         0\n",
       "activity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7487254, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of window dataset (2 sec with 0% overlap): (74872, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 2\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 0\n",
    "\n",
    "window_data, _ = sliding_window_samples(data, window_size, overlap_ratio)\n",
    "print(f\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "window_data = window_data[:, :, :-1]\n",
    "# window_data = window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "window_data = window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_noise_single_window(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param noise_level: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy data.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_single_window(data, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly crop a single window of data and pad it to maintain original shape.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param crop_ratio: Ratio of the original window size to keep.\n",
    "    :return: Cropped and padded data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    new_size = int(window_size * crop_ratio)\n",
    "    start = np.random.randint(0, window_size - new_size)\n",
    "    end = start + new_size\n",
    "    cropped_data = data[start:end, :]\n",
    "\n",
    "    # Pad the cropped data to maintain original window size\n",
    "    padding_size = window_size - new_size\n",
    "    padding = np.zeros((padding_size, data.shape[1]))\n",
    "    padded_data = np.vstack((cropped_data, padding))\n",
    "\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of warping.\n",
    "    :return: Warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.copy(data)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        time_points = np.linspace(0, 1, window_size)\n",
    "        random_points = np.linspace(0, 1, np.random.randint(4, 10))\n",
    "        warp_values = 1 + np.random.normal(0, warp_factor, random_points.size)\n",
    "        interpolator = CubicSpline(random_points, warp_values)\n",
    "        warped_data[:, j] *= interpolator(time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply time warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of time warping.\n",
    "    :return: Time-warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_points = np.linspace(0, 1, window_size)\n",
    "    random_points = np.sort(np.random.rand(np.random.randint(3, 6)))\n",
    "    warp_values = np.random.normal(1, warp_factor, random_points.size)\n",
    "    interpolator = CubicSpline(random_points, warp_values)\n",
    "    warped_time = interpolator(time_points)\n",
    "    warped_time -= warped_time.min()\n",
    "    warped_time /= warped_time.max()\n",
    "    warped_time *= (window_size - 1)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        interpolator = CubicSpline(np.arange(window_size), data[:, j])\n",
    "        warped_data[:, j] = interpolator(warped_time)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of noisy data: (100, 3)\n",
      "shape of cropped data: (100, 3)\n",
      "shape of warped data: (100, 3)\n",
      "shape of time warped data: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# add random noise\n",
    "noisy_data = add_random_noise_single_window(window_data[0], noise_level=0.1)\n",
    "print(f\"shape of noisy data: {noisy_data.shape}\")\n",
    "# random cropping\n",
    "cropped_data = random_cropping_single_window(window_data[0], crop_ratio=0.8)\n",
    "print(f\"shape of cropped data: {cropped_data.shape}\")\n",
    "# magnitude warping\n",
    "warped_data = magnitude_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of warped data: {warped_data.shape}\")\n",
    "# time warping\n",
    "time_warped_data = time_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of time warped data: {time_warped_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the original data\n",
    "augmented_data = np.copy(window_data)\n",
    "\n",
    "# create labels for augmented data\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "# loop over all windows\n",
    "for i in range(window_data.shape[0]):\n",
    "    # choose one number from 0 to 3\n",
    "    random_number = np.random.randint(0, 4)\n",
    "\n",
    "    if random_number == 0:\n",
    "        augmented_data[i] = add_random_noise_single_window(window_data[i], noise_level=0.1)\n",
    "        augmented_labels.append(0)\n",
    "    elif random_number == 1:\n",
    "        augmented_data[i] = random_cropping_single_window(window_data[i], crop_ratio=0.8)\n",
    "        augmented_labels.append(1)\n",
    "    elif random_number == 2:\n",
    "        augmented_data[i] = magnitude_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(2)\n",
    "    else:\n",
    "        augmented_data[i] = time_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18705, 1: 18526, 2: 18866, 3: 18775}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of augmented data: (74872, 100, 3)\n",
      "shape of augmented labels: 74872\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of augmented data: {augmented_data.shape}\")\n",
    "print(f\"shape of augmented labels: {len(augmented_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented data and labels\n",
    "np.save('./data_processing/augmented_data_join.npy', augmented_data)\n",
    "np.save('./data_processing/augmented_labels_join.npy', augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmented data and labels\n",
    "augmented_data = np.load('./data_processing/augmented_data_join.npy', allow_pickle=True)\n",
    "augmented_labels = np.load('./data_processing/augmented_labels_join.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74872, 100, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74872,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18705, 1: 18526, 2: 18866, 3: 18775}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of arrays to a single numpy array\n",
    "# if isinstance(augmented_data, list):\n",
    "#     augmented_data = np.stack(augmented_data)\n",
    "\n",
    "# # Ensure the data type is float32\n",
    "# augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "# augmented_labels_tensor = torch.from_numpy(np.array(augmented_labels)).long()\n",
    "\n",
    "# # split data into train and test set\n",
    "# train_size = int(0.8 * augmented_data.shape[0])\n",
    "# test_size = augmented_data.shape[0] - train_size\n",
    "# train_data, test_data = torch.utils.data.random_split(augmented_data, [train_size, test_size])\n",
    "# train_labels, test_labels = torch.utils.data.random_split(augmented_labels, [train_size, test_size])\n",
    "\n",
    "# # create train and test dataset\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # create train and test dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # check if GPU is available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming augmented_data and augmented_labels are numpy arrays\n",
    "augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "augmented_labels_tensor = torch.from_numpy(augmented_labels)\n",
    "#convert labels to long\n",
    "augmented_labels_tensor = augmented_labels_tensor.long()\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(0.8 * len(augmented_data_tensor))\n",
    "test_size = len(augmented_data_tensor) - train_size\n",
    "\n",
    "# Creating datasets\n",
    "dataset = TensorDataset(augmented_data_tensor, augmented_labels_tensor)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Function to extract tensors from Subset\n",
    "def extract_subset_data(subset, dataset):\n",
    "    return dataset.tensors[0][subset.indices], dataset.tensors[1][subset.indices]\n",
    "\n",
    "# Extract data and labels from train and test sets\n",
    "train_data, train_labels = extract_subset_data(train_dataset, dataset)\n",
    "test_data, test_labels = extract_subset_data(test_dataset, dataset)\n",
    "\n",
    "# create train and test TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_loader: 468\n",
      "shape of test_loader: 117\n"
     ]
    }
   ],
   "source": [
    "#  print the shape of train_loader and test_loader\n",
    "print(f\"shape of train_loader: {len(train_loader)}\")\n",
    "print(f\"shape of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    #calculate accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return running_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. Test Loss: {test_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}\")\n",
    "    return train_losses, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20.. Train Loss: 1.282.. Test Loss: 1.132.. Test Accuracy: 0.487\n",
      "Epoch: 2/20.. Train Loss: 1.010.. Test Loss: 0.943.. Test Accuracy: 0.494\n",
      "Epoch: 3/20.. Train Loss: 0.921.. Test Loss: 0.909.. Test Accuracy: 0.548\n",
      "Epoch: 4/20.. Train Loss: 0.898.. Test Loss: 0.892.. Test Accuracy: 0.533\n",
      "Epoch: 5/20.. Train Loss: 0.885.. Test Loss: 0.881.. Test Accuracy: 0.556\n",
      "Epoch: 6/20.. Train Loss: 0.875.. Test Loss: 0.871.. Test Accuracy: 0.584\n",
      "Epoch: 7/20.. Train Loss: 0.866.. Test Loss: 0.863.. Test Accuracy: 0.585\n",
      "Epoch: 8/20.. Train Loss: 0.858.. Test Loss: 0.856.. Test Accuracy: 0.596\n",
      "Epoch: 9/20.. Train Loss: 0.850.. Test Loss: 0.847.. Test Accuracy: 0.599\n",
      "Epoch: 10/20.. Train Loss: 0.842.. Test Loss: 0.838.. Test Accuracy: 0.594\n",
      "Epoch: 11/20.. Train Loss: 0.833.. Test Loss: 0.829.. Test Accuracy: 0.601\n",
      "Epoch: 12/20.. Train Loss: 0.825.. Test Loss: 0.822.. Test Accuracy: 0.599\n",
      "Epoch: 13/20.. Train Loss: 0.815.. Test Loss: 0.813.. Test Accuracy: 0.605\n",
      "Epoch: 14/20.. Train Loss: 0.806.. Test Loss: 0.808.. Test Accuracy: 0.593\n",
      "Epoch: 15/20.. Train Loss: 0.796.. Test Loss: 0.799.. Test Accuracy: 0.592\n",
      "Epoch: 16/20.. Train Loss: 0.787.. Test Loss: 0.789.. Test Accuracy: 0.606\n",
      "Epoch: 17/20.. Train Loss: 0.777.. Test Loss: 0.773.. Test Accuracy: 0.626\n",
      "Epoch: 18/20.. Train Loss: 0.767.. Test Loss: 0.801.. Test Accuracy: 0.577\n",
      "Epoch: 19/20.. Train Loss: 0.758.. Test Loss: 0.774.. Test Accuracy: 0.595\n",
      "Epoch: 20/20.. Train Loss: 0.751.. Test Loss: 0.752.. Test Accuracy: 0.650\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = CNNFeatureExtractor(num_classes=4)\n",
    "\n",
    "# move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 20\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "model_name = f\"cnn_feature_extractor_join_{timestamp}.pt\"\n",
    "torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Train Loss: 1.403.. Test Loss: 1.388.. Test Accuracy: 0.256\n",
      "Epoch: 2/100.. Train Loss: 1.395.. Test Loss: 1.386.. Test Accuracy: 0.260\n",
      "Epoch: 3/100.. Train Loss: 1.391.. Test Loss: 1.385.. Test Accuracy: 0.261\n",
      "Epoch: 4/100.. Train Loss: 1.390.. Test Loss: 1.384.. Test Accuracy: 0.264\n",
      "Epoch: 5/100.. Train Loss: 1.389.. Test Loss: 1.383.. Test Accuracy: 0.272\n",
      "Epoch: 6/100.. Train Loss: 1.386.. Test Loss: 1.383.. Test Accuracy: 0.266\n",
      "Epoch: 7/100.. Train Loss: 1.385.. Test Loss: 1.382.. Test Accuracy: 0.281\n",
      "Epoch: 8/100.. Train Loss: 1.384.. Test Loss: 1.381.. Test Accuracy: 0.283\n",
      "Epoch: 9/100.. Train Loss: 1.383.. Test Loss: 1.380.. Test Accuracy: 0.290\n",
      "Epoch: 10/100.. Train Loss: 1.383.. Test Loss: 1.380.. Test Accuracy: 0.276\n",
      "Epoch: 11/100.. Train Loss: 1.382.. Test Loss: 1.379.. Test Accuracy: 0.293\n",
      "Epoch: 12/100.. Train Loss: 1.380.. Test Loss: 1.379.. Test Accuracy: 0.276\n",
      "Epoch: 13/100.. Train Loss: 1.380.. Test Loss: 1.377.. Test Accuracy: 0.295\n",
      "Epoch: 14/100.. Train Loss: 1.378.. Test Loss: 1.376.. Test Accuracy: 0.301\n",
      "Epoch: 15/100.. Train Loss: 1.377.. Test Loss: 1.375.. Test Accuracy: 0.305\n",
      "Epoch: 16/100.. Train Loss: 1.376.. Test Loss: 1.374.. Test Accuracy: 0.290\n",
      "Epoch: 17/100.. Train Loss: 1.375.. Test Loss: 1.373.. Test Accuracy: 0.315\n",
      "Epoch: 18/100.. Train Loss: 1.372.. Test Loss: 1.371.. Test Accuracy: 0.320\n",
      "Epoch: 19/100.. Train Loss: 1.370.. Test Loss: 1.369.. Test Accuracy: 0.345\n",
      "Epoch: 20/100.. Train Loss: 1.368.. Test Loss: 1.365.. Test Accuracy: 0.336\n",
      "Epoch: 21/100.. Train Loss: 1.363.. Test Loss: 1.361.. Test Accuracy: 0.304\n",
      "Epoch: 22/100.. Train Loss: 1.356.. Test Loss: 1.350.. Test Accuracy: 0.378\n",
      "Epoch: 23/100.. Train Loss: 1.339.. Test Loss: 1.319.. Test Accuracy: 0.433\n",
      "Epoch: 24/100.. Train Loss: 1.270.. Test Loss: 1.171.. Test Accuracy: 0.465\n",
      "Epoch: 25/100.. Train Loss: 1.075.. Test Loss: 0.992.. Test Accuracy: 0.506\n",
      "Epoch: 26/100.. Train Loss: 0.980.. Test Loss: 0.939.. Test Accuracy: 0.527\n",
      "Epoch: 27/100.. Train Loss: 0.939.. Test Loss: 0.910.. Test Accuracy: 0.568\n",
      "Epoch: 28/100.. Train Loss: 0.912.. Test Loss: 0.893.. Test Accuracy: 0.570\n",
      "Epoch: 29/100.. Train Loss: 0.890.. Test Loss: 0.867.. Test Accuracy: 0.584\n",
      "Epoch: 30/100.. Train Loss: 0.867.. Test Loss: 0.852.. Test Accuracy: 0.561\n",
      "Epoch: 31/100.. Train Loss: 0.847.. Test Loss: 0.827.. Test Accuracy: 0.606\n",
      "Epoch: 32/100.. Train Loss: 0.828.. Test Loss: 0.806.. Test Accuracy: 0.611\n",
      "Epoch: 33/100.. Train Loss: 0.807.. Test Loss: 0.799.. Test Accuracy: 0.591\n",
      "Epoch: 34/100.. Train Loss: 0.787.. Test Loss: 0.769.. Test Accuracy: 0.623\n",
      "Epoch: 35/100.. Train Loss: 0.770.. Test Loss: 0.751.. Test Accuracy: 0.632\n",
      "Epoch: 36/100.. Train Loss: 0.752.. Test Loss: 0.729.. Test Accuracy: 0.650\n",
      "Epoch: 37/100.. Train Loss: 0.737.. Test Loss: 0.715.. Test Accuracy: 0.648\n",
      "Epoch: 38/100.. Train Loss: 0.725.. Test Loss: 0.708.. Test Accuracy: 0.645\n",
      "Epoch: 39/100.. Train Loss: 0.715.. Test Loss: 0.697.. Test Accuracy: 0.653\n",
      "Epoch: 40/100.. Train Loss: 0.707.. Test Loss: 0.687.. Test Accuracy: 0.667\n",
      "Epoch: 41/100.. Train Loss: 0.701.. Test Loss: 0.691.. Test Accuracy: 0.649\n",
      "Epoch: 42/100.. Train Loss: 0.694.. Test Loss: 0.684.. Test Accuracy: 0.649\n",
      "Epoch: 43/100.. Train Loss: 0.686.. Test Loss: 0.667.. Test Accuracy: 0.675\n",
      "Epoch: 44/100.. Train Loss: 0.681.. Test Loss: 0.659.. Test Accuracy: 0.679\n",
      "Epoch: 45/100.. Train Loss: 0.678.. Test Loss: 0.656.. Test Accuracy: 0.672\n",
      "Epoch: 46/100.. Train Loss: 0.670.. Test Loss: 0.670.. Test Accuracy: 0.670\n",
      "Epoch: 47/100.. Train Loss: 0.668.. Test Loss: 0.646.. Test Accuracy: 0.683\n",
      "Epoch: 48/100.. Train Loss: 0.663.. Test Loss: 0.672.. Test Accuracy: 0.654\n",
      "Epoch: 49/100.. Train Loss: 0.660.. Test Loss: 0.642.. Test Accuracy: 0.674\n",
      "Epoch: 50/100.. Train Loss: 0.657.. Test Loss: 0.658.. Test Accuracy: 0.671\n",
      "Epoch: 51/100.. Train Loss: 0.652.. Test Loss: 0.659.. Test Accuracy: 0.674\n",
      "Epoch: 52/100.. Train Loss: 0.646.. Test Loss: 0.637.. Test Accuracy: 0.684\n",
      "Epoch: 53/100.. Train Loss: 0.645.. Test Loss: 0.630.. Test Accuracy: 0.680\n",
      "Epoch: 54/100.. Train Loss: 0.641.. Test Loss: 0.624.. Test Accuracy: 0.684\n",
      "Epoch: 55/100.. Train Loss: 0.640.. Test Loss: 0.624.. Test Accuracy: 0.699\n",
      "Epoch: 56/100.. Train Loss: 0.638.. Test Loss: 0.631.. Test Accuracy: 0.691\n",
      "Epoch: 57/100.. Train Loss: 0.634.. Test Loss: 0.618.. Test Accuracy: 0.693\n",
      "Epoch: 58/100.. Train Loss: 0.630.. Test Loss: 0.616.. Test Accuracy: 0.703\n",
      "Epoch: 59/100.. Train Loss: 0.628.. Test Loss: 0.610.. Test Accuracy: 0.700\n",
      "Epoch: 60/100.. Train Loss: 0.627.. Test Loss: 0.612.. Test Accuracy: 0.705\n",
      "Epoch: 61/100.. Train Loss: 0.620.. Test Loss: 0.611.. Test Accuracy: 0.689\n",
      "Epoch: 62/100.. Train Loss: 0.620.. Test Loss: 0.605.. Test Accuracy: 0.705\n",
      "Epoch: 63/100.. Train Loss: 0.619.. Test Loss: 0.600.. Test Accuracy: 0.710\n",
      "Epoch: 64/100.. Train Loss: 0.615.. Test Loss: 0.606.. Test Accuracy: 0.690\n",
      "Epoch: 65/100.. Train Loss: 0.613.. Test Loss: 0.605.. Test Accuracy: 0.708\n",
      "Epoch: 66/100.. Train Loss: 0.611.. Test Loss: 0.605.. Test Accuracy: 0.706\n",
      "Epoch: 67/100.. Train Loss: 0.609.. Test Loss: 0.598.. Test Accuracy: 0.704\n",
      "Epoch: 68/100.. Train Loss: 0.607.. Test Loss: 0.597.. Test Accuracy: 0.712\n",
      "Epoch: 69/100.. Train Loss: 0.605.. Test Loss: 0.592.. Test Accuracy: 0.714\n",
      "Epoch: 70/100.. Train Loss: 0.603.. Test Loss: 0.590.. Test Accuracy: 0.708\n",
      "Epoch: 71/100.. Train Loss: 0.599.. Test Loss: 0.616.. Test Accuracy: 0.691\n",
      "Epoch: 72/100.. Train Loss: 0.597.. Test Loss: 0.611.. Test Accuracy: 0.700\n",
      "Epoch: 73/100.. Train Loss: 0.596.. Test Loss: 0.605.. Test Accuracy: 0.706\n",
      "Epoch: 74/100.. Train Loss: 0.595.. Test Loss: 0.610.. Test Accuracy: 0.704\n",
      "Epoch: 75/100.. Train Loss: 0.593.. Test Loss: 0.599.. Test Accuracy: 0.704\n",
      "Epoch: 76/100.. Train Loss: 0.590.. Test Loss: 0.579.. Test Accuracy: 0.712\n",
      "Epoch: 77/100.. Train Loss: 0.589.. Test Loss: 0.584.. Test Accuracy: 0.716\n",
      "Epoch: 78/100.. Train Loss: 0.586.. Test Loss: 0.587.. Test Accuracy: 0.714\n",
      "Epoch: 79/100.. Train Loss: 0.586.. Test Loss: 0.580.. Test Accuracy: 0.712\n",
      "Epoch: 80/100.. Train Loss: 0.583.. Test Loss: 0.578.. Test Accuracy: 0.716\n",
      "Epoch: 81/100.. Train Loss: 0.582.. Test Loss: 0.584.. Test Accuracy: 0.715\n",
      "Epoch: 82/100.. Train Loss: 0.578.. Test Loss: 0.577.. Test Accuracy: 0.709\n",
      "Epoch: 83/100.. Train Loss: 0.579.. Test Loss: 0.593.. Test Accuracy: 0.707\n",
      "Epoch: 84/100.. Train Loss: 0.577.. Test Loss: 0.574.. Test Accuracy: 0.721\n",
      "Epoch: 85/100.. Train Loss: 0.575.. Test Loss: 0.574.. Test Accuracy: 0.718\n",
      "Epoch: 86/100.. Train Loss: 0.572.. Test Loss: 0.588.. Test Accuracy: 0.710\n",
      "Epoch: 87/100.. Train Loss: 0.573.. Test Loss: 0.581.. Test Accuracy: 0.716\n",
      "Epoch: 88/100.. Train Loss: 0.570.. Test Loss: 0.571.. Test Accuracy: 0.725\n",
      "Epoch: 89/100.. Train Loss: 0.569.. Test Loss: 0.575.. Test Accuracy: 0.720\n",
      "Epoch: 90/100.. Train Loss: 0.568.. Test Loss: 0.566.. Test Accuracy: 0.723\n",
      "Epoch: 91/100.. Train Loss: 0.567.. Test Loss: 0.565.. Test Accuracy: 0.718\n",
      "Epoch: 92/100.. Train Loss: 0.565.. Test Loss: 0.564.. Test Accuracy: 0.720\n",
      "Epoch: 93/100.. Train Loss: 0.565.. Test Loss: 0.557.. Test Accuracy: 0.726\n",
      "Epoch: 94/100.. Train Loss: 0.562.. Test Loss: 0.561.. Test Accuracy: 0.724\n",
      "Epoch: 95/100.. Train Loss: 0.560.. Test Loss: 0.556.. Test Accuracy: 0.727\n",
      "Epoch: 96/100.. Train Loss: 0.559.. Test Loss: 0.556.. Test Accuracy: 0.725\n",
      "Epoch: 97/100.. Train Loss: 0.558.. Test Loss: 0.567.. Test Accuracy: 0.723\n",
      "Epoch: 98/100.. Train Loss: 0.557.. Test Loss: 0.552.. Test Accuracy: 0.726\n",
      "Epoch: 99/100.. Train Loss: 0.556.. Test Loss: 0.550.. Test Accuracy: 0.730\n",
      "Epoch: 100/100.. Train Loss: 0.556.. Test Loss: 0.563.. Test Accuracy: 0.714\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "model = TCN(num_inputs, num_channels, kernel_size, num_classes=4).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00003)  # Replace lr with your learning rate\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 100\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "model_name = f\"tcn_join_{timestamp}.pt\"\n",
    "torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
