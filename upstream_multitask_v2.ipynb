{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from scipy.fftpack import fft, ifft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 64  # Set your batch size\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  22\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data1['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  15\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data2['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects:  30\n"
     ]
    }
   ],
   "source": [
    "#count number of unique subjects\n",
    "print(\"Number of unique subjects: \", data3['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique subjects:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "#print all of the unique subjects\n",
    "print(\"Unique subjects: \", data3['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert subject_id to int\n",
    "data3['subject_id'] = data3['subject_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # join all data in one dataframe row-wise\n",
    "# data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)\n",
    "data = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values in subject_id column\n",
    "data['subject_id'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subjects:  [ 9  7 26 29  1 24]\n",
      "Train data shape:  (897667, 5)\n",
      "Test data shape:  (225105, 5)\n"
     ]
    }
   ],
   "source": [
    "#split train and test data\n",
    "#randomly select 20% of subjects for test data\n",
    "test_subjects = data['subject_id'].unique()\n",
    "test_subjects = np.random.choice(test_subjects, size=int(0.2*len(test_subjects)), replace=False)\n",
    "# test_subjects = [ 9  7 26 29  1 24]\n",
    "print(\"Test subjects: \", test_subjects)\n",
    "\n",
    "#split data into train and test\n",
    "train_data = data[~data['subject_id'].isin(test_subjects)]\n",
    "test_data = data[data['subject_id'].isin(test_subjects)]\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z normalization with respect to train data\n",
    "train_data_mean = train_data[['acc_x', 'acc_y', 'acc_z']].mean()\n",
    "train_data_std = train_data[['acc_x', 'acc_y', 'acc_z']].std()\n",
    "# Normalize Training Data\n",
    "train_data.loc[:, ['acc_x', 'acc_y', 'acc_z']] = (train_data[['acc_x', 'acc_y', 'acc_z']] - train_data_mean) / train_data_std\n",
    "\n",
    "# Normalize Test Data with Training Statistics\n",
    "test_data.loc[:, ['acc_x', 'acc_y', 'acc_z']] = (test_data[['acc_x', 'acc_y', 'acc_z']] - train_data_mean) / train_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_x    0.816012\n",
       "acc_y   -0.007595\n",
       "acc_z    0.074082\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_x    0.398664\n",
       "acc_y    0.375481\n",
       "acc_z    0.366527\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train window dataset (8 sec with 50% overlap): (4487, 400, 5)\n",
      "shape of test window dataset (8 sec with 50% overlap): (1124, 400, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 8\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 50\n",
    "\n",
    "# sampling_rate = 50\n",
    "# time_window = 2\n",
    "# window_size = sampling_rate * time_window\n",
    "# overlap_ratio = 0\n",
    "\n",
    "train_window_data, _ = sliding_window_samples(train_data, window_size, overlap_ratio)\n",
    "print(f\"shape of train window dataset ({time_window} sec with {overlap_ratio}% overlap): {train_window_data.shape}\")\n",
    "\n",
    "test_window_data, _ = sliding_window_samples(test_data, window_size, overlap_ratio)\n",
    "print(f\"shape of test window dataset ({time_window} sec with {overlap_ratio}% overlap): {test_window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, -0.9355155521152527, 0.12009837894797139, 2.223044603166194,\n",
       "        'null_class'],\n",
       "       [0, -0.9424832899580327, 0.13119524723936618, 2.200308651740837,\n",
       "        'null_class'],\n",
       "       [0, -0.9146124605540366, 0.1348942033364978, 2.188940676028158,\n",
       "        'null_class'],\n",
       "       ...,\n",
       "       [0, 0.433641884812917, -0.841630287242125, -0.17559328706204944,\n",
       "        'standing'],\n",
       "       [0, 0.42319052198299445, -0.849028215623565, -0.17559328706204944,\n",
       "        'standing'],\n",
       "       [0, 0.42319052198299445, -0.849028215623565, -0.17559328706204944,\n",
       "        'standing']], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Only the Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "train_window_data = train_window_data[:, :, :-1]\n",
    "# train_window_data = train_window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "train_window_data = train_window_data[:, :, 1:]\n",
    "\n",
    "test_window_data = test_window_data[:, :, :-1]\n",
    "test_window_data = test_window_data[:, :, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_window_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jitter(data, noise_factor=0.05):\n",
    "    jitter = noise_factor * np.random.randn(*data.shape)\n",
    "    return data + jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, min_scale=0.5, max_scale=1.5):\n",
    "    scaling_factor = np.random.uniform(min_scale, max_scale)\n",
    "    return data * scaling_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(data):\n",
    "    # Invert the sign of the data to simulate sensor rotation\n",
    "    return -data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_data(data):\n",
    "    return -data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(data):\n",
    "    # This function now correctly handles 2D data arrays\n",
    "    return data[::-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(data, num_segments=4):\n",
    "    segment_length = data.shape[0] // num_segments  # Adjusted to the first dimension for 2D data\n",
    "    permuted_indices = np.random.permutation(num_segments)\n",
    "    return np.concatenate(\n",
    "        [data[segment_length * idx:segment_length * (idx + 1), :] for idx in permuted_indices], axis=0)  # Concatenating along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "\n",
    "def time_warp(data, warp_factor_range=(0.8, 1.2)):\n",
    "    sequence_length, num_channels = data.shape\n",
    "    original_time_points = np.linspace(0, 1, sequence_length)\n",
    "    warp_factor = np.random.uniform(*warp_factor_range)\n",
    "    \n",
    "    # Generate new time points based on the warp factor\n",
    "    warped_time_points = np.linspace(0, warp_factor, sequence_length)\n",
    "\n",
    "    warped_data = np.zeros_like(data)\n",
    "    for j in range(num_channels):\n",
    "        # Interpolate each channel\n",
    "        interpolation = interp1d(original_time_points, data[:, j], \n",
    "                                 kind='linear', fill_value=\"extrapolate\")\n",
    "        warped_data[:, j] = interpolation(warped_time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_channels(data):\n",
    "    # Assuming data is 2D with shape (sequence_length, num_channels)\n",
    "    shuffled_indices = np.random.permutation(data.shape[1])  # Shuffle along the second dimension\n",
    "    return data[:, shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_randomization_single_sample(windowed_sample, beta=1):\n",
    "    \"\"\"\n",
    "    Apply Phase Randomization to a single windowed sample.\n",
    "    \n",
    "    Parameters:\n",
    "    - windowed_sample: A numpy array of shape (400, 3) representing a single sample.\n",
    "    - beta: A hyper-parameter to control the dynamic range of the phase modulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of shape (400, 3) representing the augmented sample.\n",
    "    \"\"\"\n",
    "    # Initialize the output array with the same shape as the input sample\n",
    "    augmented_sample = np.zeros_like(windowed_sample)\n",
    "\n",
    "    # Iterate through each channel (x, y, z)\n",
    "    for channel in range(windowed_sample.shape[1]):\n",
    "        # Apply Fourier transform to the channel signal\n",
    "        fft_signal = fft(windowed_sample[:, channel])\n",
    "        \n",
    "        # Compute amplitude and original phase\n",
    "        amplitude = np.abs(fft_signal)\n",
    "        original_phase = np.angle(fft_signal)\n",
    "        \n",
    "        # Generate random phase component\n",
    "        random_phase_shift = beta * (np.pi * np.random.rand(*original_phase.shape) - np.pi)\n",
    "        random_phase = original_phase + random_phase_shift\n",
    "        \n",
    "        # Combine original amplitude with the randomly generated phase component\n",
    "        # to obtain a new frequency-domain representation\n",
    "        fft_augmented = amplitude * (np.cos(random_phase) + 1j * np.sin(random_phase))\n",
    "        \n",
    "        # Apply inverse Fourier transform to get the augmented time-series signal\n",
    "        augmented_signal = ifft(fft_augmented)\n",
    "        \n",
    "        # Assign the real part of the reconstructed signal to the output array\n",
    "        augmented_sample[:, channel] = augmented_signal.real\n",
    "\n",
    "    return augmented_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.fftpack import fft, ifft\n",
    "\n",
    "# def phase_randomization(windowed_data):\n",
    "#     # Initialize the output array with the same shape as the input\n",
    "#     randomized_data = np.zeros_like(windowed_data)\n",
    "\n",
    "#     # Iterate through each sample\n",
    "#     for i in range(windowed_data.shape[0]):\n",
    "#         # Iterate through each channel (x, y, z)\n",
    "#         for j in range(windowed_data.shape[2]):\n",
    "#             # Apply Fourier transform to the signal\n",
    "#             fft_signal = fft(windowed_data[i, :, j])\n",
    "#             # Compute amplitude and phase\n",
    "#             amplitude = np.abs(fft_signal)\n",
    "#             phase = np.angle(fft_signal)\n",
    "            \n",
    "#             # Randomize phase, but preserve the phase of the DC component\n",
    "#             random_phase = np.exp(1j * (phase + 2 * np.pi * np.random.rand(*phase.shape)))\n",
    "#             random_phase[0] = 1  # Preserve DC component by setting its phase factor to 1 (no change)\n",
    "\n",
    "#             # Reconstruct the signal with the original amplitude and randomized phase\n",
    "#             randomized_signal = ifft(amplitude * random_phase)\n",
    "#             # Assign the real part of the reconstructed signal to the output array\n",
    "#             randomized_data[i, :, j] = randomized_signal.real\n",
    "\n",
    "#     return randomized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "def amplitude_randomization_single_sample(windowed_sample, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply Amplitude Randomization to a single windowed sample.\n",
    "    \n",
    "    Parameters:\n",
    "    - windowed_sample: A numpy array of shape (400, 3) representing a single sample.\n",
    "    - alpha: A parameter controlling the extent of amplitude modulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of shape (400, 3) representing the augmented sample.\n",
    "    \"\"\"\n",
    "    # Initialize the output array with the same shape as the input sample\n",
    "    augmented_sample = np.zeros_like(windowed_sample)\n",
    "\n",
    "    # Iterate through each channel (x, y, z)\n",
    "    for channel in range(windowed_sample.shape[1]):\n",
    "        # Apply Fourier transform to the channel signal\n",
    "        fft_signal = fft(windowed_sample[:, channel])\n",
    "        \n",
    "        # Compute amplitude and phase\n",
    "        amplitude = np.abs(fft_signal)\n",
    "        phase = np.angle(fft_signal)\n",
    "        \n",
    "        # Generate random amplitude modulation\n",
    "        random_modulation = np.random.uniform(-alpha, alpha, size=amplitude.shape)\n",
    "        \n",
    "        # Modulate the amplitude\n",
    "        modulated_amplitude = (alpha + random_modulation) * amplitude\n",
    "        \n",
    "        # Combine modulated amplitude with the original phase\n",
    "        fft_augmented = modulated_amplitude * (np.cos(phase) + 1j * np.sin(phase))\n",
    "        \n",
    "        # Apply inverse Fourier transform to get the augmented time-series signal\n",
    "        augmented_signal = ifft(fft_augmented)\n",
    "        \n",
    "        # Assign the real part of the reconstructed signal to the output array\n",
    "        augmented_sample[:, channel] = augmented_signal.real\n",
    "\n",
    "    return augmented_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store datasets\n",
    "train_dataset = [[] for _ in range(10)]\n",
    "\n",
    "# Loop over all training data\n",
    "for data in train_window_data:\n",
    "    # loop over all transformations\n",
    "    # print(f\"shape of data: {data.shape}\")\n",
    "    data_array = np.array(data, dtype=np.float32)\n",
    "    for j in range(10):\n",
    "        # Original data with label 0\n",
    "        train_dataset[j].append((data_array, 0))\n",
    "        # Apply transformation based on j and save it in the transformed_data variable\n",
    "        if j == 0:\n",
    "            transformed_data = add_jitter(data_array)\n",
    "        elif j == 1:\n",
    "            transformed_data = scale_data(data_array)\n",
    "        elif j == 2:\n",
    "            transformed_data = rotate_data(data_array)\n",
    "        elif j == 3:\n",
    "            transformed_data = negate_data(data_array)\n",
    "        elif j == 4:\n",
    "            transformed_data = horizontal_flip(data_array)\n",
    "        elif j == 5:\n",
    "            transformed_data = permute_data(data_array)\n",
    "        elif j == 6:\n",
    "            transformed_data = time_warp(data_array)\n",
    "        elif j == 7:\n",
    "            transformed_data = shuffle_channels(data_array)\n",
    "        elif j == 8:\n",
    "            transformed_data = amplitude_randomization_single_sample(data_array)\n",
    "        elif j == 9:\n",
    "            transformed_data = phase_randomization_single_sample(data_array)\n",
    "        # Append the transformed data with label 1\n",
    "        transformed_data_array = np.array(transformed_data, dtype=np.float32)\n",
    "        train_dataset[j].append((transformed_data_array, 1))\n",
    "\n",
    "for dataset in train_dataset:\n",
    "    shapes = set(tuple(d.shape) for d, _ in dataset)\n",
    "    if len(shapes) > 1:\n",
    "        print(\"Inconsistent shapes found in dataset:\", shapes)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for j in range(10):\n",
    "    data, labels = zip(*train_dataset[j])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    train_dataset[j] = (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9355155 ,  0.12009838,  2.2230446 ],\n",
       "        [-0.9424833 ,  0.13119525,  2.2003086 ],\n",
       "        [-0.9146125 ,  0.1348942 ,  2.1889408 ],\n",
       "        ...,\n",
       "        [ 0.43364188, -0.8416303 , -0.17559329],\n",
       "        [ 0.42319053, -0.8490282 , -0.17559329],\n",
       "        [ 0.42319053, -0.8490282 , -0.17559329]],\n",
       "\n",
       "       [[-0.6096269 ,  0.07826187,  1.4486427 ],\n",
       "        [-0.6141674 ,  0.08549313,  1.4338268 ],\n",
       "        [-0.59600544,  0.08790354,  1.426419  ],\n",
       "        ...,\n",
       "        [ 0.2825819 , -0.5484467 , -0.11442502],\n",
       "        [ 0.2757713 , -0.5532676 , -0.11442502],\n",
       "        [ 0.2757713 , -0.5532676 , -0.11442502]],\n",
       "\n",
       "       [[ 0.4440935 , -0.81943655, -0.14906807],\n",
       "        [ 0.510287  , -0.8453292 , -0.06949241],\n",
       "        [ 0.68796384, -0.83793133, -0.11496421],\n",
       "        ...,\n",
       "        [ 0.44757736, -0.8490282 , -0.17559329],\n",
       "        [ 0.43712574, -0.83793133, -0.18696123],\n",
       "        [ 0.4266744 , -0.8416303 , -0.17559329]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.37484795, -0.3270232 , -0.04579343],\n",
       "        [ 0.37484795, -0.38994816, -0.07662313],\n",
       "        [ 0.37742472, -0.40636337, -0.06541233],\n",
       "        ...,\n",
       "        [ 0.5577991 , -0.3680612 , -0.14388797],\n",
       "        [ 0.604181  , -0.5048546 , -0.33166894],\n",
       "        [ 0.35938725, -0.3790047 , -0.32045814]],\n",
       "\n",
       "       [[ 0.4893838 , -0.59010124,  0.0214512 ],\n",
       "        [ 0.4893838 , -0.59010124,  0.0214512 ],\n",
       "        [ 0.5486093 , -0.5568106 ,  0.06692302],\n",
       "        ...,\n",
       "        [-1.1689343 ,  0.39382115,  2.3101988 ],\n",
       "        [-1.3222243 ,  0.34573472,  2.1396794 ],\n",
       "        [-1.4093207 ,  0.4567034 ,  1.8630594 ]],\n",
       "\n",
       "       [[ 0.5903946 , -0.71190053,  0.02587882],\n",
       "        [ 0.5903946 , -0.71190053,  0.02587882],\n",
       "        [ 0.66184455, -0.6717386 ,  0.0807362 ],\n",
       "        ...,\n",
       "        [-1.4102072 ,  0.47510743,  2.7870333 ],\n",
       "        [-1.5951368 ,  0.41709578,  2.581318  ],\n",
       "        [-1.7002102 ,  0.5509688 ,  2.2476025 ]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training dataset 0: (8974, 400, 3)\n",
      "shape of training dataset 1: (8974, 400, 3)\n",
      "shape of training dataset 2: (8974, 400, 3)\n",
      "shape of training dataset 3: (8974, 400, 3)\n",
      "shape of training dataset 4: (8974, 400, 3)\n",
      "shape of training dataset 5: (8974, 400, 3)\n",
      "shape of training dataset 6: (8974, 400, 3)\n",
      "shape of training dataset 7: (8974, 400, 3)\n",
      "shape of training dataset 8: (8974, 400, 3)\n",
      "shape of training dataset 9: (8974, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of all training datasets\n",
    "for j in range(10):\n",
    "    print(f\"shape of training dataset {j}: {train_dataset[j][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of training dataset 0: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 1: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 2: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 3: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 4: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 5: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 6: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 7: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 8: (array([0, 1]), array([4487, 4487], dtype=int64))\n",
      "Class distribution of training dataset 9: (array([0, 1]), array([4487, 4487], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# print the class distribution of all training datasets\n",
    "for j in range(10):\n",
    "    print(f\"Class distribution of training dataset {j}: {np.unique(train_dataset[j][1], return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to store datasets for test data\n",
    "test_dataset = [[] for _ in range(10)]\n",
    "\n",
    "# loop over all test data\n",
    "for data in test_window_data:\n",
    "    data_array = np.array(data, dtype=np.float32)\n",
    "    # loop over all transformations\n",
    "    for j in range(10):\n",
    "        # Original data with label 0\n",
    "        test_dataset[j].append((data_array, 0))\n",
    "        # Apply transformation based on j and save it in the transformed_data variable\n",
    "        if j == 0:\n",
    "            transformed_data = add_jitter(data_array)\n",
    "        elif j == 1:\n",
    "            transformed_data = scale_data(data_array)\n",
    "        elif j == 2:\n",
    "            transformed_data = rotate_data(data_array)\n",
    "        elif j == 3:\n",
    "            transformed_data = negate_data(data_array)\n",
    "        elif j == 4:\n",
    "            transformed_data = horizontal_flip(data_array)\n",
    "        elif j == 5:\n",
    "            transformed_data = permute_data(data_array)\n",
    "        elif j == 6:\n",
    "            transformed_data = time_warp(data_array)\n",
    "        elif j == 7:\n",
    "            transformed_data = shuffle_channels(data_array)\n",
    "        elif j == 8:\n",
    "            transformed_data = amplitude_randomization_single_sample(data_array)\n",
    "        elif j == 9:\n",
    "            transformed_data = phase_randomization_single_sample(data_array)\n",
    "        # Append the transformed data with label 1\n",
    "        transformed_data_array = np.array(transformed_data, dtype=np.float32)\n",
    "        test_dataset[j].append((transformed_data_array, 1))\n",
    "\n",
    "# check for inconsistent shapes\n",
    "for dataset in test_dataset:\n",
    "    shapes = set(tuple(d.shape) for d, _ in dataset)\n",
    "    if len(shapes) > 1:\n",
    "        print(\"Inconsistent shapes found in dataset:\", shapes)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for j in range(10):\n",
    "    data, labels = zip(*test_dataset[j])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    test_dataset[j] = (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test dataset 0: (2248, 400, 3)\n",
      "shape of test dataset 1: (2248, 400, 3)\n",
      "shape of test dataset 2: (2248, 400, 3)\n",
      "shape of test dataset 3: (2248, 400, 3)\n",
      "shape of test dataset 4: (2248, 400, 3)\n",
      "shape of test dataset 5: (2248, 400, 3)\n",
      "shape of test dataset 6: (2248, 400, 3)\n",
      "shape of test dataset 7: (2248, 400, 3)\n",
      "shape of test dataset 8: (2248, 400, 3)\n",
      "shape of test dataset 9: (2248, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of all test datasets\n",
    "for j in range(10):\n",
    "    print(f\"shape of test dataset {j}: {test_dataset[j][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of test dataset 0: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 1: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 2: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 3: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 4: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 5: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 6: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 7: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 8: (array([0, 1]), array([1124, 1124], dtype=int64))\n",
      "Class distribution of test dataset 9: (array([0, 1]), array([1124, 1124], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# print the class distribution of all test datasets\n",
    "for j in range(10):\n",
    "    print(f\"Class distribution of test dataset {j}: {np.unique(test_dataset[j][1], return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # Convert data and labels to PyTorch tensors\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Assuming labels are integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Initialize DataLoader for each dataset\n",
    "train_loaders = []\n",
    "\n",
    "for j in range(10):\n",
    "    # Assuming train_dataset[j][0] is the data and train_dataset[j][1] are the labels\n",
    "    data, labels = train_dataset[j]\n",
    "    transformed_dataset = CustomDataset(data, labels)\n",
    "    train_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loaders.append(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test loaders\n",
    "\n",
    "test_loaders = []\n",
    "for j in range(10):\n",
    "    test_data, test_labels = test_dataset[j]\n",
    "    test_transformed_dataset = CustomDataset(test_data, test_labels)\n",
    "    test_loader = DataLoader(test_transformed_dataset, batch_size=batch_size, shuffle=False)  # Usually, we don't shuffle test data\n",
    "    test_loaders.append(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# need to reshape and transpose the data to fit the input shape of the model\n",
    "\n",
    "class TPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TPN, self).__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(96, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(10)  # 8 heads for 8 different transformations\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.trunk(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully-connected layer\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TPN()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # training loop\n",
    "# for epoch in range(30):\n",
    "#     print(f\"Epoch {epoch + 1}\")\n",
    "#     for j in range(8):\n",
    "#         train_loss = 0\n",
    "#         model.train()\n",
    "#         for i, (data, labels) in enumerate(train_loaders[j]):\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(data.transpose(1, 2))  # Transpose the data to fit the input shape of the model\n",
    "#             loss = criterion(outputs[j].squeeze(), labels.float())  # Squeeze the output of the model to fit the loss function\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#         print(f\"Training loss of dataset {j}: {train_loss / len(train_loaders[j])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TPN()\n",
    "# optimizer = Adam(model.parameters(), lr=0.0003)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}\")\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for data, labels in train_loader:  # Assuming train_loader is a combined DataLoader for all tasks\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data.transpose(1, 2))  # Transpose data to match the model's expected input shape\n",
    "#         print(f\"input shape: {data.transpose(1, 2).shape}\")\n",
    "#         print(f\"output length: {len(outputs)}\")\n",
    "#         print(f\"output shape: {outputs[0].shape}\")\n",
    "#         print(f\"label shape: {labels[j].shape}\")\n",
    "#         print(f\"labels: {labels[j]}\")\n",
    "#         loss = 0\n",
    "#         for j in range(8):  # Assuming 8 tasks\n",
    "#             task_loss = criterion(outputs[j].squeeze(), labels[j].float())\n",
    "#             loss += task_loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Average Training Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TPN()\n",
    "# optimizer = Adam(model.parameters(), lr=0.0003)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}\")\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Assuming all DataLoader have the same length\n",
    "#     for batch in zip(*train_loaders):\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = 0\n",
    "\n",
    "#         for j, (data, labels) in enumerate(batch):\n",
    "#             data = data.transpose(1, 2)  # Transpose to match input shape\n",
    "#             outputs = model(data)\n",
    "#             task_loss = criterion(outputs[j].squeeze(), labels.float())\n",
    "#             loss += task_loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loaders[0])  # Average over batches\n",
    "#     print(f\"Average Training Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Training Loss: 5.743296383120489\n",
      "Average Validation Loss: 0.6913394977649053\n",
      "Epoch 2\n",
      "Average Training Loss: 4.356228701611783\n",
      "Average Validation Loss: 0.7139193349414401\n",
      "Epoch 3\n",
      "Average Training Loss: 3.7044164170610143\n",
      "Average Validation Loss: 0.7054248137606515\n",
      "Epoch 4\n",
      "Average Training Loss: 3.4483247158375194\n",
      "Average Validation Loss: 0.6819045080078973\n",
      "Epoch 5\n",
      "Average Training Loss: 3.3177149600171028\n",
      "Average Validation Loss: 0.6997658593787087\n",
      "Epoch 6\n",
      "Average Training Loss: 3.2197941803763097\n",
      "Average Validation Loss: 0.697597904337777\n",
      "Epoch 7\n",
      "Average Training Loss: 3.1505285070297564\n",
      "Average Validation Loss: 0.6930003414551417\n",
      "Epoch 8\n",
      "Average Training Loss: 3.1125137383210744\n",
      "Average Validation Loss: 0.6812408483690686\n",
      "Epoch 9\n",
      "Average Training Loss: 3.069252499451874\n",
      "Average Validation Loss: 0.6647918240891563\n",
      "Epoch 10\n",
      "Average Training Loss: 2.9905190450925354\n",
      "Average Validation Loss: 0.6500415951013565\n",
      "Epoch 11\n",
      "Average Training Loss: 2.9461948601066643\n",
      "Average Validation Loss: 0.6548639254437553\n",
      "Epoch 12\n",
      "Average Training Loss: 2.9236771181120096\n",
      "Average Validation Loss: 0.6425316863589816\n",
      "Epoch 13\n",
      "Average Training Loss: 2.8879831429068923\n",
      "Average Validation Loss: 0.6367864973015256\n",
      "Epoch 14\n",
      "Average Training Loss: 2.864012403691069\n",
      "Average Validation Loss: 0.6312564644548628\n",
      "Epoch 15\n",
      "Average Training Loss: 2.862292541679761\n",
      "Average Validation Loss: 0.6344644510083728\n",
      "Epoch 16\n",
      "Average Training Loss: 2.8122474758337574\n",
      "Average Validation Loss: 0.622782751917839\n",
      "Epoch 17\n",
      "Average Training Loss: 2.8054500420888266\n",
      "Average Validation Loss: 0.6324964430597093\n",
      "Epoch 18\n",
      "Average Training Loss: 2.770936964251471\n",
      "Average Validation Loss: 0.6351068119208018\n",
      "Epoch 19\n",
      "Average Training Loss: 2.753522512760568\n",
      "Average Validation Loss: 0.6227795365783904\n",
      "Epoch 20\n",
      "Average Training Loss: 2.749191274034216\n",
      "Average Validation Loss: 0.6235832754108641\n",
      "Epoch 21\n",
      "Average Training Loss: 2.7361855337805783\n",
      "Average Validation Loss: 0.6144498570097817\n",
      "Epoch 22\n",
      "Average Training Loss: 2.7256635435929537\n",
      "Average Validation Loss: 0.6072734726799859\n",
      "Epoch 23\n",
      "Average Training Loss: 2.6884330587184175\n",
      "Average Validation Loss: 0.6085636698537402\n",
      "Epoch 24\n",
      "Average Training Loss: 2.68960480487093\n",
      "Average Validation Loss: 0.6026563429170184\n",
      "Epoch 25\n",
      "Average Training Loss: 2.6896547121359102\n",
      "Average Validation Loss: 0.6074592984384961\n",
      "Epoch 26\n",
      "Average Training Loss: 2.6489017009735107\n",
      "Average Validation Loss: 0.604175963335567\n",
      "Epoch 27\n",
      "Average Training Loss: 2.653341269662194\n",
      "Average Validation Loss: 0.5901758157544665\n",
      "Epoch 28\n",
      "Average Training Loss: 2.6196593950826226\n",
      "Average Validation Loss: 0.5846828801764382\n",
      "Epoch 29\n",
      "Average Training Loss: 2.6108969999543317\n",
      "Average Validation Loss: 0.582866932782862\n",
      "Epoch 30\n",
      "Average Training Loss: 2.603589835741841\n",
      "Average Validation Loss: 0.566944394674566\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "model = TPN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "min_delta = 0.001  # Minimum change to qualify as an improvement\n",
    "best_loss = np.inf  # Initialize best loss to infinity\n",
    "counter = 0  # Initialize counter for early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in zip(*train_loaders):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        for j, (data, labels) in enumerate(batch):\n",
    "            data = data.transpose(1, 2)  # Transpose to match input shape\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data)\n",
    "            task_loss = criterion(outputs[j].squeeze(), labels.float())\n",
    "            loss += task_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loaders[0])\n",
    "    print(f\"Average Training Loss: {avg_loss}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.transpose(1, 2)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data)\n",
    "            val_loss = criterion(outputs[0].squeeze(), labels.float())  # Assuming single task validation\n",
    "            validation_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = validation_loss / len(test_loader)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if best_loss - avg_val_loss > min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0  # Reset counter if validation loss improved\n",
    "    else:\n",
    "        counter += 1  # Increment counter if no improvement\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break  # Stop training if no improvement for 'patience' consecutive epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 - Accuracy: 0.5040035587188612, F1-Score: 0.16479400749063672\n",
      "Task 1 - Accuracy: 0.9061387900355872, F1-Score: 0.90181479758027\n",
      "Task 2 - Accuracy: 0.9942170818505338, F1-Score: 0.9942196531791908\n",
      "Task 3 - Accuracy: 0.994661921708185, F1-Score: 0.994661921708185\n",
      "Task 4 - Accuracy: 0.75355871886121, F1-Score: 0.7349282296650718\n",
      "Task 5 - Accuracy: 0.7175266903914591, F1-Score: 0.7061545580749653\n",
      "Task 6 - Accuracy: 0.6725978647686833, F1-Score: 0.5960482985729968\n",
      "Task 7 - Accuracy: 0.8674377224199288, F1-Score: 0.8579599618684461\n",
      "Task 8 - Accuracy: 0.9506227758007118, F1-Score: 0.9502465262214254\n",
      "Task 9 - Accuracy: 0.969306049822064, F1-Score: 0.969829470922606\n",
      "Average Accuracy: 0.8330071174377224, Average F1-Score: 0.7870657425283795\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize lists to store metrics for each task\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during evaluation\n",
    "    for j, test_loader in enumerate(test_loaders):\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for data, labels in test_loader:\n",
    "            data = data.transpose(1, 2)  # Transpose data if necessary\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data)\n",
    "            predictions = torch.round(outputs[j].squeeze())  # Convert to binary predictions\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics for the current task\n",
    "        acc = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Task {j} - Accuracy: {acc}, F1-Score: {f1}\")\n",
    "\n",
    "# You can also calculate the average metrics across tasks, if needed\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"Average Accuracy: {avg_accuracy}, Average F1-Score: {avg_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epoch; Average Accuracy: 0.8046596975088968, Average F1-Score: 0.7754674519258395\n",
    "# 30 epoch; Average Accuracy: 0.8065502669039145, Average F1-Score: 0.7555106030018032\n",
    "# 30 epoch, 2 sec window; Average Accuracy: 0.7903709462461128, Average F1-Score: 0.7800402486716894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# torch.save(model.state_dict(), './multitask/tpn_30_epoch_regularized_3.pt') # using phase randomization replace the negate augmentation\n",
    "# torch.save(model.state_dict(), './multitask/tpn_30_epoch_regularized_4.pt') # 9 transformation, using phase randomization\n",
    "# torch.save(model.state_dict(), './multitask/tpn_30_epoch_regularized_5.pt') # 9 transformation, using amplitude randomization\n",
    "# torch.save(model.state_dict(), './multitask/tpn_30_epoch_regularized_6.pt') # 10 transformation, using amplitude and phase randomization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
