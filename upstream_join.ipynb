{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data in one dataframe row-wise\n",
    "# data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)\n",
    "data = pd.concat([data1, data3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286451, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.156435e+01</td>\n",
       "      <td>-3.142137e+00</td>\n",
       "      <td>-1.027698e+00</td>\n",
       "      <td>2.850134e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.311629e+00</td>\n",
       "      <td>4.683064e+00</td>\n",
       "      <td>3.358477e+00</td>\n",
       "      <td>3.758104e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>-7.050938e+00</td>\n",
       "      <td>-2.759063e+00</td>\n",
       "      <td>2.777778e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>-3.678750e+00</td>\n",
       "      <td>-2.972222e-01</td>\n",
       "      <td>2.759063e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>9.166667e-01</td>\n",
       "      <td>5.930556e-01</td>\n",
       "      <td>6.131250e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "      <td>3.310875e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id         acc_x         acc_y         acc_z\n",
       "count  4.286451e+06  4.286451e+06  4.286451e+06  4.286451e+06\n",
       "mean   1.156435e+01 -3.142137e+00 -1.027698e+00  2.850134e+00\n",
       "std    7.311629e+00  4.683064e+00  3.358477e+00  3.758104e+00\n",
       "min    0.000000e+00 -3.924000e+01 -3.924000e+01 -3.924000e+01\n",
       "25%    5.000000e+00 -7.050938e+00 -2.759063e+00  2.777778e-03\n",
       "50%    1.100000e+01 -3.678750e+00 -2.972222e-01  2.759063e+00\n",
       "75%    1.700000e+01  9.166667e-01  5.930556e-01  6.131250e+00\n",
       "max    2.900000e+01  3.893344e+01  3.310875e+01  3.893344e+01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id    0\n",
       "acc_x         0\n",
       "acc_y         0\n",
       "acc_z         0\n",
       "activity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286451, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of window dataset (2 sec with 0% overlap): (42864, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 2\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 0\n",
    "\n",
    "window_data, _ = sliding_window_samples(data, window_size, overlap_ratio)\n",
    "print(f\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "window_data = window_data[:, :, :-1]\n",
    "# window_data = window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "window_data = window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_noise_single_window(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param noise_level: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy data.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_single_window(data, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly crop a single window of data and pad it to maintain original shape.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param crop_ratio: Ratio of the original window size to keep.\n",
    "    :return: Cropped and padded data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    new_size = int(window_size * crop_ratio)\n",
    "    start = np.random.randint(0, window_size - new_size)\n",
    "    end = start + new_size\n",
    "    cropped_data = data[start:end, :]\n",
    "\n",
    "    # Pad the cropped data to maintain original window size\n",
    "    padding_size = window_size - new_size\n",
    "    padding = np.zeros((padding_size, data.shape[1]))\n",
    "    padded_data = np.vstack((cropped_data, padding))\n",
    "\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of warping.\n",
    "    :return: Warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.copy(data)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        time_points = np.linspace(0, 1, window_size)\n",
    "        random_points = np.linspace(0, 1, np.random.randint(4, 10))\n",
    "        warp_values = 1 + np.random.normal(0, warp_factor, random_points.size)\n",
    "        interpolator = CubicSpline(random_points, warp_values)\n",
    "        warped_data[:, j] *= interpolator(time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply time warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of time warping.\n",
    "    :return: Time-warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_points = np.linspace(0, 1, window_size)\n",
    "    random_points = np.sort(np.random.rand(np.random.randint(3, 6)))\n",
    "    warp_values = np.random.normal(1, warp_factor, random_points.size)\n",
    "    interpolator = CubicSpline(random_points, warp_values)\n",
    "    warped_time = interpolator(time_points)\n",
    "    warped_time -= warped_time.min()\n",
    "    warped_time /= warped_time.max()\n",
    "    warped_time *= (window_size - 1)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        interpolator = CubicSpline(np.arange(window_size), data[:, j])\n",
    "        warped_data[:, j] = interpolator(warped_time)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of noisy data: (100, 3)\n",
      "shape of cropped data: (100, 3)\n",
      "shape of warped data: (100, 3)\n",
      "shape of time warped data: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# add random noise\n",
    "noisy_data = add_random_noise_single_window(window_data[0], noise_level=0.1)\n",
    "print(f\"shape of noisy data: {noisy_data.shape}\")\n",
    "# random cropping\n",
    "cropped_data = random_cropping_single_window(window_data[0], crop_ratio=0.8)\n",
    "print(f\"shape of cropped data: {cropped_data.shape}\")\n",
    "# magnitude warping\n",
    "warped_data = magnitude_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of warped data: {warped_data.shape}\")\n",
    "# time warping\n",
    "time_warped_data = time_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of time warped data: {time_warped_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the original data\n",
    "augmented_data = np.copy(window_data)\n",
    "\n",
    "# create labels for augmented data\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "# loop over all windows\n",
    "for i in range(window_data.shape[0]):\n",
    "    # choose one number from 0 to 3\n",
    "    random_number = np.random.randint(0, 4)\n",
    "\n",
    "    if random_number == 0:\n",
    "        augmented_data[i] = add_random_noise_single_window(window_data[i], noise_level=0.1)\n",
    "        augmented_labels.append(0)\n",
    "    elif random_number == 1:\n",
    "        augmented_data[i] = random_cropping_single_window(window_data[i], crop_ratio=0.8)\n",
    "        augmented_labels.append(1)\n",
    "    elif random_number == 2:\n",
    "        augmented_data[i] = magnitude_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(2)\n",
    "    else:\n",
    "        augmented_data[i] = time_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10553, 1: 10513, 2: 10870, 3: 10928}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of augmented data: (42864, 100, 3)\n",
      "shape of augmented labels: 42864\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of augmented data: {augmented_data.shape}\")\n",
    "print(f\"shape of augmented labels: {len(augmented_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented data and labels\n",
    "# np.save('./data_processing/augmented_data_join.npy', augmented_data)\n",
    "# np.save('./data_processing/augmented_labels_join.npy', augmented_labels)\n",
    "\n",
    "np.save('./data_processing/augmented_data_join_2.npy', augmented_data) # 2 datasets\n",
    "np.save('./data_processing/augmented_labels_join_2.npy', augmented_labels) # 2 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmented data and labels\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join.npy', allow_pickle=True)\n",
    "\n",
    "augmented_data = np.load('./data_processing/augmented_data_join_2.npy', allow_pickle=True)\n",
    "augmented_labels = np.load('./data_processing/augmented_labels_join_2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864, 100, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10553, 1: 10513, 2: 10870, 3: 10928}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of arrays to a single numpy array\n",
    "# if isinstance(augmented_data, list):\n",
    "#     augmented_data = np.stack(augmented_data)\n",
    "\n",
    "# # Ensure the data type is float32\n",
    "# augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "# augmented_labels_tensor = torch.from_numpy(np.array(augmented_labels)).long()\n",
    "\n",
    "# # split data into train and test set\n",
    "# train_size = int(0.8 * augmented_data.shape[0])\n",
    "# test_size = augmented_data.shape[0] - train_size\n",
    "# train_data, test_data = torch.utils.data.random_split(augmented_data, [train_size, test_size])\n",
    "# train_labels, test_labels = torch.utils.data.random_split(augmented_labels, [train_size, test_size])\n",
    "\n",
    "# # create train and test dataset\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # create train and test dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # check if GPU is available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming augmented_data and augmented_labels are numpy arrays\n",
    "augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "augmented_labels_tensor = torch.from_numpy(augmented_labels)\n",
    "#convert labels to long\n",
    "augmented_labels_tensor = augmented_labels_tensor.long()\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(0.8 * len(augmented_data_tensor))\n",
    "test_size = len(augmented_data_tensor) - train_size\n",
    "\n",
    "# Creating datasets\n",
    "dataset = TensorDataset(augmented_data_tensor, augmented_labels_tensor)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Function to extract tensors from Subset\n",
    "def extract_subset_data(subset, dataset):\n",
    "    return dataset.tensors[0][subset.indices], dataset.tensors[1][subset.indices]\n",
    "\n",
    "# Extract data and labels from train and test sets\n",
    "train_data, train_labels = extract_subset_data(train_dataset, dataset)\n",
    "test_data, test_labels = extract_subset_data(test_dataset, dataset)\n",
    "\n",
    "# create train and test TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_loader: 268\n",
      "shape of test_loader: 67\n"
     ]
    }
   ],
   "source": [
    "#  print the shape of train_loader and test_loader\n",
    "print(f\"shape of train_loader: {len(train_loader)}\")\n",
    "print(f\"shape of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    #calculate accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return running_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. Test Loss: {test_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}\")\n",
    "    return train_losses, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50.. Train Loss: 1.338.. Test Loss: 1.286.. Test Accuracy: 0.442\n",
      "Epoch: 2/50.. Train Loss: 1.214.. Test Loss: 1.137.. Test Accuracy: 0.478\n",
      "Epoch: 3/50.. Train Loss: 1.075.. Test Loss: 1.031.. Test Accuracy: 0.513\n",
      "Epoch: 4/50.. Train Loss: 1.007.. Test Loss: 0.991.. Test Accuracy: 0.537\n",
      "Epoch: 5/50.. Train Loss: 0.979.. Test Loss: 0.972.. Test Accuracy: 0.538\n",
      "Epoch: 6/50.. Train Loss: 0.963.. Test Loss: 0.958.. Test Accuracy: 0.544\n",
      "Epoch: 7/50.. Train Loss: 0.951.. Test Loss: 0.946.. Test Accuracy: 0.564\n",
      "Epoch: 8/50.. Train Loss: 0.941.. Test Loss: 0.937.. Test Accuracy: 0.572\n",
      "Epoch: 9/50.. Train Loss: 0.931.. Test Loss: 0.927.. Test Accuracy: 0.606\n",
      "Epoch: 10/50.. Train Loss: 0.922.. Test Loss: 0.917.. Test Accuracy: 0.588\n",
      "Epoch: 11/50.. Train Loss: 0.913.. Test Loss: 0.909.. Test Accuracy: 0.616\n",
      "Epoch: 12/50.. Train Loss: 0.903.. Test Loss: 0.898.. Test Accuracy: 0.592\n",
      "Epoch: 13/50.. Train Loss: 0.894.. Test Loss: 0.889.. Test Accuracy: 0.582\n",
      "Epoch: 14/50.. Train Loss: 0.883.. Test Loss: 0.878.. Test Accuracy: 0.609\n",
      "Epoch: 15/50.. Train Loss: 0.873.. Test Loss: 0.868.. Test Accuracy: 0.584\n",
      "Epoch: 16/50.. Train Loss: 0.862.. Test Loss: 0.854.. Test Accuracy: 0.624\n",
      "Epoch: 17/50.. Train Loss: 0.851.. Test Loss: 0.844.. Test Accuracy: 0.607\n",
      "Epoch: 18/50.. Train Loss: 0.840.. Test Loss: 0.833.. Test Accuracy: 0.607\n",
      "Epoch: 19/50.. Train Loss: 0.830.. Test Loss: 0.822.. Test Accuracy: 0.618\n",
      "Epoch: 20/50.. Train Loss: 0.821.. Test Loss: 0.816.. Test Accuracy: 0.603\n",
      "Epoch: 21/50.. Train Loss: 0.812.. Test Loss: 0.820.. Test Accuracy: 0.586\n",
      "Epoch: 22/50.. Train Loss: 0.804.. Test Loss: 0.805.. Test Accuracy: 0.625\n",
      "Epoch: 23/50.. Train Loss: 0.798.. Test Loss: 0.800.. Test Accuracy: 0.602\n",
      "Epoch: 24/50.. Train Loss: 0.790.. Test Loss: 0.783.. Test Accuracy: 0.621\n",
      "Epoch: 25/50.. Train Loss: 0.784.. Test Loss: 0.780.. Test Accuracy: 0.643\n",
      "Epoch: 26/50.. Train Loss: 0.779.. Test Loss: 0.775.. Test Accuracy: 0.627\n",
      "Epoch: 27/50.. Train Loss: 0.774.. Test Loss: 0.771.. Test Accuracy: 0.641\n",
      "Epoch: 28/50.. Train Loss: 0.771.. Test Loss: 0.767.. Test Accuracy: 0.618\n",
      "Epoch: 29/50.. Train Loss: 0.766.. Test Loss: 0.763.. Test Accuracy: 0.675\n",
      "Epoch: 30/50.. Train Loss: 0.764.. Test Loss: 0.770.. Test Accuracy: 0.608\n",
      "Epoch: 31/50.. Train Loss: 0.758.. Test Loss: 0.753.. Test Accuracy: 0.638\n",
      "Epoch: 32/50.. Train Loss: 0.756.. Test Loss: 0.750.. Test Accuracy: 0.650\n",
      "Epoch: 33/50.. Train Loss: 0.754.. Test Loss: 0.749.. Test Accuracy: 0.674\n",
      "Epoch: 34/50.. Train Loss: 0.752.. Test Loss: 0.745.. Test Accuracy: 0.650\n",
      "Epoch: 35/50.. Train Loss: 0.749.. Test Loss: 0.759.. Test Accuracy: 0.641\n",
      "Epoch: 36/50.. Train Loss: 0.747.. Test Loss: 0.749.. Test Accuracy: 0.625\n",
      "Epoch: 37/50.. Train Loss: 0.742.. Test Loss: 0.758.. Test Accuracy: 0.609\n",
      "Epoch: 38/50.. Train Loss: 0.742.. Test Loss: 0.739.. Test Accuracy: 0.649\n",
      "Epoch: 39/50.. Train Loss: 0.740.. Test Loss: 0.745.. Test Accuracy: 0.646\n",
      "Epoch: 40/50.. Train Loss: 0.738.. Test Loss: 0.748.. Test Accuracy: 0.641\n",
      "Epoch: 41/50.. Train Loss: 0.737.. Test Loss: 0.732.. Test Accuracy: 0.656\n",
      "Epoch: 42/50.. Train Loss: 0.735.. Test Loss: 0.734.. Test Accuracy: 0.650\n",
      "Epoch: 43/50.. Train Loss: 0.732.. Test Loss: 0.726.. Test Accuracy: 0.667\n",
      "Epoch: 44/50.. Train Loss: 0.734.. Test Loss: 0.749.. Test Accuracy: 0.624\n",
      "Epoch: 45/50.. Train Loss: 0.730.. Test Loss: 0.732.. Test Accuracy: 0.685\n",
      "Epoch: 46/50.. Train Loss: 0.728.. Test Loss: 0.743.. Test Accuracy: 0.657\n",
      "Epoch: 47/50.. Train Loss: 0.726.. Test Loss: 0.731.. Test Accuracy: 0.664\n",
      "Epoch: 48/50.. Train Loss: 0.726.. Test Loss: 0.729.. Test Accuracy: 0.641\n",
      "Epoch: 49/50.. Train Loss: 0.724.. Test Loss: 0.717.. Test Accuracy: 0.669\n",
      "Epoch: 50/50.. Train Loss: 0.726.. Test Loss: 0.716.. Test Accuracy: 0.663\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = CNNFeatureExtractor(num_classes=4)\n",
    "\n",
    "# move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 50\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"cnn_feature_extractor_join_2_dataset.pt\"\n",
    "# model_name = f\"cnn_feature_extractor_join_2_dataset_4_augmentations.pt\"\n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Train Loss: 1.398.. Test Loss: 1.390.. Test Accuracy: 0.243\n",
      "Epoch: 2/100.. Train Loss: 1.395.. Test Loss: 1.389.. Test Accuracy: 0.247\n",
      "Epoch: 3/100.. Train Loss: 1.393.. Test Loss: 1.388.. Test Accuracy: 0.246\n",
      "Epoch: 4/100.. Train Loss: 1.392.. Test Loss: 1.388.. Test Accuracy: 0.252\n",
      "Epoch: 5/100.. Train Loss: 1.390.. Test Loss: 1.387.. Test Accuracy: 0.253\n",
      "Epoch: 6/100.. Train Loss: 1.390.. Test Loss: 1.387.. Test Accuracy: 0.256\n",
      "Epoch: 7/100.. Train Loss: 1.390.. Test Loss: 1.387.. Test Accuracy: 0.257\n",
      "Epoch: 8/100.. Train Loss: 1.388.. Test Loss: 1.386.. Test Accuracy: 0.264\n",
      "Epoch: 9/100.. Train Loss: 1.388.. Test Loss: 1.386.. Test Accuracy: 0.260\n",
      "Epoch: 10/100.. Train Loss: 1.388.. Test Loss: 1.386.. Test Accuracy: 0.256\n",
      "Epoch: 11/100.. Train Loss: 1.387.. Test Loss: 1.386.. Test Accuracy: 0.261\n",
      "Epoch: 12/100.. Train Loss: 1.387.. Test Loss: 1.385.. Test Accuracy: 0.260\n",
      "Epoch: 13/100.. Train Loss: 1.387.. Test Loss: 1.385.. Test Accuracy: 0.260\n",
      "Epoch: 14/100.. Train Loss: 1.386.. Test Loss: 1.385.. Test Accuracy: 0.263\n",
      "Epoch: 15/100.. Train Loss: 1.385.. Test Loss: 1.385.. Test Accuracy: 0.265\n",
      "Epoch: 16/100.. Train Loss: 1.386.. Test Loss: 1.384.. Test Accuracy: 0.267\n",
      "Epoch: 17/100.. Train Loss: 1.385.. Test Loss: 1.384.. Test Accuracy: 0.269\n",
      "Epoch: 18/100.. Train Loss: 1.384.. Test Loss: 1.384.. Test Accuracy: 0.267\n",
      "Epoch: 19/100.. Train Loss: 1.384.. Test Loss: 1.384.. Test Accuracy: 0.266\n",
      "Epoch: 20/100.. Train Loss: 1.384.. Test Loss: 1.384.. Test Accuracy: 0.265\n",
      "Epoch: 21/100.. Train Loss: 1.384.. Test Loss: 1.383.. Test Accuracy: 0.270\n",
      "Epoch: 22/100.. Train Loss: 1.384.. Test Loss: 1.384.. Test Accuracy: 0.270\n",
      "Epoch: 23/100.. Train Loss: 1.383.. Test Loss: 1.383.. Test Accuracy: 0.266\n",
      "Epoch: 24/100.. Train Loss: 1.383.. Test Loss: 1.383.. Test Accuracy: 0.273\n",
      "Epoch: 25/100.. Train Loss: 1.382.. Test Loss: 1.383.. Test Accuracy: 0.275\n",
      "Epoch: 26/100.. Train Loss: 1.382.. Test Loss: 1.382.. Test Accuracy: 0.281\n",
      "Epoch: 27/100.. Train Loss: 1.382.. Test Loss: 1.382.. Test Accuracy: 0.276\n",
      "Epoch: 28/100.. Train Loss: 1.382.. Test Loss: 1.382.. Test Accuracy: 0.277\n",
      "Epoch: 29/100.. Train Loss: 1.382.. Test Loss: 1.381.. Test Accuracy: 0.279\n",
      "Epoch: 30/100.. Train Loss: 1.381.. Test Loss: 1.381.. Test Accuracy: 0.278\n",
      "Epoch: 31/100.. Train Loss: 1.381.. Test Loss: 1.381.. Test Accuracy: 0.283\n",
      "Epoch: 32/100.. Train Loss: 1.380.. Test Loss: 1.380.. Test Accuracy: 0.283\n",
      "Epoch: 33/100.. Train Loss: 1.379.. Test Loss: 1.380.. Test Accuracy: 0.287\n",
      "Epoch: 34/100.. Train Loss: 1.380.. Test Loss: 1.380.. Test Accuracy: 0.287\n",
      "Epoch: 35/100.. Train Loss: 1.379.. Test Loss: 1.379.. Test Accuracy: 0.291\n",
      "Epoch: 36/100.. Train Loss: 1.378.. Test Loss: 1.379.. Test Accuracy: 0.293\n",
      "Epoch: 37/100.. Train Loss: 1.378.. Test Loss: 1.378.. Test Accuracy: 0.297\n",
      "Epoch: 38/100.. Train Loss: 1.377.. Test Loss: 1.377.. Test Accuracy: 0.300\n",
      "Epoch: 39/100.. Train Loss: 1.376.. Test Loss: 1.377.. Test Accuracy: 0.298\n",
      "Epoch: 40/100.. Train Loss: 1.375.. Test Loss: 1.376.. Test Accuracy: 0.309\n",
      "Epoch: 41/100.. Train Loss: 1.374.. Test Loss: 1.375.. Test Accuracy: 0.315\n",
      "Epoch: 42/100.. Train Loss: 1.374.. Test Loss: 1.374.. Test Accuracy: 0.319\n",
      "Epoch: 43/100.. Train Loss: 1.372.. Test Loss: 1.373.. Test Accuracy: 0.333\n",
      "Epoch: 44/100.. Train Loss: 1.372.. Test Loss: 1.371.. Test Accuracy: 0.333\n",
      "Epoch: 45/100.. Train Loss: 1.369.. Test Loss: 1.368.. Test Accuracy: 0.338\n",
      "Epoch: 46/100.. Train Loss: 1.366.. Test Loss: 1.365.. Test Accuracy: 0.351\n",
      "Epoch: 47/100.. Train Loss: 1.362.. Test Loss: 1.361.. Test Accuracy: 0.380\n",
      "Epoch: 48/100.. Train Loss: 1.359.. Test Loss: 1.355.. Test Accuracy: 0.385\n",
      "Epoch: 49/100.. Train Loss: 1.349.. Test Loss: 1.343.. Test Accuracy: 0.413\n",
      "Epoch: 50/100.. Train Loss: 1.332.. Test Loss: 1.318.. Test Accuracy: 0.432\n",
      "Epoch: 51/100.. Train Loss: 1.293.. Test Loss: 1.255.. Test Accuracy: 0.464\n",
      "Epoch: 52/100.. Train Loss: 1.197.. Test Loss: 1.121.. Test Accuracy: 0.458\n",
      "Epoch: 53/100.. Train Loss: 1.073.. Test Loss: 1.025.. Test Accuracy: 0.528\n",
      "Epoch: 54/100.. Train Loss: 1.014.. Test Loss: 0.986.. Test Accuracy: 0.527\n",
      "Epoch: 55/100.. Train Loss: 0.981.. Test Loss: 0.960.. Test Accuracy: 0.569\n",
      "Epoch: 56/100.. Train Loss: 0.957.. Test Loss: 0.938.. Test Accuracy: 0.588\n",
      "Epoch: 57/100.. Train Loss: 0.935.. Test Loss: 0.917.. Test Accuracy: 0.594\n",
      "Epoch: 58/100.. Train Loss: 0.914.. Test Loss: 0.896.. Test Accuracy: 0.612\n",
      "Epoch: 59/100.. Train Loss: 0.893.. Test Loss: 0.875.. Test Accuracy: 0.614\n",
      "Epoch: 60/100.. Train Loss: 0.871.. Test Loss: 0.857.. Test Accuracy: 0.618\n",
      "Epoch: 61/100.. Train Loss: 0.846.. Test Loss: 0.836.. Test Accuracy: 0.638\n",
      "Epoch: 62/100.. Train Loss: 0.826.. Test Loss: 0.807.. Test Accuracy: 0.634\n",
      "Epoch: 63/100.. Train Loss: 0.803.. Test Loss: 0.790.. Test Accuracy: 0.644\n",
      "Epoch: 64/100.. Train Loss: 0.786.. Test Loss: 0.768.. Test Accuracy: 0.647\n",
      "Epoch: 65/100.. Train Loss: 0.770.. Test Loss: 0.768.. Test Accuracy: 0.640\n",
      "Epoch: 66/100.. Train Loss: 0.757.. Test Loss: 0.746.. Test Accuracy: 0.653\n",
      "Epoch: 67/100.. Train Loss: 0.749.. Test Loss: 0.776.. Test Accuracy: 0.601\n",
      "Epoch: 68/100.. Train Loss: 0.738.. Test Loss: 0.746.. Test Accuracy: 0.624\n",
      "Epoch: 69/100.. Train Loss: 0.735.. Test Loss: 0.726.. Test Accuracy: 0.647\n",
      "Epoch: 70/100.. Train Loss: 0.730.. Test Loss: 0.726.. Test Accuracy: 0.648\n",
      "Epoch: 71/100.. Train Loss: 0.727.. Test Loss: 0.721.. Test Accuracy: 0.663\n",
      "Epoch: 72/100.. Train Loss: 0.723.. Test Loss: 0.719.. Test Accuracy: 0.647\n",
      "Epoch: 73/100.. Train Loss: 0.721.. Test Loss: 0.719.. Test Accuracy: 0.653\n",
      "Epoch: 74/100.. Train Loss: 0.717.. Test Loss: 0.711.. Test Accuracy: 0.653\n",
      "Epoch: 75/100.. Train Loss: 0.715.. Test Loss: 0.704.. Test Accuracy: 0.655\n",
      "Epoch: 76/100.. Train Loss: 0.712.. Test Loss: 0.709.. Test Accuracy: 0.656\n",
      "Epoch: 77/100.. Train Loss: 0.711.. Test Loss: 0.711.. Test Accuracy: 0.647\n",
      "Epoch: 78/100.. Train Loss: 0.711.. Test Loss: 0.702.. Test Accuracy: 0.664\n",
      "Epoch: 79/100.. Train Loss: 0.708.. Test Loss: 0.710.. Test Accuracy: 0.667\n",
      "Epoch: 80/100.. Train Loss: 0.706.. Test Loss: 0.704.. Test Accuracy: 0.664\n",
      "Epoch: 81/100.. Train Loss: 0.704.. Test Loss: 0.704.. Test Accuracy: 0.646\n",
      "Epoch: 82/100.. Train Loss: 0.703.. Test Loss: 0.694.. Test Accuracy: 0.665\n",
      "Epoch: 83/100.. Train Loss: 0.699.. Test Loss: 0.694.. Test Accuracy: 0.664\n",
      "Epoch: 84/100.. Train Loss: 0.701.. Test Loss: 0.691.. Test Accuracy: 0.665\n",
      "Epoch: 85/100.. Train Loss: 0.695.. Test Loss: 0.695.. Test Accuracy: 0.650\n",
      "Epoch: 86/100.. Train Loss: 0.696.. Test Loss: 0.702.. Test Accuracy: 0.646\n",
      "Epoch: 87/100.. Train Loss: 0.696.. Test Loss: 0.701.. Test Accuracy: 0.664\n",
      "Epoch: 88/100.. Train Loss: 0.692.. Test Loss: 0.689.. Test Accuracy: 0.657\n",
      "Epoch: 89/100.. Train Loss: 0.691.. Test Loss: 0.684.. Test Accuracy: 0.668\n",
      "Epoch: 90/100.. Train Loss: 0.691.. Test Loss: 0.689.. Test Accuracy: 0.664\n",
      "Epoch: 91/100.. Train Loss: 0.689.. Test Loss: 0.690.. Test Accuracy: 0.654\n",
      "Epoch: 92/100.. Train Loss: 0.687.. Test Loss: 0.683.. Test Accuracy: 0.672\n",
      "Epoch: 93/100.. Train Loss: 0.687.. Test Loss: 0.713.. Test Accuracy: 0.643\n",
      "Epoch: 94/100.. Train Loss: 0.686.. Test Loss: 0.676.. Test Accuracy: 0.678\n",
      "Epoch: 95/100.. Train Loss: 0.685.. Test Loss: 0.676.. Test Accuracy: 0.662\n",
      "Epoch: 96/100.. Train Loss: 0.684.. Test Loss: 0.699.. Test Accuracy: 0.676\n",
      "Epoch: 97/100.. Train Loss: 0.682.. Test Loss: 0.673.. Test Accuracy: 0.678\n",
      "Epoch: 98/100.. Train Loss: 0.681.. Test Loss: 0.678.. Test Accuracy: 0.682\n",
      "Epoch: 99/100.. Train Loss: 0.681.. Test Loss: 0.672.. Test Accuracy: 0.672\n",
      "Epoch: 100/100.. Train Loss: 0.678.. Test Loss: 0.668.. Test Accuracy: 0.676\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "model = TCN(num_inputs, num_channels, kernel_size, num_classes=4).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00003)  # Replace lr with your learning rate\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 100\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"tcn_join_2_dataset.pt\"\n",
    "# model_name = f\"tcn_join_2_dataset_4_augmentations.pt\"   \n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
