{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique subjects: 22\n"
     ]
    }
   ],
   "source": [
    "# count number of unique subjects\n",
    "print(f\"number of unique subjects: {data1['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['subject_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique subjects: 15\n"
     ]
    }
   ],
   "source": [
    "# print number of unique subjects\n",
    "print(f\"number of unique subjects: {data2['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3['subject_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['subject_id'] = data3['subject_id'] + 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3['subject_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique subjects: 30\n"
     ]
    }
   ],
   "source": [
    "# print number of unique subjects\n",
    "print(f\"number of unique subjects: {data3['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data in one dataframe row-wise\n",
    "# data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)\n",
    "data = pd.concat([data1, data3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286451, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534450</th>\n",
       "      <td>37</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439931</th>\n",
       "      <td>3</td>\n",
       "      <td>-5.518125</td>\n",
       "      <td>-0.306563</td>\n",
       "      <td>6.437813</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932401</th>\n",
       "      <td>13</td>\n",
       "      <td>-4.291875</td>\n",
       "      <td>-4.905000</td>\n",
       "      <td>6.131250</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194902</th>\n",
       "      <td>27</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>0.731944</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607124</th>\n",
       "      <td>10</td>\n",
       "      <td>-9.196875</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id     acc_x     acc_y     acc_z    activity\n",
       "534450           37  0.998611  0.087500 -0.175000    standing\n",
       "439931            3 -5.518125 -0.306563  6.437813  null_class\n",
       "1932401          13 -4.291875 -4.905000  6.131250  null_class\n",
       "194902           27 -0.455556  0.636111  0.731944  null_class\n",
       "1607124          10 -9.196875  0.613125  0.613125  null_class"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.732692e+01</td>\n",
       "      <td>-3.142137e+00</td>\n",
       "      <td>-1.027698e+00</td>\n",
       "      <td>2.850134e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.358579e+01</td>\n",
       "      <td>4.683064e+00</td>\n",
       "      <td>3.358477e+00</td>\n",
       "      <td>3.758104e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>-7.050938e+00</td>\n",
       "      <td>-2.759063e+00</td>\n",
       "      <td>2.777778e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>-3.678750e+00</td>\n",
       "      <td>-2.972222e-01</td>\n",
       "      <td>2.759063e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>9.166667e-01</td>\n",
       "      <td>5.930556e-01</td>\n",
       "      <td>6.131250e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.100000e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "      <td>3.310875e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id         acc_x         acc_y         acc_z\n",
       "count  4.286451e+06  4.286451e+06  4.286451e+06  4.286451e+06\n",
       "mean   1.732692e+01 -3.142137e+00 -1.027698e+00  2.850134e+00\n",
       "std    1.358579e+01  4.683064e+00  3.358477e+00  3.758104e+00\n",
       "min    0.000000e+00 -3.924000e+01 -3.924000e+01 -3.924000e+01\n",
       "25%    7.000000e+00 -7.050938e+00 -2.759063e+00  2.777778e-03\n",
       "50%    1.400000e+01 -3.678750e+00 -2.972222e-01  2.759063e+00\n",
       "75%    2.300000e+01  9.166667e-01  5.930556e-01  6.131250e+00\n",
       "max    5.100000e+01  3.893344e+01  3.310875e+01  3.893344e+01"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id    0\n",
       "acc_x         0\n",
       "acc_y         0\n",
       "acc_z         0\n",
       "activity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique subjects: 52\n"
     ]
    }
   ],
   "source": [
    "# print number of unique subjects\n",
    "print(f\"number of unique subjects: {data['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array along with the mode label for each window.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows, indices, and labels\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    labels = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * win_len)\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        window = data[curr:curr + win_len]\n",
    "        windows.append(window.iloc[:, 1:-1])  # Exclude the first and last columns\n",
    "        indices.append([curr, curr + win_len])\n",
    "        \n",
    "        # Extract and compute the mode of the encoded labels for the current window\n",
    "        window_labels = window['subject_id']\n",
    "        mode_result = mode(window_labels)\n",
    "        window_label = mode_result[0] if mode_result[0].size > 0 else mode_result\n",
    "        labels.append(window_label)\n",
    "\n",
    "        curr += win_len - overlapping_elements\n",
    "\n",
    "    result_windows = np.array(windows)\n",
    "    result_indices = np.array(indices)\n",
    "    result_labels = np.array(labels)\n",
    "    return result_windows, result_indices, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16036\\1845283343.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtime_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtime_window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moverlap_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwindow_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_subject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msliding_window_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16036\\692544352.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, samples_per_window, overlap_ratio)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwin_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Extract and compute the mode of the encoded labels for the current window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mwindow_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subject_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mmode_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mwindow_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmode_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmode_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tori_har\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    519\u001b[0m                 \u001b[1;31m# behavior of those would break backward compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_remove_sentinel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaired\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhypotest_fun_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_add_reduced_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtuple_to_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tori_har\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, axis, nan_policy, keepdims)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[0mNaN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_nan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mModeResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNaN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNaN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m     \u001b[0mmodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mModeResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 2\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 0\n",
    "\n",
    "window_data, _, window_subject = sliding_window_samples(data, window_size, overlap_ratio)\n",
    "print(f\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ],\n",
       "       [ 0.3065625,  9.196875 , -1.22625  ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove the label column\n",
    "# window_data = window_data[:, :, :-1]\n",
    "# # window_data = window_data[:, :, :-1]\n",
    "# #remove the subject column\n",
    "# window_data = window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_subject.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_noise_single_window(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param noise_level: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy data.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_single_window(data, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly crop a single window of data and pad it to maintain original shape.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param crop_ratio: Ratio of the original window size to keep.\n",
    "    :return: Cropped and padded data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    new_size = int(window_size * crop_ratio)\n",
    "    start = np.random.randint(0, window_size - new_size)\n",
    "    end = start + new_size\n",
    "    cropped_data = data[start:end, :]\n",
    "\n",
    "    # Pad the cropped data to maintain original window size\n",
    "    padding_size = window_size - new_size\n",
    "    padding = np.zeros((padding_size, data.shape[1]))\n",
    "    padded_data = np.vstack((cropped_data, padding))\n",
    "\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of warping.\n",
    "    :return: Warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.copy(data)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        time_points = np.linspace(0, 1, window_size)\n",
    "        random_points = np.linspace(0, 1, np.random.randint(4, 10))\n",
    "        warp_values = 1 + np.random.normal(0, warp_factor, random_points.size)\n",
    "        interpolator = CubicSpline(random_points, warp_values)\n",
    "        warped_data[:, j] *= interpolator(time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply time warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of time warping.\n",
    "    :return: Time-warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_points = np.linspace(0, 1, window_size)\n",
    "    random_points = np.sort(np.random.rand(np.random.randint(3, 6)))\n",
    "    warp_values = np.random.normal(1, warp_factor, random_points.size)\n",
    "    interpolator = CubicSpline(random_points, warp_values)\n",
    "    warped_time = interpolator(time_points)\n",
    "    warped_time -= warped_time.min()\n",
    "    warped_time /= warped_time.max()\n",
    "    warped_time *= (window_size - 1)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        interpolator = CubicSpline(np.arange(window_size), data[:, j])\n",
    "        warped_data[:, j] = interpolator(warped_time)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "def amplitude_randomization_single_sample(windowed_sample, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply Amplitude Randomization to a single windowed sample.\n",
    "    \n",
    "    Parameters:\n",
    "    - windowed_sample: A numpy array of shape (400, 3) representing a single sample.\n",
    "    - alpha: A parameter controlling the extent of amplitude modulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of shape (400, 3) representing the augmented sample.\n",
    "    \"\"\"\n",
    "    # Initialize the output array with the same shape as the input sample\n",
    "    augmented_sample = np.zeros_like(windowed_sample)\n",
    "\n",
    "    # Iterate through each channel (x, y, z)\n",
    "    for channel in range(windowed_sample.shape[1]):\n",
    "        # Apply Fourier transform to the channel signal\n",
    "        fft_signal = fft(windowed_sample[:, channel])\n",
    "        \n",
    "        # Compute amplitude and phase\n",
    "        amplitude = np.abs(fft_signal)\n",
    "        phase = np.angle(fft_signal)\n",
    "        \n",
    "        # Generate random amplitude modulation\n",
    "        random_modulation = np.random.uniform(-alpha, alpha, size=amplitude.shape)\n",
    "        \n",
    "        # Modulate the amplitude\n",
    "        modulated_amplitude = (alpha + random_modulation) * amplitude\n",
    "        \n",
    "        # Combine modulated amplitude with the original phase\n",
    "        fft_augmented = modulated_amplitude * (np.cos(phase) + 1j * np.sin(phase))\n",
    "        \n",
    "        # Apply inverse Fourier transform to get the augmented time-series signal\n",
    "        augmented_signal = ifft(fft_augmented)\n",
    "        \n",
    "        # Assign the real part of the reconstructed signal to the output array\n",
    "        augmented_sample[:, channel] = augmented_signal.real\n",
    "\n",
    "    return augmented_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of noisy data: (100, 3)\n",
      "shape of cropped data: (100, 3)\n",
      "shape of warped data: (100, 3)\n",
      "shape of time warped data: (100, 3)\n",
      "shape of amplitude randomized data: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# add random noise\n",
    "noisy_data = add_random_noise_single_window(window_data[0], noise_level=0.1)\n",
    "print(f\"shape of noisy data: {noisy_data.shape}\")\n",
    "# random cropping\n",
    "cropped_data = random_cropping_single_window(window_data[0], crop_ratio=0.8)\n",
    "print(f\"shape of cropped data: {cropped_data.shape}\")\n",
    "# magnitude warping\n",
    "warped_data = magnitude_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of warped data: {warped_data.shape}\")\n",
    "# time warping\n",
    "time_warped_data = time_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of time warped data: {time_warped_data.shape}\")\n",
    "\n",
    "# amplitude randomization\n",
    "amplitude_randomized_data = amplitude_randomization_single_sample(window_data[0])\n",
    "print(f\"shape of amplitude randomized data: {amplitude_randomized_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the original data\n",
    "augmented_data = np.copy(window_data)\n",
    "\n",
    "# create labels for augmented data\n",
    "augmented_labels = []\n",
    "\n",
    "#create subject id for augmented data\n",
    "augmented_subject = []\n",
    "\n",
    "\n",
    "# loop over all windows\n",
    "for i in range(window_data.shape[0]):\n",
    "    # choose one number from 0 to 4\n",
    "    random_number = np.random.randint(0, 4)\n",
    "\n",
    "    if random_number == 0:\n",
    "        augmented_data[i] = add_random_noise_single_window(window_data[i], noise_level=0.1)\n",
    "        augmented_labels.append(0)\n",
    "        augmented_subject.append(window_subject[i])\n",
    "    elif random_number == 1:\n",
    "        augmented_data[i] = random_cropping_single_window(window_data[i], crop_ratio=0.8)\n",
    "        augmented_labels.append(1)\n",
    "        augmented_subject.append(window_subject[i])\n",
    "    elif random_number == 2:\n",
    "        augmented_data[i] = magnitude_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(2)\n",
    "        augmented_subject.append(window_subject[i])\n",
    "    elif random_number == 3:\n",
    "        augmented_data[i] = time_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(3)\n",
    "        augmented_subject.append(window_subject[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10611, 1: 10923, 2: 10750, 3: 10580}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1290, 1: 1590, 2: 1229, 3: 1792, 4: 1792, 5: 1280, 6: 1280, 7: 1536, 8: 1536, 9: 1024, 10: 1792, 11: 1536, 12: 1536, 13: 1080, 14: 1280, 15: 1380, 16: 1260, 17: 1792, 18: 1280, 19: 1536, 20: 1536, 21: 1280, 22: 399, 23: 346, 24: 384, 25: 336, 26: 319, 27: 486, 28: 332, 29: 319, 30: 319, 31: 372, 32: 330, 33: 328, 34: 350, 35: 340, 36: 344, 37: 422, 38: 408, 39: 425, 40: 369, 41: 377, 42: 403, 43: 354, 44: 381, 45: 401, 46: 416, 47: 410, 48: 372, 49: 394, 50: 365, 51: 426}\n"
     ]
    }
   ],
   "source": [
    "# count distribution of subjects\n",
    "unique, counts = np.unique(augmented_subject, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42864"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10611 + 10923 + 10750 + 10580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of augmented data: (42864, 100, 3)\n",
      "shape of augmented labels: 42864\n",
      "shape of augmented subject: 42864\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of augmented data: {augmented_data.shape}\")\n",
    "print(f\"shape of augmented labels: {len(augmented_labels)}\")\n",
    "print(f\"shape of augmented subject: {len(augmented_subject)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented data and labels\n",
    "# np.save('./data_processing/augmented_data_join.npy', augmented_data)\n",
    "# np.save('./data_processing/augmented_labels_join.npy', augmented_labels)\n",
    "\n",
    "# np.save('./data_processing/augmented_data_join_2.npy', augmented_data) # 2 datasets\n",
    "# np.save('./data_processing/augmented_labels_join_2.npy', augmented_labels) # 2 datasets\n",
    "\n",
    "# np.save('./data_processing/augmented_data_join_3.npy', augmented_data) # 2 datasets, 5 augmentations\n",
    "# np.save('./data_processing/augmented_labels_join_3.npy', augmented_labels) # 2 datasets, 5 augmentations\n",
    "\n",
    "# np.save('./data_processing/augmented_data_join_4.npy', augmented_data) # 2 datasets, 4 augmentations, subject id\n",
    "# np.save('./data_processing/augmented_labels_join_4.npy', augmented_labels) # 2 datasets, 4 augmentations, subject id\n",
    "# np.save('./data_processing/augmented_subject_join_4.npy', augmented_subject) # 2 datasets, 4 augmentations, subject id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReverseLayerF(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "class CNNFeatureExtractorWithAdversarial(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_subjects=52):\n",
    "        super(CNNFeatureExtractorWithAdversarial, self).__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust based on your dataset\n",
    "        \n",
    "        # Activity classifier\n",
    "        self.activity_classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "        # Subject discriminator\n",
    "        self.subject_discriminator_fc1 = nn.Linear(128, 64)\n",
    "        self.subject_discriminator_fc2 = nn.Linear(64, num_subjects)\n",
    "\n",
    "    def forward(self, x, alpha=0.0):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        features = F.relu(self.fc1(x))\n",
    "\n",
    "        activity_output = self.activity_classifier(features)\n",
    "\n",
    "        reversed_features = ReverseLayerF.apply(features, alpha)\n",
    "        subject_output = F.relu(self.subject_discriminator_fc1(reversed_features))\n",
    "        subject_output = self.subject_discriminator_fc2(subject_output)\n",
    "\n",
    "        return activity_output, subject_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCNWithAdversarial(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4, num_subjects=52):\n",
    "        super(TCNWithAdversarial, self).__init__()\n",
    "        # TCN base layers\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_activity = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "        # Subject discriminator\n",
    "        self.fc_subject = nn.Linear(num_channels[-1], 64)  # Intermediate layer for subject discriminator\n",
    "        self.subject_discriminator = nn.Linear(64, num_subjects)\n",
    "\n",
    "    def forward(self, x, alpha=0.0):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        activity_output = self.fc_activity(x)\n",
    "\n",
    "        # Adversarial branch for subject discrimination\n",
    "        reversed_features = ReverseLayerF.apply(x, alpha)\n",
    "        subject_features = F.relu(self.fc_subject(reversed_features))\n",
    "        subject_output = self.subject_discriminator(subject_features)\n",
    "\n",
    "        return activity_output, subject_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmented data and labels\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join.npy', allow_pickle=True)\n",
    "\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join_2.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join_2.npy', allow_pickle=True)\n",
    "\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join_3.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join_3.npy', allow_pickle=True)\n",
    "\n",
    "augmented_data = np.load('./data_processing/augmented_data_join_4.npy', allow_pickle=True)\n",
    "augmented_labels = np.load('./data_processing/augmented_labels_join_4.npy', allow_pickle=True)\n",
    "augmented_subject = np.load('./data_processing/augmented_subject_join_4.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864, 100, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_subject.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10611, 1: 10923, 2: 10750, 3: 10580}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1290, 1: 1590, 2: 1229, 3: 1792, 4: 1792, 5: 1280, 6: 1280, 7: 1536, 8: 1536, 9: 1024, 10: 1792, 11: 1536, 12: 1536, 13: 1080, 14: 1280, 15: 1380, 16: 1260, 17: 1792, 18: 1280, 19: 1536, 20: 1536, 21: 1280, 22: 399, 23: 346, 24: 384, 25: 336, 26: 319, 27: 486, 28: 332, 29: 319, 30: 319, 31: 372, 32: 330, 33: 328, 34: 350, 35: 340, 36: 344, 37: 422, 38: 408, 39: 425, 40: 369, 41: 377, 42: 403, 43: 354, 44: 381, 45: 401, 46: 416, 47: 410, 48: 372, 49: 394, 50: 365, 51: 426}\n"
     ]
    }
   ],
   "source": [
    "# count distribution of subjects\n",
    "unique, counts = np.unique(augmented_subject, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of arrays to a single numpy array\n",
    "# if isinstance(augmented_data, list):\n",
    "#     augmented_data = np.stack(augmented_data)\n",
    "\n",
    "# # Ensure the data type is float32\n",
    "# augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "# augmented_labels_tensor = torch.from_numpy(np.array(augmented_labels)).long()\n",
    "\n",
    "# # split data into train and test set\n",
    "# train_size = int(0.8 * augmented_data.shape[0])\n",
    "# test_size = augmented_data.shape[0] - train_size\n",
    "# train_data, test_data = torch.utils.data.random_split(augmented_data, [train_size, test_size])\n",
    "# train_labels, test_labels = torch.utils.data.random_split(augmented_labels, [train_size, test_size])\n",
    "\n",
    "# # create train and test dataset\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # create train and test dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # check if GPU is available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming augmented_data and augmented_labels are numpy arrays\n",
    "augmented_data = augmented_data.astype(np.float32)\n",
    "augmented_subject = augmented_subject.astype(np.int64)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "augmented_labels_tensor = torch.from_numpy(augmented_labels)\n",
    "#convert labels to long\n",
    "augmented_labels_tensor = augmented_labels_tensor.long()\n",
    "augmented_subject_tensor = torch.from_numpy(augmented_subject).long()\n",
    "\n",
    "# Create a dataset with data, labels, and subject IDs\n",
    "dataset = TensorDataset(augmented_data_tensor, augmented_labels_tensor, augmented_subject_tensor)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_loader: 268\n",
      "shape of test_loader: 67\n"
     ]
    }
   ],
   "source": [
    "#  print the shape of train_loader and test_loader\n",
    "print(f\"shape of train_loader: {len(train_loader)}\")\n",
    "print(f\"shape of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(model, train_loader, criterion_activity, criterion_subject, optimizer, alpha, device):\n",
    "    model.train()\n",
    "    running_loss_activity = 0.0\n",
    "    running_loss_subject = 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels, subjects) in enumerate(train_loader):\n",
    "        inputs, labels, subjects = inputs.to(device), labels.to(device), subjects.to(device)\n",
    "        inputs = inputs.transpose(1, 2)  # Adjust dimensions if necessary\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get outputs for both tasks\n",
    "        activity_output, subject_output = model(inputs, alpha)\n",
    "\n",
    "        # Calculate separate losses\n",
    "        loss_activity = criterion_activity(activity_output, labels)\n",
    "        loss_subject = criterion_subject(subject_output, subjects)\n",
    "\n",
    "        # Adversarial training: Update model to minimize activity loss and subject loss\n",
    "        # Note: For adversarial learning, you might adjust the weight of loss_subject or apply gradient reversal within the model\n",
    "        total_loss = loss_activity - (alpha * loss_subject)  # Subtract subject loss to promote subject-invariant features\n",
    "\n",
    "        # Backward and optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics or accumulate losses\n",
    "        running_loss_activity += loss_activity.item()\n",
    "        running_loss_subject += loss_subject.item()\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss_activity = running_loss_activity / len(train_loader)\n",
    "    avg_loss_subject = running_loss_subject / len(train_loader)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    return avg_loss_activity, avg_loss_subject, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test_adversarial(model, test_loader, criterion_activity, criterion_subject, alpha, device):\n",
    "    model.eval()\n",
    "    running_loss_activity = 0.0\n",
    "    running_loss_subject = 0.0\n",
    "    #calculate accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels, subjects = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            activity_output, subject_output = model(inputs, alpha)\n",
    "            loss_activity = criterion_activity(activity_output, labels)\n",
    "            loss_subject = criterion_subject(subject_output, subjects)\n",
    "\n",
    "            running_loss_activity += loss_activity.item()\n",
    "            running_loss_subject += loss_subject.item()\n",
    "\n",
    "            #calculate accuracy\n",
    "            _, predicted = torch.max(activity_output.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return running_loss_activity / len(test_loader), running_loss_subject / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion_activity, criterion_subject, optimizer, alpha, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_act_loss, train_sub_loss, train_loss = train_adversarial(model, train_loader, criterion_activity, criterion_subject, optimizer, alpha, device)\n",
    "        test_act_loss, test_sub_los, test_accuracy = test_adversarial(model, test_loader, criterion_activity, criterion_subject, alpha, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_act_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. Train Act Loss: {train_act_loss:.3f}.. Train Subj Loss: {train_sub_loss:.3f}.. Test Act Loss: {test_act_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}\")\n",
    "    return train_losses, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50.. Train Loss: 0.849.. Train Act Loss: 1.344.. Train Subj Loss: 3.963.. Test Act Loss: 1.296.. Test Accuracy: 0.449\n",
      "Epoch: 2/50.. Train Loss: 0.732.. Train Act Loss: 1.228.. Train Subj Loss: 3.966.. Test Act Loss: 1.146.. Test Accuracy: 0.527\n",
      "Epoch: 3/50.. Train Loss: 0.579.. Train Act Loss: 1.076.. Train Subj Loss: 3.973.. Test Act Loss: 1.019.. Test Accuracy: 0.537\n",
      "Epoch: 4/50.. Train Loss: 0.494.. Train Act Loss: 0.992.. Train Subj Loss: 3.984.. Test Act Loss: 0.971.. Test Accuracy: 0.536\n",
      "Epoch: 5/50.. Train Loss: 0.460.. Train Act Loss: 0.959.. Train Subj Loss: 3.995.. Test Act Loss: 0.949.. Test Accuracy: 0.565\n",
      "Epoch: 6/50.. Train Loss: 0.441.. Train Act Loss: 0.942.. Train Subj Loss: 4.006.. Test Act Loss: 0.935.. Test Accuracy: 0.533\n",
      "Epoch: 7/50.. Train Loss: 0.427.. Train Act Loss: 0.929.. Train Subj Loss: 4.018.. Test Act Loss: 0.922.. Test Accuracy: 0.555\n",
      "Epoch: 8/50.. Train Loss: 0.414.. Train Act Loss: 0.918.. Train Subj Loss: 4.033.. Test Act Loss: 0.913.. Test Accuracy: 0.533\n",
      "Epoch: 9/50.. Train Loss: 0.402.. Train Act Loss: 0.908.. Train Subj Loss: 4.052.. Test Act Loss: 0.902.. Test Accuracy: 0.579\n",
      "Epoch: 10/50.. Train Loss: 0.389.. Train Act Loss: 0.898.. Train Subj Loss: 4.073.. Test Act Loss: 0.891.. Test Accuracy: 0.590\n",
      "Epoch: 11/50.. Train Loss: 0.376.. Train Act Loss: 0.888.. Train Subj Loss: 4.101.. Test Act Loss: 0.881.. Test Accuracy: 0.586\n",
      "Epoch: 12/50.. Train Loss: 0.361.. Train Act Loss: 0.878.. Train Subj Loss: 4.139.. Test Act Loss: 0.871.. Test Accuracy: 0.594\n",
      "Epoch: 13/50.. Train Loss: 0.345.. Train Act Loss: 0.868.. Train Subj Loss: 4.186.. Test Act Loss: 0.861.. Test Accuracy: 0.596\n",
      "Epoch: 14/50.. Train Loss: 0.327.. Train Act Loss: 0.859.. Train Subj Loss: 4.254.. Test Act Loss: 0.853.. Test Accuracy: 0.581\n",
      "Epoch: 15/50.. Train Loss: 0.308.. Train Act Loss: 0.850.. Train Subj Loss: 4.342.. Test Act Loss: 0.844.. Test Accuracy: 0.585\n",
      "Epoch: 16/50.. Train Loss: 0.283.. Train Act Loss: 0.841.. Train Subj Loss: 4.470.. Test Act Loss: 0.837.. Test Accuracy: 0.586\n",
      "Epoch: 17/50.. Train Loss: 0.253.. Train Act Loss: 0.834.. Train Subj Loss: 4.645.. Test Act Loss: 0.829.. Test Accuracy: 0.606\n",
      "Epoch: 18/50.. Train Loss: 0.216.. Train Act Loss: 0.827.. Train Subj Loss: 4.890.. Test Act Loss: 0.822.. Test Accuracy: 0.610\n",
      "Epoch: 19/50.. Train Loss: 0.167.. Train Act Loss: 0.821.. Train Subj Loss: 5.233.. Test Act Loss: 0.816.. Test Accuracy: 0.619\n",
      "Epoch: 20/50.. Train Loss: 0.091.. Train Act Loss: 0.816.. Train Subj Loss: 5.808.. Test Act Loss: 0.816.. Test Accuracy: 0.619\n",
      "Epoch: 21/50.. Train Loss: -0.046.. Train Act Loss: 0.816.. Train Subj Loss: 6.896.. Test Act Loss: 0.820.. Test Accuracy: 0.618\n",
      "Epoch: 22/50.. Train Loss: -0.123.. Train Act Loss: 0.826.. Train Subj Loss: 7.590.. Test Act Loss: 0.834.. Test Accuracy: 0.621\n",
      "Epoch: 23/50.. Train Loss: -0.031.. Train Act Loss: 0.838.. Train Subj Loss: 6.950.. Test Act Loss: 0.842.. Test Accuracy: 0.623\n",
      "Epoch: 24/50.. Train Loss: -0.056.. Train Act Loss: 0.844.. Train Subj Loss: 7.198.. Test Act Loss: 0.849.. Test Accuracy: 0.635\n",
      "Epoch: 25/50.. Train Loss: -0.098.. Train Act Loss: 0.850.. Train Subj Loss: 7.583.. Test Act Loss: 0.853.. Test Accuracy: 0.631\n",
      "Epoch: 26/50.. Train Loss: -0.174.. Train Act Loss: 0.854.. Train Subj Loss: 8.227.. Test Act Loss: 0.858.. Test Accuracy: 0.626\n",
      "Epoch: 27/50.. Train Loss: -0.251.. Train Act Loss: 0.860.. Train Subj Loss: 8.886.. Test Act Loss: 0.866.. Test Accuracy: 0.614\n",
      "Epoch: 28/50.. Train Loss: -0.314.. Train Act Loss: 0.866.. Train Subj Loss: 9.440.. Test Act Loss: 0.872.. Test Accuracy: 0.634\n",
      "Epoch: 29/50.. Train Loss: -0.368.. Train Act Loss: 0.873.. Train Subj Loss: 9.923.. Test Act Loss: 0.877.. Test Accuracy: 0.631\n",
      "Epoch: 30/50.. Train Loss: -0.402.. Train Act Loss: 0.880.. Train Subj Loss: 10.254.. Test Act Loss: 0.885.. Test Accuracy: 0.618\n",
      "Epoch: 31/50.. Train Loss: -0.422.. Train Act Loss: 0.887.. Train Subj Loss: 10.475.. Test Act Loss: 0.894.. Test Accuracy: 0.625\n",
      "Epoch: 32/50.. Train Loss: -0.445.. Train Act Loss: 0.893.. Train Subj Loss: 10.703.. Test Act Loss: 0.894.. Test Accuracy: 0.607\n",
      "Epoch: 33/50.. Train Loss: -0.490.. Train Act Loss: 0.897.. Train Subj Loss: 11.099.. Test Act Loss: 0.902.. Test Accuracy: 0.614\n",
      "Epoch: 34/50.. Train Loss: -0.540.. Train Act Loss: 0.901.. Train Subj Loss: 11.528.. Test Act Loss: 0.907.. Test Accuracy: 0.625\n",
      "Epoch: 35/50.. Train Loss: -0.612.. Train Act Loss: 0.903.. Train Subj Loss: 12.119.. Test Act Loss: 0.908.. Test Accuracy: 0.620\n",
      "Epoch: 36/50.. Train Loss: -0.682.. Train Act Loss: 0.907.. Train Subj Loss: 12.712.. Test Act Loss: 0.917.. Test Accuracy: 0.628\n",
      "Epoch: 37/50.. Train Loss: -0.756.. Train Act Loss: 0.910.. Train Subj Loss: 13.330.. Test Act Loss: 0.927.. Test Accuracy: 0.623\n",
      "Epoch: 38/50.. Train Loss: -0.811.. Train Act Loss: 0.913.. Train Subj Loss: 13.793.. Test Act Loss: 0.919.. Test Accuracy: 0.613\n",
      "Epoch: 39/50.. Train Loss: -0.919.. Train Act Loss: 0.916.. Train Subj Loss: 14.682.. Test Act Loss: 0.924.. Test Accuracy: 0.627\n",
      "Epoch: 40/50.. Train Loss: -1.008.. Train Act Loss: 0.919.. Train Subj Loss: 15.414.. Test Act Loss: 0.925.. Test Accuracy: 0.627\n",
      "Epoch: 41/50.. Train Loss: -1.116.. Train Act Loss: 0.922.. Train Subj Loss: 16.305.. Test Act Loss: 0.934.. Test Accuracy: 0.618\n",
      "Epoch: 42/50.. Train Loss: -1.198.. Train Act Loss: 0.928.. Train Subj Loss: 17.008.. Test Act Loss: 0.941.. Test Accuracy: 0.636\n",
      "Epoch: 43/50.. Train Loss: -1.290.. Train Act Loss: 0.934.. Train Subj Loss: 17.787.. Test Act Loss: 0.940.. Test Accuracy: 0.634\n",
      "Epoch: 44/50.. Train Loss: -1.384.. Train Act Loss: 0.938.. Train Subj Loss: 18.579.. Test Act Loss: 0.939.. Test Accuracy: 0.623\n",
      "Epoch: 45/50.. Train Loss: -1.495.. Train Act Loss: 0.938.. Train Subj Loss: 19.465.. Test Act Loss: 0.950.. Test Accuracy: 0.615\n",
      "Epoch: 46/50.. Train Loss: -1.639.. Train Act Loss: 0.941.. Train Subj Loss: 20.642.. Test Act Loss: 0.951.. Test Accuracy: 0.611\n",
      "Epoch: 47/50.. Train Loss: -1.774.. Train Act Loss: 0.945.. Train Subj Loss: 21.754.. Test Act Loss: 0.957.. Test Accuracy: 0.626\n",
      "Epoch: 48/50.. Train Loss: -1.898.. Train Act Loss: 0.951.. Train Subj Loss: 22.794.. Test Act Loss: 0.963.. Test Accuracy: 0.620\n",
      "Epoch: 49/50.. Train Loss: -2.034.. Train Act Loss: 0.955.. Train Subj Loss: 23.915.. Test Act Loss: 0.960.. Test Accuracy: 0.621\n",
      "Epoch: 50/50.. Train Loss: -2.127.. Train Act Loss: 0.961.. Train Subj Loss: 24.705.. Test Act Loss: 0.966.. Test Accuracy: 0.614\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "num_subjects = 52\n",
    "\n",
    "model = CNNFeatureExtractorWithAdversarial(num_classes=num_classes, num_subjects=num_subjects)\n",
    "\n",
    "# move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "criterion_subject = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "alpha = 0.125\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 50\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion_activity, criterion_subject, optimizer, alpha, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"cnn_feature_extractor_join_2_dataset.pt\"\n",
    "# model_name = f\"cnn_feature_extractor_join_2_dataset_5_augmentations.pt\"\n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "model = TCNWithAdversarial(num_inputs, num_channels, kernel_size, num_classes=num_classes, num_subjects=num_subjects).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "criterion_subject = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00003)  # Replace lr with your learning rate\n",
    "alpha = 0.125\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 85\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion_activity, criterion_subject, optimizer, alpha, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"tcn_join_2_dataset.pt\"\n",
    "# model_name = f\"tcn_join_2_dataset_5_augmentations.pt\"\n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
