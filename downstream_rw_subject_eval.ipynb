{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique subjects is 15\n"
     ]
    }
   ],
   "source": [
    "# print the count of unique subjects\n",
    "print('The number of unique subjects is {}'.format(data['subject_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(data['activity'])\n",
    "data['encoded_activity'] = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>encoded_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity  encoded_activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up                 1\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up                 1\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up                 1\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up                 1\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up                 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of classes\n",
    "num_classes = data['encoded_activity'].nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the mean and std for normalization\n",
    "mean = {'acc_x': 0.816012, 'acc_y': -0.007595, 'acc_z': 0.074082}\n",
    "std = {'acc_x': 0.398664, 'acc_y': 0.375481, 'acc_z': 0.366527}\n",
    "\n",
    "# normalize the data for acc_x, acc_y, acc_z\n",
    "data['acc_x'] = (data['acc_x'] - mean['acc_x']) / std['acc_x']\n",
    "data['acc_y'] = (data['acc_y'] - mean['acc_y']) / std['acc_y']\n",
    "data['acc_z'] = (data['acc_z'] - mean['acc_z']) / std['acc_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id          0\n",
       "acc_x               0\n",
       "acc_y               0\n",
       "acc_z               0\n",
       "activity            0\n",
       "encoded_activity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train users is 10\n",
      "The number of test users is 5\n",
      "The shape of train is (2200794, 6)\n",
      "The shape of test is (1000009, 6)\n"
     ]
    }
   ],
   "source": [
    "# split train and test users\n",
    "# randomly select 70% of the users for training\n",
    "train_subjects = np.random.choice(data['subject_id'].unique(), int(0.7*len(data['subject_id'].unique())), replace=False)\n",
    "# split the data into train and test\n",
    "train = data[data['subject_id'].isin(train_subjects)]\n",
    "test = data[~data['subject_id'].isin(train_subjects)]\n",
    "\n",
    "# print test and train users\n",
    "print('The number of train users is {}'.format(train['subject_id'].nunique()))\n",
    "print('The number of test users is {}'.format(test['subject_id'].nunique()))\n",
    "\n",
    "# print the shape of train and test\n",
    "print('The shape of train is {}'.format(train.shape))\n",
    "print('The shape of test is {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 14,  5,  0, 11,  4, 10, 12,  7, 13], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test subjects are [1 3 6 8 9]\n"
     ]
    }
   ],
   "source": [
    "# print the test subjects\n",
    "print('The test subjects are {}'.format(test['subject_id'].unique()))\n",
    "\n",
    "# [1 3 6 8 9] are the test subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train subjects are [ 0  2  4  5  7 11 12]\n",
      "The shape of train_75 is (1579134, 6)\n"
     ]
    }
   ],
   "source": [
    "# randomly select 75% of the users for training\n",
    "train_subjects_75 = np.random.choice(train['subject_id'].unique(), int(0.75*len(train['subject_id'].unique())), replace=False)\n",
    "train_75 = data[data['subject_id'].isin(train_subjects_75)]\n",
    "\n",
    "# print the train subjects\n",
    "print('The train subjects are {}'.format(train_75['subject_id'].unique()))\n",
    "#print shape of train_75\n",
    "print('The shape of train_75 is {}'.format(train_75.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train subjects are [ 2  5  7 11 12]\n",
      "The shape of train_50 is (1118653, 6)\n"
     ]
    }
   ],
   "source": [
    "# randomly select 50% of the users for training\n",
    "train_subjects_50 = np.random.choice(train['subject_id'].unique(), int(0.5*len(train['subject_id'].unique())), replace=False)\n",
    "train_50 = data[data['subject_id'].isin(train_subjects_50)]\n",
    "\n",
    "# print the train subjects\n",
    "print('The train subjects are {}'.format(train_50['subject_id'].unique()))\n",
    "#print shape of train_50\n",
    "print('The shape of train_50 is {}'.format(train_50.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train subjects are [10 14]\n",
      "The shape of train_25 is (442378, 6)\n"
     ]
    }
   ],
   "source": [
    "# randomly select 25% of the users for training\n",
    "train_subjects_25 = np.random.choice(train['subject_id'].unique(), int(0.25*len(train['subject_id'].unique())), replace=False)\n",
    "train_25 = data[data['subject_id'].isin(train_subjects_25)]\n",
    "\n",
    "# print the train subjects\n",
    "print('The train subjects are {}'.format(train_25['subject_id'].unique()))\n",
    "#print shape of train_25\n",
    "print('The shape of train_25 is {}'.format(train_25.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train subjects are [14]\n",
      "The shape of train_10 is (222116, 6)\n"
     ]
    }
   ],
   "source": [
    "# randomly select 10% of the users for training\n",
    "train_subjects_10 = np.random.choice(train['subject_id'].unique(), int(0.1*len(train['subject_id'].unique())), replace=False)\n",
    "train_10 = data[data['subject_id'].isin(train_subjects_10)]\n",
    "\n",
    "# print the train subjects\n",
    "print('The train subjects are {}'.format(train_10['subject_id'].unique()))\n",
    "#print shape of train_10\n",
    "print('The shape of train_10 is {}'.format(train_10.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array along with the mode label for each window.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows, indices, and labels\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    labels = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * win_len)\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        window = data[curr:curr + win_len]\n",
    "        windows.append(window.iloc[:, :-2])  # Exclude the last two columns (original and encoded labels)\n",
    "        indices.append([curr, curr + win_len])\n",
    "        \n",
    "        # Extract and compute the mode of the encoded labels for the current window\n",
    "        window_labels = window['encoded_activity']\n",
    "        mode_result = mode(window_labels)\n",
    "        window_label = mode_result[0] if mode_result[0].size > 0 else mode_result\n",
    "        labels.append(window_label)\n",
    "\n",
    "        curr += win_len - overlapping_elements\n",
    "\n",
    "    result_windows = np.array(windows)\n",
    "    result_indices = np.array(indices)\n",
    "    result_labels = np.array(labels)\n",
    "    return result_windows, result_indices, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train window dataset (8 sec with 50% overlap): (11002, 400, 4)\n",
      "shape of test window dataset (8 sec with 50% overlap): (4999, 400, 4)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 8\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 50\n",
    "\n",
    "train_window_data, _, train_window_label = sliding_window_samples(train, window_size, overlap_ratio)\n",
    "print(f\"shape of train window dataset ({time_window} sec with {overlap_ratio}% overlap): {train_window_data.shape}\")\n",
    "\n",
    "test_window_data, _, test_window_label = sliding_window_samples(test, window_size, overlap_ratio)\n",
    "print(f\"shape of test window dataset ({time_window} sec with {overlap_ratio}% overlap): {test_window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -26.06293019,  -5.37906046,   3.46762449],\n",
       "       [  0.        , -26.03897518,  -5.29554092,   3.5996475 ],\n",
       "       [  0.        , -26.00493649,  -5.26486027,   3.64859342],\n",
       "       ...,\n",
       "       [  0.        , -26.13381695,  -4.23423555,   3.95580135],\n",
       "       [  0.        , -26.16785564,  -3.89187469,   3.90532757],\n",
       "       [  0.        , -26.12014629,  -4.00375785,   3.63495186]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        , -25.5075753 ,  -4.43600342,   6.51132932],\n",
       "       [  1.        , -25.69123874,  -4.65707985,   6.63361226],\n",
       "       [  1.        , -25.81781149,  -4.74408825,   6.7427993 ],\n",
       "       ...,\n",
       "       [  1.        , -24.91434391,  -4.57406633,   8.66257602],\n",
       "       [  1.        , -24.89106616,  -4.40130126,   8.7713811 ],\n",
       "       [  1.        , -24.84172637,  -4.05377369,   8.8242285 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the subject column\n",
    "train_window_data = train_window_data[:, :, 1:]\n",
    "test_window_data = test_window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train is (11002, 400, 3)\n",
      "The shape of test is (4999, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of train and test\n",
    "print('The shape of train is {}'.format(train_window_data.shape))\n",
    "print('The shape of test is {}'.format(test_window_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train label is 11002\n",
      "The length of test label is 4999\n"
     ]
    }
   ],
   "source": [
    "# length of train and test label\n",
    "print('The length of train label is {}'.format(len(train_window_label)))\n",
    "print('The length of test label is {}'.format(len(test_window_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sliding_window_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m window_size \u001b[38;5;241m=\u001b[39m sampling_rate \u001b[38;5;241m*\u001b[39m time_window\n\u001b[0;32m      4\u001b[0m overlap_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 6\u001b[0m train_window_data_75, _, train_window_label_75 \u001b[38;5;241m=\u001b[39m \u001b[43msliding_window_samples\u001b[49m(train_75, window_size, overlap_ratio)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of train window dataset (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverlap_ratio\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% overlap): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_window_data_75\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sliding_window_samples' is not defined"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 8\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 50\n",
    "\n",
    "train_window_data_75, _, train_window_label_75 = sliding_window_samples(train_75, window_size, overlap_ratio)\n",
    "print(f\"shape of train window dataset ({time_window} sec with {overlap_ratio}% overlap): {train_window_data_75.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the subject column\n",
    "train_window_data_75 = train_window_data_75[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of train_75 is {}'.format(train_window_data_75.shape))\n",
    "print('The length of train_75 label is {}'.format(len(train_window_label_75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_data_50, _, train_window_label_50 = sliding_window_samples(train_50, window_size, overlap_ratio)\n",
    "print(f\"shape of train window dataset ({time_window} sec with {overlap_ratio}% overlap): {train_window_data_50.shape}\")\n",
    "\n",
    "# remove the subject column\n",
    "train_window_data_50 = train_window_data_50[:, :, 1:]\n",
    "\n",
    "print('The shape of train_50 is {}'.format(train_window_data_50.shape))\n",
    "print('The length of train_50 label is {}'.format(len(train_window_label_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_data_25, _, train_window_label_25 = sliding_window_samples(train_25, window_size, overlap_ratio)\n",
    "# remove the subject column\n",
    "train_window_data_25 = train_window_data_25[:, :, 1:]\n",
    "\n",
    "print('The shape of train_25 is {}'.format(train_window_data_25.shape))\n",
    "print('The length of train_25 label is {}'.format(len(train_window_label_25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_data_10, _, train_window_label_10 = sliding_window_samples(train_10, window_size, overlap_ratio)\n",
    "# remove the subject column\n",
    "train_window_data_10 = train_window_data_10[:, :, 1:]\n",
    "\n",
    "print('The shape of train_10 is {}'.format(train_window_data_10.shape))\n",
    "print('The length of train_10 label is {}'.format(len(train_window_label_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subset of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of sampled train label is (8251,)\n",
      "The shape of sampled train is (8251, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sample size\n",
    "sample_size = int(0.75 * len(train_window_data))\n",
    "\n",
    "# Generate random indices\n",
    "indices = random.sample(range(len(train_window_data)), sample_size)\n",
    "\n",
    "# Sample the data and labels\n",
    "sampled_train_window_data_75 = [train_window_data[i] for i in indices]\n",
    "sampled_train_window_label_75 = [train_window_label[i] for i in indices]\n",
    "\n",
    "# print the shape of sampled train data and label\n",
    "print('The shape of sampled train label is {}'.format(np.array(sampled_train_window_label_75).shape))\n",
    "print('The shape of sampled train is {}'.format(np.array(sampled_train_window_data_75).shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of sampled train label is (5501,)\n",
      "The shape of sampled train is (5501, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sample size\n",
    "sample_size = int(0.5 * len(train_window_data))\n",
    "\n",
    "# Generate random indices\n",
    "indices = random.sample(range(len(train_window_data)), sample_size)\n",
    "\n",
    "# Sample the data and labels\n",
    "sampled_train_window_data_50 = [train_window_data[i] for i in indices]\n",
    "sampled_train_window_label_50 = [train_window_label[i] for i in indices]\n",
    "\n",
    "# print the shape of sampled train data and label\n",
    "print('The shape of sampled train label is {}'.format(np.array(sampled_train_window_label_50).shape))\n",
    "print('The shape of sampled train is {}'.format(np.array(sampled_train_window_data_50).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of sampled train label is (2750,)\n",
      "The shape of sampled train is (2750, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sample size\n",
    "sample_size = int(0.25 * len(train_window_data))\n",
    "\n",
    "# Generate random indices\n",
    "indices = random.sample(range(len(train_window_data)), sample_size)\n",
    "\n",
    "# Sample the data and labels\n",
    "sampled_train_window_data_25 = [train_window_data[i] for i in indices]\n",
    "sampled_train_window_label_25 = [train_window_label[i] for i in indices]\n",
    "\n",
    "# print the shape of sampled train data and label\n",
    "print('The shape of sampled train label is {}'.format(np.array(sampled_train_window_label_25).shape))\n",
    "print('The shape of sampled train is {}'.format(np.array(sampled_train_window_data_25).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of sampled train label is (1100,)\n",
      "The shape of sampled train is (1100, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sample size\n",
    "sample_size = int(0.1 * len(train_window_data))\n",
    "\n",
    "# Generate random indices\n",
    "indices = random.sample(range(len(train_window_data)), sample_size)\n",
    "\n",
    "# Sample the data and labels\n",
    "sampled_train_window_data_10 = [train_window_data[i] for i in indices]\n",
    "sampled_train_window_label_10 = [train_window_label[i] for i in indices]\n",
    "\n",
    "# print the shape of sampled train data and label\n",
    "print('The shape of sampled train label is {}'.format(np.array(sampled_train_window_label_10).shape))\n",
    "print('The shape of sampled train is {}'.format(np.array(sampled_train_window_data_10).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataloader for train and test\n",
    "def generate_dataloader(data, label, batch_size, is_shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate dataloader for train and test\n",
    "\n",
    "    :param data: input data\n",
    "    :param label: input label\n",
    "    :param batch_size: batch size\n",
    "    :return: train and test dataloader\n",
    "    \"\"\"\n",
    "    # Check if data and label are lists, and convert them to NumPy arrays if they are\n",
    "    if isinstance(data, list):\n",
    "        data = np.array(data)\n",
    "    if isinstance(label, list):\n",
    "        label = np.array(label)\n",
    "    \n",
    "    # Convert data and label to tensor\n",
    "    data_tensor = torch.from_numpy(data).float()  # Ensure data is converted to float for PyTorch\n",
    "    label_tensor = torch.from_numpy(label).long()  # Labels typically converted to long for classification tasks\n",
    "    \n",
    "    # Generate dataloader\n",
    "    dataset = TensorDataset(data_tensor, label_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataloader for train and test\n",
    "train_dataloader = generate_dataloader(train_window_data, train_window_label, batch_size)   \n",
    "test_dataloader = generate_dataloader(test_window_data, test_window_label, batch_size, is_shuffle=False)\n",
    "\n",
    "# generate dataloader for train sampled data and label\n",
    "train_dataloader_75 = generate_dataloader(sampled_train_window_data_75, sampled_train_window_label_75, batch_size)\n",
    "train_dataloader_50 = generate_dataloader(sampled_train_window_data_50, sampled_train_window_label_50, batch_size)\n",
    "train_dataloader_25 = generate_dataloader(sampled_train_window_data_25, sampled_train_window_label_25, batch_size)\n",
    "train_dataloader_10 = generate_dataloader(sampled_train_window_data_10, sampled_train_window_label_10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(12800, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.transpose(1, 2)  # Assuming this is necessary for your model\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect all true labels and predictions for F1 score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    # Calculate F1 score. You might need to adjust the 'average' parameter based on your task\n",
    "    # For binary classification, you can use 'binary'. For multi-class, consider 'macro' or 'weighted'\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return running_loss / len(test_loader), accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_f1_scores = []  # List to store F1-scores for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Testing phase\n",
    "        test_loss, test_accuracy, test_f1 = test(model, test_loader, criterion, device)  # Modified to receive F1-score\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_f1_scores.append(test_f1)  # Store the F1-score\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. \"\n",
    "              f\"Test Loss: {test_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}.. Test F1 Score: {test_f1:.3f}\")\n",
    "\n",
    "    return train_losses, test_losses, test_accuracies, test_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 0.855.. Test Loss: 1.019.. Test Accuracy: 0.677.. Test F1 Score: 0.668\n",
      "Epoch: 2/30.. Train Loss: 0.525.. Test Loss: 1.008.. Test Accuracy: 0.682.. Test F1 Score: 0.691\n",
      "Epoch: 3/30.. Train Loss: 0.442.. Test Loss: 1.080.. Test Accuracy: 0.705.. Test F1 Score: 0.714\n",
      "Epoch: 4/30.. Train Loss: 0.345.. Test Loss: 1.228.. Test Accuracy: 0.722.. Test F1 Score: 0.713\n",
      "Epoch: 5/30.. Train Loss: 0.290.. Test Loss: 1.189.. Test Accuracy: 0.734.. Test F1 Score: 0.736\n",
      "Epoch: 6/30.. Train Loss: 0.257.. Test Loss: 1.392.. Test Accuracy: 0.747.. Test F1 Score: 0.753\n",
      "Epoch: 7/30.. Train Loss: 0.240.. Test Loss: 1.307.. Test Accuracy: 0.732.. Test F1 Score: 0.738\n",
      "Epoch: 8/30.. Train Loss: 0.232.. Test Loss: 1.524.. Test Accuracy: 0.682.. Test F1 Score: 0.694\n",
      "Epoch: 9/30.. Train Loss: 0.193.. Test Loss: 1.395.. Test Accuracy: 0.722.. Test F1 Score: 0.725\n",
      "Epoch: 10/30.. Train Loss: 0.175.. Test Loss: 1.560.. Test Accuracy: 0.688.. Test F1 Score: 0.686\n",
      "Epoch: 11/30.. Train Loss: 0.204.. Test Loss: 1.365.. Test Accuracy: 0.735.. Test F1 Score: 0.731\n",
      "Epoch: 12/30.. Train Loss: 0.161.. Test Loss: 1.965.. Test Accuracy: 0.621.. Test F1 Score: 0.620\n",
      "Epoch: 13/30.. Train Loss: 0.173.. Test Loss: 1.629.. Test Accuracy: 0.684.. Test F1 Score: 0.681\n",
      "Epoch: 14/30.. Train Loss: 0.135.. Test Loss: 2.002.. Test Accuracy: 0.650.. Test F1 Score: 0.647\n",
      "Epoch: 15/30.. Train Loss: 0.120.. Test Loss: 2.140.. Test Accuracy: 0.629.. Test F1 Score: 0.628\n",
      "Epoch: 16/30.. Train Loss: 0.127.. Test Loss: 1.849.. Test Accuracy: 0.704.. Test F1 Score: 0.711\n",
      "Epoch: 17/30.. Train Loss: 0.131.. Test Loss: 2.478.. Test Accuracy: 0.645.. Test F1 Score: 0.656\n",
      "Epoch: 18/30.. Train Loss: 0.181.. Test Loss: 2.260.. Test Accuracy: 0.639.. Test F1 Score: 0.648\n",
      "Epoch: 19/30.. Train Loss: 0.109.. Test Loss: 2.188.. Test Accuracy: 0.721.. Test F1 Score: 0.724\n",
      "Epoch: 20/30.. Train Loss: 0.099.. Test Loss: 2.773.. Test Accuracy: 0.631.. Test F1 Score: 0.634\n",
      "Epoch: 21/30.. Train Loss: 0.110.. Test Loss: 2.283.. Test Accuracy: 0.651.. Test F1 Score: 0.653\n",
      "Epoch: 22/30.. Train Loss: 0.169.. Test Loss: 1.924.. Test Accuracy: 0.665.. Test F1 Score: 0.670\n",
      "Epoch: 23/30.. Train Loss: 0.113.. Test Loss: 2.744.. Test Accuracy: 0.670.. Test F1 Score: 0.673\n",
      "Epoch: 24/30.. Train Loss: 0.086.. Test Loss: 2.600.. Test Accuracy: 0.660.. Test F1 Score: 0.658\n",
      "Epoch: 25/30.. Train Loss: 0.082.. Test Loss: 2.510.. Test Accuracy: 0.663.. Test F1 Score: 0.663\n",
      "Epoch: 26/30.. Train Loss: 0.084.. Test Loss: 2.693.. Test Accuracy: 0.679.. Test F1 Score: 0.675\n",
      "Epoch: 27/30.. Train Loss: 0.090.. Test Loss: 3.393.. Test Accuracy: 0.631.. Test F1 Score: 0.633\n",
      "Epoch: 28/30.. Train Loss: 0.083.. Test Loss: 2.285.. Test Accuracy: 0.720.. Test F1 Score: 0.718\n",
      "Epoch: 29/30.. Train Loss: 0.134.. Test Loss: 2.262.. Test Accuracy: 0.685.. Test F1 Score: 0.688\n",
      "Epoch: 30/30.. Train Loss: 0.076.. Test Loss: 2.413.. Test Accuracy: 0.717.. Test F1 Score: 0.720\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies, test_f1_scores = train_and_test(model, train_dataloader, test_dataloader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 0.811.. Test Loss: 0.997.. Test Accuracy: 0.652.. Test F1 Score: 0.652\n",
      "Epoch: 2/30.. Train Loss: 0.506.. Test Loss: 1.295.. Test Accuracy: 0.670.. Test F1 Score: 0.659\n",
      "Epoch: 3/30.. Train Loss: 0.390.. Test Loss: 1.314.. Test Accuracy: 0.635.. Test F1 Score: 0.642\n",
      "Epoch: 4/30.. Train Loss: 0.327.. Test Loss: 1.193.. Test Accuracy: 0.674.. Test F1 Score: 0.682\n",
      "Epoch: 5/30.. Train Loss: 0.311.. Test Loss: 1.013.. Test Accuracy: 0.722.. Test F1 Score: 0.727\n",
      "Epoch: 6/30.. Train Loss: 0.244.. Test Loss: 1.272.. Test Accuracy: 0.648.. Test F1 Score: 0.653\n",
      "Epoch: 7/30.. Train Loss: 0.224.. Test Loss: 1.226.. Test Accuracy: 0.653.. Test F1 Score: 0.657\n",
      "Epoch: 8/30.. Train Loss: 0.181.. Test Loss: 1.507.. Test Accuracy: 0.645.. Test F1 Score: 0.651\n",
      "Epoch: 9/30.. Train Loss: 0.163.. Test Loss: 1.869.. Test Accuracy: 0.631.. Test F1 Score: 0.633\n",
      "Epoch: 10/30.. Train Loss: 0.154.. Test Loss: 1.895.. Test Accuracy: 0.668.. Test F1 Score: 0.677\n",
      "Epoch: 11/30.. Train Loss: 0.235.. Test Loss: 2.007.. Test Accuracy: 0.674.. Test F1 Score: 0.681\n",
      "Epoch: 12/30.. Train Loss: 0.142.. Test Loss: 2.184.. Test Accuracy: 0.622.. Test F1 Score: 0.612\n",
      "Epoch: 13/30.. Train Loss: 0.139.. Test Loss: 2.082.. Test Accuracy: 0.639.. Test F1 Score: 0.641\n",
      "Epoch: 14/30.. Train Loss: 0.150.. Test Loss: 2.018.. Test Accuracy: 0.659.. Test F1 Score: 0.667\n",
      "Epoch: 15/30.. Train Loss: 0.125.. Test Loss: 2.431.. Test Accuracy: 0.663.. Test F1 Score: 0.668\n",
      "Epoch: 16/30.. Train Loss: 0.119.. Test Loss: 2.536.. Test Accuracy: 0.654.. Test F1 Score: 0.661\n",
      "Epoch: 17/30.. Train Loss: 0.136.. Test Loss: 2.929.. Test Accuracy: 0.591.. Test F1 Score: 0.588\n",
      "Epoch: 18/30.. Train Loss: 0.097.. Test Loss: 2.808.. Test Accuracy: 0.669.. Test F1 Score: 0.671\n",
      "Epoch: 19/30.. Train Loss: 0.159.. Test Loss: 2.348.. Test Accuracy: 0.595.. Test F1 Score: 0.606\n",
      "Epoch: 20/30.. Train Loss: 0.117.. Test Loss: 3.169.. Test Accuracy: 0.626.. Test F1 Score: 0.632\n",
      "Epoch: 21/30.. Train Loss: 0.130.. Test Loss: 2.830.. Test Accuracy: 0.619.. Test F1 Score: 0.628\n",
      "Epoch: 22/30.. Train Loss: 0.080.. Test Loss: 2.815.. Test Accuracy: 0.644.. Test F1 Score: 0.652\n",
      "Epoch: 23/30.. Train Loss: 0.082.. Test Loss: 2.866.. Test Accuracy: 0.642.. Test F1 Score: 0.648\n",
      "Epoch: 24/30.. Train Loss: 0.090.. Test Loss: 2.779.. Test Accuracy: 0.656.. Test F1 Score: 0.649\n",
      "Epoch: 25/30.. Train Loss: 0.139.. Test Loss: 2.973.. Test Accuracy: 0.655.. Test F1 Score: 0.655\n",
      "Epoch: 26/30.. Train Loss: 0.182.. Test Loss: 2.347.. Test Accuracy: 0.623.. Test F1 Score: 0.632\n",
      "Epoch: 27/30.. Train Loss: 0.076.. Test Loss: 2.972.. Test Accuracy: 0.639.. Test F1 Score: 0.637\n",
      "Epoch: 28/30.. Train Loss: 0.077.. Test Loss: 3.157.. Test Accuracy: 0.650.. Test F1 Score: 0.655\n",
      "Epoch: 29/30.. Train Loss: 0.065.. Test Loss: 3.201.. Test Accuracy: 0.666.. Test F1 Score: 0.665\n",
      "Epoch: 30/30.. Train Loss: 0.051.. Test Loss: 2.964.. Test Accuracy: 0.674.. Test F1 Score: 0.678\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies, test_f1_scores = train_and_test(model, train_dataloader_75, test_dataloader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 0.981.. Test Loss: 1.078.. Test Accuracy: 0.632.. Test F1 Score: 0.642\n",
      "Epoch: 2/30.. Train Loss: 0.608.. Test Loss: 1.226.. Test Accuracy: 0.599.. Test F1 Score: 0.634\n",
      "Epoch: 3/30.. Train Loss: 0.485.. Test Loss: 1.133.. Test Accuracy: 0.661.. Test F1 Score: 0.669\n",
      "Epoch: 4/30.. Train Loss: 0.421.. Test Loss: 1.043.. Test Accuracy: 0.718.. Test F1 Score: 0.723\n",
      "Epoch: 5/30.. Train Loss: 0.361.. Test Loss: 1.227.. Test Accuracy: 0.718.. Test F1 Score: 0.718\n",
      "Epoch: 6/30.. Train Loss: 0.270.. Test Loss: 1.500.. Test Accuracy: 0.712.. Test F1 Score: 0.714\n",
      "Epoch: 7/30.. Train Loss: 0.251.. Test Loss: 1.743.. Test Accuracy: 0.644.. Test F1 Score: 0.660\n",
      "Epoch: 8/30.. Train Loss: 0.211.. Test Loss: 1.566.. Test Accuracy: 0.647.. Test F1 Score: 0.665\n",
      "Epoch: 9/30.. Train Loss: 0.185.. Test Loss: 1.822.. Test Accuracy: 0.659.. Test F1 Score: 0.664\n",
      "Epoch: 10/30.. Train Loss: 0.214.. Test Loss: 2.041.. Test Accuracy: 0.640.. Test F1 Score: 0.653\n",
      "Epoch: 11/30.. Train Loss: 0.184.. Test Loss: 1.775.. Test Accuracy: 0.675.. Test F1 Score: 0.684\n",
      "Epoch: 12/30.. Train Loss: 0.174.. Test Loss: 1.621.. Test Accuracy: 0.736.. Test F1 Score: 0.735\n",
      "Epoch: 13/30.. Train Loss: 0.151.. Test Loss: 1.763.. Test Accuracy: 0.663.. Test F1 Score: 0.670\n",
      "Epoch: 14/30.. Train Loss: 0.134.. Test Loss: 2.119.. Test Accuracy: 0.630.. Test F1 Score: 0.640\n",
      "Epoch: 15/30.. Train Loss: 0.121.. Test Loss: 2.747.. Test Accuracy: 0.620.. Test F1 Score: 0.639\n",
      "Epoch: 16/30.. Train Loss: 0.169.. Test Loss: 3.027.. Test Accuracy: 0.624.. Test F1 Score: 0.621\n",
      "Epoch: 17/30.. Train Loss: 0.248.. Test Loss: 2.298.. Test Accuracy: 0.626.. Test F1 Score: 0.645\n",
      "Epoch: 18/30.. Train Loss: 0.125.. Test Loss: 2.187.. Test Accuracy: 0.685.. Test F1 Score: 0.685\n",
      "Epoch: 19/30.. Train Loss: 0.098.. Test Loss: 2.452.. Test Accuracy: 0.632.. Test F1 Score: 0.629\n",
      "Epoch: 20/30.. Train Loss: 0.103.. Test Loss: 2.386.. Test Accuracy: 0.674.. Test F1 Score: 0.687\n",
      "Epoch: 21/30.. Train Loss: 0.100.. Test Loss: 2.595.. Test Accuracy: 0.596.. Test F1 Score: 0.610\n",
      "Epoch: 22/30.. Train Loss: 0.089.. Test Loss: 2.724.. Test Accuracy: 0.638.. Test F1 Score: 0.653\n",
      "Epoch: 23/30.. Train Loss: 0.080.. Test Loss: 2.798.. Test Accuracy: 0.597.. Test F1 Score: 0.608\n",
      "Epoch: 24/30.. Train Loss: 0.109.. Test Loss: 2.820.. Test Accuracy: 0.590.. Test F1 Score: 0.602\n",
      "Epoch: 25/30.. Train Loss: 0.120.. Test Loss: 2.480.. Test Accuracy: 0.635.. Test F1 Score: 0.645\n",
      "Epoch: 26/30.. Train Loss: 0.184.. Test Loss: 2.360.. Test Accuracy: 0.649.. Test F1 Score: 0.660\n",
      "Epoch: 27/30.. Train Loss: 0.105.. Test Loss: 2.209.. Test Accuracy: 0.643.. Test F1 Score: 0.651\n",
      "Epoch: 28/30.. Train Loss: 0.084.. Test Loss: 2.979.. Test Accuracy: 0.638.. Test F1 Score: 0.644\n",
      "Epoch: 29/30.. Train Loss: 0.096.. Test Loss: 2.754.. Test Accuracy: 0.629.. Test F1 Score: 0.643\n",
      "Epoch: 30/30.. Train Loss: 0.084.. Test Loss: 2.691.. Test Accuracy: 0.663.. Test F1 Score: 0.675\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies, test_f1_scores = train_and_test(model, train_dataloader_50, test_dataloader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25% Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 1.173.. Test Loss: 0.985.. Test Accuracy: 0.686.. Test F1 Score: 0.692\n",
      "Epoch: 2/30.. Train Loss: 0.727.. Test Loss: 1.049.. Test Accuracy: 0.718.. Test F1 Score: 0.713\n",
      "Epoch: 3/30.. Train Loss: 0.588.. Test Loss: 0.997.. Test Accuracy: 0.672.. Test F1 Score: 0.652\n",
      "Epoch: 4/30.. Train Loss: 0.500.. Test Loss: 1.046.. Test Accuracy: 0.661.. Test F1 Score: 0.644\n",
      "Epoch: 5/30.. Train Loss: 0.468.. Test Loss: 1.267.. Test Accuracy: 0.638.. Test F1 Score: 0.653\n",
      "Epoch: 6/30.. Train Loss: 0.456.. Test Loss: 1.136.. Test Accuracy: 0.623.. Test F1 Score: 0.632\n",
      "Epoch: 7/30.. Train Loss: 0.368.. Test Loss: 1.392.. Test Accuracy: 0.640.. Test F1 Score: 0.660\n",
      "Epoch: 8/30.. Train Loss: 0.317.. Test Loss: 1.467.. Test Accuracy: 0.665.. Test F1 Score: 0.678\n",
      "Epoch: 9/30.. Train Loss: 0.328.. Test Loss: 1.890.. Test Accuracy: 0.564.. Test F1 Score: 0.552\n",
      "Epoch: 10/30.. Train Loss: 0.461.. Test Loss: 1.201.. Test Accuracy: 0.719.. Test F1 Score: 0.723\n",
      "Epoch: 11/30.. Train Loss: 0.291.. Test Loss: 1.460.. Test Accuracy: 0.660.. Test F1 Score: 0.666\n",
      "Epoch: 12/30.. Train Loss: 0.309.. Test Loss: 1.485.. Test Accuracy: 0.652.. Test F1 Score: 0.660\n",
      "Epoch: 13/30.. Train Loss: 0.269.. Test Loss: 1.728.. Test Accuracy: 0.683.. Test F1 Score: 0.682\n",
      "Epoch: 14/30.. Train Loss: 0.221.. Test Loss: 1.400.. Test Accuracy: 0.688.. Test F1 Score: 0.688\n",
      "Epoch: 15/30.. Train Loss: 0.181.. Test Loss: 1.511.. Test Accuracy: 0.715.. Test F1 Score: 0.713\n",
      "Epoch: 16/30.. Train Loss: 0.161.. Test Loss: 1.747.. Test Accuracy: 0.642.. Test F1 Score: 0.645\n",
      "Epoch: 17/30.. Train Loss: 0.178.. Test Loss: 1.680.. Test Accuracy: 0.679.. Test F1 Score: 0.679\n",
      "Epoch: 18/30.. Train Loss: 0.196.. Test Loss: 1.994.. Test Accuracy: 0.656.. Test F1 Score: 0.663\n",
      "Epoch: 19/30.. Train Loss: 0.221.. Test Loss: 1.606.. Test Accuracy: 0.707.. Test F1 Score: 0.709\n",
      "Epoch: 20/30.. Train Loss: 0.199.. Test Loss: 1.880.. Test Accuracy: 0.672.. Test F1 Score: 0.680\n",
      "Epoch: 21/30.. Train Loss: 0.227.. Test Loss: 1.750.. Test Accuracy: 0.672.. Test F1 Score: 0.682\n",
      "Epoch: 22/30.. Train Loss: 0.157.. Test Loss: 2.060.. Test Accuracy: 0.619.. Test F1 Score: 0.630\n",
      "Epoch: 23/30.. Train Loss: 0.155.. Test Loss: 1.706.. Test Accuracy: 0.711.. Test F1 Score: 0.706\n",
      "Epoch: 24/30.. Train Loss: 0.161.. Test Loss: 2.050.. Test Accuracy: 0.655.. Test F1 Score: 0.651\n",
      "Epoch: 25/30.. Train Loss: 0.124.. Test Loss: 2.062.. Test Accuracy: 0.682.. Test F1 Score: 0.685\n",
      "Epoch: 26/30.. Train Loss: 0.113.. Test Loss: 1.934.. Test Accuracy: 0.716.. Test F1 Score: 0.717\n",
      "Epoch: 27/30.. Train Loss: 0.128.. Test Loss: 2.346.. Test Accuracy: 0.639.. Test F1 Score: 0.645\n",
      "Epoch: 28/30.. Train Loss: 0.104.. Test Loss: 2.193.. Test Accuracy: 0.612.. Test F1 Score: 0.613\n",
      "Epoch: 29/30.. Train Loss: 0.118.. Test Loss: 2.376.. Test Accuracy: 0.632.. Test F1 Score: 0.633\n",
      "Epoch: 30/30.. Train Loss: 0.123.. Test Loss: 2.737.. Test Accuracy: 0.668.. Test F1 Score: 0.677\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies, test_f1_scores = train_and_test(model, train_dataloader_25, test_dataloader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 1.656.. Test Loss: 1.417.. Test Accuracy: 0.450.. Test F1 Score: 0.455\n",
      "Epoch: 2/30.. Train Loss: 0.908.. Test Loss: 1.216.. Test Accuracy: 0.551.. Test F1 Score: 0.548\n",
      "Epoch: 3/30.. Train Loss: 0.706.. Test Loss: 1.172.. Test Accuracy: 0.643.. Test F1 Score: 0.631\n",
      "Epoch: 4/30.. Train Loss: 0.572.. Test Loss: 1.328.. Test Accuracy: 0.618.. Test F1 Score: 0.627\n",
      "Epoch: 5/30.. Train Loss: 0.478.. Test Loss: 1.467.. Test Accuracy: 0.625.. Test F1 Score: 0.638\n",
      "Epoch: 6/30.. Train Loss: 0.417.. Test Loss: 1.507.. Test Accuracy: 0.626.. Test F1 Score: 0.635\n",
      "Epoch: 7/30.. Train Loss: 0.437.. Test Loss: 1.384.. Test Accuracy: 0.604.. Test F1 Score: 0.613\n",
      "Epoch: 8/30.. Train Loss: 0.389.. Test Loss: 1.651.. Test Accuracy: 0.601.. Test F1 Score: 0.609\n",
      "Epoch: 9/30.. Train Loss: 0.335.. Test Loss: 2.084.. Test Accuracy: 0.592.. Test F1 Score: 0.607\n",
      "Epoch: 10/30.. Train Loss: 0.294.. Test Loss: 1.911.. Test Accuracy: 0.581.. Test F1 Score: 0.597\n",
      "Epoch: 11/30.. Train Loss: 0.312.. Test Loss: 1.432.. Test Accuracy: 0.670.. Test F1 Score: 0.672\n",
      "Epoch: 12/30.. Train Loss: 0.373.. Test Loss: 1.936.. Test Accuracy: 0.606.. Test F1 Score: 0.624\n",
      "Epoch: 13/30.. Train Loss: 0.259.. Test Loss: 1.890.. Test Accuracy: 0.636.. Test F1 Score: 0.645\n",
      "Epoch: 14/30.. Train Loss: 0.249.. Test Loss: 1.955.. Test Accuracy: 0.585.. Test F1 Score: 0.597\n",
      "Epoch: 15/30.. Train Loss: 0.205.. Test Loss: 2.582.. Test Accuracy: 0.620.. Test F1 Score: 0.615\n",
      "Epoch: 16/30.. Train Loss: 0.229.. Test Loss: 2.181.. Test Accuracy: 0.626.. Test F1 Score: 0.629\n",
      "Epoch: 17/30.. Train Loss: 0.206.. Test Loss: 2.392.. Test Accuracy: 0.600.. Test F1 Score: 0.613\n",
      "Epoch: 18/30.. Train Loss: 0.185.. Test Loss: 2.322.. Test Accuracy: 0.633.. Test F1 Score: 0.640\n",
      "Epoch: 19/30.. Train Loss: 0.200.. Test Loss: 2.390.. Test Accuracy: 0.633.. Test F1 Score: 0.643\n",
      "Epoch: 20/30.. Train Loss: 0.168.. Test Loss: 2.365.. Test Accuracy: 0.649.. Test F1 Score: 0.649\n",
      "Epoch: 21/30.. Train Loss: 0.143.. Test Loss: 2.649.. Test Accuracy: 0.647.. Test F1 Score: 0.651\n",
      "Epoch: 22/30.. Train Loss: 0.135.. Test Loss: 2.906.. Test Accuracy: 0.612.. Test F1 Score: 0.616\n",
      "Epoch: 23/30.. Train Loss: 0.134.. Test Loss: 2.805.. Test Accuracy: 0.619.. Test F1 Score: 0.618\n",
      "Epoch: 24/30.. Train Loss: 0.129.. Test Loss: 2.743.. Test Accuracy: 0.643.. Test F1 Score: 0.646\n",
      "Epoch: 25/30.. Train Loss: 0.122.. Test Loss: 2.781.. Test Accuracy: 0.639.. Test F1 Score: 0.645\n",
      "Epoch: 26/30.. Train Loss: 0.125.. Test Loss: 3.240.. Test Accuracy: 0.599.. Test F1 Score: 0.603\n",
      "Epoch: 27/30.. Train Loss: 0.126.. Test Loss: 3.031.. Test Accuracy: 0.644.. Test F1 Score: 0.652\n",
      "Epoch: 28/30.. Train Loss: 0.128.. Test Loss: 3.539.. Test Accuracy: 0.622.. Test F1 Score: 0.628\n",
      "Epoch: 29/30.. Train Loss: 0.283.. Test Loss: 2.705.. Test Accuracy: 0.617.. Test F1 Score: 0.618\n",
      "Epoch: 30/30.. Train Loss: 0.276.. Test Loss: 2.145.. Test Accuracy: 0.600.. Test F1 Score: 0.590\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureExtractor(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies, test_f1_scores = train_and_test(model, train_dataloader_10, test_dataloader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SupervisedTPN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(SupervisedTPN, self).__init__()\n",
    "#         self.trunk = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.AdaptiveMaxPool1d(output_size=1)\n",
    "#         )\n",
    "\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Linear(96, 1024),  # Adjusted to match the document's description\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, num_classes)  # Softmax applied externally during training\n",
    "#         )\n",
    "#         # No softmax here as it's included in nn.CrossEntropyLoss during training\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.trunk(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the output for the fully-connected layer\n",
    "#         output = self.head(x)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# model = SupervisedTPN(num_classes=10)  # Example for 10 classes\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=0.0001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for data, labels in train_loader:  # Assuming a single DataLoader for supervised learning\n",
    "#         optimizer.zero_grad()\n",
    "#         data = data.transpose(1, 2)  # Transpose to match input shape\n",
    "#         outputs = model(data)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
