{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306563</td>\n",
       "      <td>9.196875</td>\n",
       "      <td>-1.22625</td>\n",
       "      <td>null_class</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y    acc_z    activity activity_label_2\n",
       "0           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "1           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "2           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "3           0  0.306563  9.196875 -1.22625  null_class       null_class\n",
       "4           0  0.306563  9.196875 -1.22625  null_class       null_class"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data1 = pd.read_csv('./ISWC21_data_plus_raw/wetlab_data.csv')\n",
    "# add header\n",
    "data1.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity', 'activity_label_2']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163679, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove activity label 2 column\n",
    "data1 = data1.drop(['activity_label_2'], axis=1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.57434</td>\n",
       "      <td>-2.02733</td>\n",
       "      <td>1.34506</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.56479</td>\n",
       "      <td>-1.99597</td>\n",
       "      <td>1.39345</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.55122</td>\n",
       "      <td>-1.98445</td>\n",
       "      <td>1.41139</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.51335</td>\n",
       "      <td>-1.97557</td>\n",
       "      <td>1.42615</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.52959</td>\n",
       "      <td>-1.98187</td>\n",
       "      <td>1.45395</td>\n",
       "      <td>climbing_up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id    acc_x    acc_y    acc_z     activity\n",
       "0           0 -9.57434 -2.02733  1.34506  climbing_up\n",
       "1           0 -9.56479 -1.99597  1.39345  climbing_up\n",
       "2           0 -9.55122 -1.98445  1.41139  climbing_up\n",
       "3           0 -9.51335 -1.97557  1.42615  climbing_up\n",
       "4           0 -9.52959 -1.98187  1.45395  climbing_up"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data2 = pd.read_csv('./ISWC21_data_plus_raw/rwhar_data.csv', header=None)\n",
    "# add header\n",
    "data2.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200803, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447222</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>null_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     acc_x     acc_y     acc_z    activity\n",
       "0           0  0.443056  0.037500  0.888889  null_class\n",
       "1           0  0.440278  0.041667  0.880556  null_class\n",
       "2           0  0.451389  0.043056  0.876389  null_class\n",
       "3           0  0.456944  0.034722  0.888889  null_class\n",
       "4           0  0.447222  0.036111  0.888889  null_class"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data without header\n",
    "data3 = pd.read_csv('./ISWC21_data_plus_raw/sbhar_data.csv', header=None)\n",
    "# add header\n",
    "data3.columns = ['subject_id', 'acc_x', 'acc_y', 'acc_z', 'activity']\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122772, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data in one dataframe row-wise\n",
    "# data = pd.concat([data1, data2, data3], ignore_index=True, axis=0)\n",
    "data = pd.concat([data1, data3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286451, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "      <td>4.286451e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.156435e+01</td>\n",
       "      <td>-3.142137e+00</td>\n",
       "      <td>-1.027698e+00</td>\n",
       "      <td>2.850134e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.311629e+00</td>\n",
       "      <td>4.683064e+00</td>\n",
       "      <td>3.358477e+00</td>\n",
       "      <td>3.758104e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "      <td>-3.924000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>-7.050938e+00</td>\n",
       "      <td>-2.759063e+00</td>\n",
       "      <td>2.777778e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>-3.678750e+00</td>\n",
       "      <td>-2.972222e-01</td>\n",
       "      <td>2.759063e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>9.166667e-01</td>\n",
       "      <td>5.930556e-01</td>\n",
       "      <td>6.131250e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "      <td>3.310875e+01</td>\n",
       "      <td>3.893344e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id         acc_x         acc_y         acc_z\n",
       "count  4.286451e+06  4.286451e+06  4.286451e+06  4.286451e+06\n",
       "mean   1.156435e+01 -3.142137e+00 -1.027698e+00  2.850134e+00\n",
       "std    7.311629e+00  4.683064e+00  3.358477e+00  3.758104e+00\n",
       "min    0.000000e+00 -3.924000e+01 -3.924000e+01 -3.924000e+01\n",
       "25%    5.000000e+00 -7.050938e+00 -2.759063e+00  2.777778e-03\n",
       "50%    1.100000e+01 -3.678750e+00 -2.972222e-01  2.759063e+00\n",
       "75%    1.700000e+01  9.166667e-01  5.930556e-01  6.131250e+00\n",
       "max    2.900000e+01  3.893344e+01  3.310875e+01  3.893344e+01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id    0\n",
       "acc_x         0\n",
       "acc_y         0\n",
       "acc_z         0\n",
       "activity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286451, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sliding window\n",
    "\n",
    "def sliding_window_samples(data, samples_per_window, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Return a sliding window measured in number of samples over a data array.\n",
    "\n",
    "    :param data: input array, can be numpy or pandas dataframe\n",
    "    :param samples_per_window: window length as number of samples\n",
    "    :param overlap_ratio: overlap is meant as percentage and should be an integer value\n",
    "    :return: tuple of windows and indices\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    indices = []\n",
    "    curr = 0\n",
    "    win_len = int(samples_per_window)\n",
    "    if overlap_ratio is not None:\n",
    "        overlapping_elements = int((overlap_ratio / 100) * (win_len))\n",
    "        if overlapping_elements >= win_len:\n",
    "            print('Number of overlapping elements exceeds window size.')\n",
    "            return\n",
    "    while curr < len(data) - win_len:\n",
    "        windows.append(data[curr:curr + win_len])\n",
    "        indices.append([curr, curr + win_len])\n",
    "        curr = curr + win_len - overlapping_elements\n",
    "    try:\n",
    "        result_windows = np.array(windows)\n",
    "        result_indices = np.array(indices)\n",
    "    except:\n",
    "        result_windows = np.empty(shape=(len(windows), win_len, data.shape[1]), dtype=object)\n",
    "        result_indices = np.array(indices)\n",
    "        for i in range(0, len(windows)):\n",
    "            result_windows[i] = windows[i]\n",
    "            result_indices[i] = indices[i]\n",
    "    return result_windows, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of window dataset (2 sec with 0% overlap): (42864, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 50\n",
    "time_window = 2\n",
    "window_size = sampling_rate * time_window\n",
    "overlap_ratio = 0\n",
    "\n",
    "window_data, _ = sliding_window_samples(data, window_size, overlap_ratio)\n",
    "print(f\"shape of window dataset (2 sec with 0% overlap): {window_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class'],\n",
       "       [0, 0.3065625, 9.196875, -1.22625, 'null_class']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column\n",
    "window_data = window_data[:, :, :-1]\n",
    "# window_data = window_data[:, :, :-1]\n",
    "#remove the subject column\n",
    "window_data = window_data[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625],\n",
       "       [0.3065625, 9.196875, -1.22625]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_noise_single_window(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param noise_level: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy data.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_single_window(data, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly crop a single window of data and pad it to maintain original shape.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param crop_ratio: Ratio of the original window size to keep.\n",
    "    :return: Cropped and padded data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    new_size = int(window_size * crop_ratio)\n",
    "    start = np.random.randint(0, window_size - new_size)\n",
    "    end = start + new_size\n",
    "    cropped_data = data[start:end, :]\n",
    "\n",
    "    # Pad the cropped data to maintain original window size\n",
    "    padding_size = window_size - new_size\n",
    "    padding = np.zeros((padding_size, data.shape[1]))\n",
    "    padded_data = np.vstack((cropped_data, padding))\n",
    "\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of warping.\n",
    "    :return: Warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.copy(data)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        time_points = np.linspace(0, 1, window_size)\n",
    "        random_points = np.linspace(0, 1, np.random.randint(4, 10))\n",
    "        warp_values = 1 + np.random.normal(0, warp_factor, random_points.size)\n",
    "        interpolator = CubicSpline(random_points, warp_values)\n",
    "        warped_data[:, j] *= interpolator(time_points)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping_single_window(data, warp_factor=0.2):\n",
    "    \"\"\"\n",
    "    Apply time warping to a single window of data.\n",
    "\n",
    "    :param data: Input data with shape (window_size, 3).\n",
    "    :param warp_factor: Factor to determine the magnitude of time warping.\n",
    "    :return: Time-warped data.\n",
    "    \"\"\"\n",
    "    window_size = data.shape[0]\n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_points = np.linspace(0, 1, window_size)\n",
    "    random_points = np.sort(np.random.rand(np.random.randint(3, 6)))\n",
    "    warp_values = np.random.normal(1, warp_factor, random_points.size)\n",
    "    interpolator = CubicSpline(random_points, warp_values)\n",
    "    warped_time = interpolator(time_points)\n",
    "    warped_time -= warped_time.min()\n",
    "    warped_time /= warped_time.max()\n",
    "    warped_time *= (window_size - 1)\n",
    "\n",
    "    for j in range(3):  # for each axis\n",
    "        interpolator = CubicSpline(np.arange(window_size), data[:, j])\n",
    "        warped_data[:, j] = interpolator(warped_time)\n",
    "\n",
    "    return warped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "def amplitude_randomization_single_sample(windowed_sample, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply Amplitude Randomization to a single windowed sample.\n",
    "    \n",
    "    Parameters:\n",
    "    - windowed_sample: A numpy array of shape (400, 3) representing a single sample.\n",
    "    - alpha: A parameter controlling the extent of amplitude modulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of shape (400, 3) representing the augmented sample.\n",
    "    \"\"\"\n",
    "    # Initialize the output array with the same shape as the input sample\n",
    "    augmented_sample = np.zeros_like(windowed_sample)\n",
    "\n",
    "    # Iterate through each channel (x, y, z)\n",
    "    for channel in range(windowed_sample.shape[1]):\n",
    "        # Apply Fourier transform to the channel signal\n",
    "        fft_signal = fft(windowed_sample[:, channel])\n",
    "        \n",
    "        # Compute amplitude and phase\n",
    "        amplitude = np.abs(fft_signal)\n",
    "        phase = np.angle(fft_signal)\n",
    "        \n",
    "        # Generate random amplitude modulation\n",
    "        random_modulation = np.random.uniform(-alpha, alpha, size=amplitude.shape)\n",
    "        \n",
    "        # Modulate the amplitude\n",
    "        modulated_amplitude = (alpha + random_modulation) * amplitude\n",
    "        \n",
    "        # Combine modulated amplitude with the original phase\n",
    "        fft_augmented = modulated_amplitude * (np.cos(phase) + 1j * np.sin(phase))\n",
    "        \n",
    "        # Apply inverse Fourier transform to get the augmented time-series signal\n",
    "        augmented_signal = ifft(fft_augmented)\n",
    "        \n",
    "        # Assign the real part of the reconstructed signal to the output array\n",
    "        augmented_sample[:, channel] = augmented_signal.real\n",
    "\n",
    "    return augmented_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of noisy data: (100, 3)\n",
      "shape of cropped data: (100, 3)\n",
      "shape of warped data: (100, 3)\n",
      "shape of time warped data: (100, 3)\n",
      "shape of amplitude randomized data: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# add random noise\n",
    "noisy_data = add_random_noise_single_window(window_data[0], noise_level=0.1)\n",
    "print(f\"shape of noisy data: {noisy_data.shape}\")\n",
    "# random cropping\n",
    "cropped_data = random_cropping_single_window(window_data[0], crop_ratio=0.8)\n",
    "print(f\"shape of cropped data: {cropped_data.shape}\")\n",
    "# magnitude warping\n",
    "warped_data = magnitude_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of warped data: {warped_data.shape}\")\n",
    "# time warping\n",
    "time_warped_data = time_warping_single_window(window_data[0], warp_factor=0.2)\n",
    "print(f\"shape of time warped data: {time_warped_data.shape}\")\n",
    "\n",
    "# amplitude randomization\n",
    "amplitude_randomized_data = amplitude_randomization_single_sample(window_data[0])\n",
    "print(f\"shape of amplitude randomized data: {amplitude_randomized_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the original data\n",
    "augmented_data = np.copy(window_data)\n",
    "\n",
    "# create labels for augmented data\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "# loop over all windows\n",
    "for i in range(window_data.shape[0]):\n",
    "    # choose one number from 0 to 4\n",
    "    random_number = np.random.randint(0, 5)\n",
    "\n",
    "    if random_number == 0:\n",
    "        augmented_data[i] = add_random_noise_single_window(window_data[i], noise_level=0.1)\n",
    "        augmented_labels.append(0)\n",
    "    elif random_number == 1:\n",
    "        augmented_data[i] = random_cropping_single_window(window_data[i], crop_ratio=0.8)\n",
    "        augmented_labels.append(1)\n",
    "    elif random_number == 2:\n",
    "        augmented_data[i] = magnitude_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(2)\n",
    "    elif random_number == 3:\n",
    "        augmented_data[i] = time_warping_single_window(window_data[i], warp_factor=0.2)\n",
    "        augmented_labels.append(3)\n",
    "    else:\n",
    "        augmented_data[i] = amplitude_randomization_single_sample(window_data[i])\n",
    "        augmented_labels.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8531, 1: 8634, 2: 8584, 3: 8518, 4: 8597}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of augmented data: (42864, 100, 3)\n",
      "shape of augmented labels: 42864\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of augmented data: {augmented_data.shape}\")\n",
    "print(f\"shape of augmented labels: {len(augmented_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented data and labels\n",
    "# np.save('./data_processing/augmented_data_join.npy', augmented_data)\n",
    "# np.save('./data_processing/augmented_labels_join.npy', augmented_labels)\n",
    "\n",
    "# np.save('./data_processing/augmented_data_join_2.npy', augmented_data) # 2 datasets\n",
    "# np.save('./data_processing/augmented_labels_join_2.npy', augmented_labels) # 2 datasets\n",
    "\n",
    "# np.save('./data_processing/augmented_data_join_3.npy', augmented_data) # 2 datasets, 5 augmentations\n",
    "# np.save('./data_processing/augmented_labels_join_3.npy', augmented_labels) # 2 datasets, 5 augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12, 128)  # Adjust the input features according to your final conv layer output\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               stride=stride, padding=0, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        # Adjusting the length of the residual to match the output\n",
    "        if out.size(2) != res.size(2):\n",
    "            desired_length = out.size(2)\n",
    "            res = res[:, :, :desired_length]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout=0.2, num_classes=4):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size + (dilation_size - 1))]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)  # Global Average Pooling\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load augmented data and labels\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join.npy', allow_pickle=True)\n",
    "\n",
    "# augmented_data = np.load('./data_processing/augmented_data_join_2.npy', allow_pickle=True)\n",
    "# augmented_labels = np.load('./data_processing/augmented_labels_join_2.npy', allow_pickle=True)\n",
    "\n",
    "augmented_data = np.load('./data_processing/augmented_data_join_3.npy', allow_pickle=True)\n",
    "augmented_labels = np.load('./data_processing/augmented_labels_join_3.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864, 100, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42864,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8596, 1: 8584, 2: 8673, 3: 8625, 4: 8386}\n"
     ]
    }
   ],
   "source": [
    "#count distribution of labels\n",
    "unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of arrays to a single numpy array\n",
    "# if isinstance(augmented_data, list):\n",
    "#     augmented_data = np.stack(augmented_data)\n",
    "\n",
    "# # Ensure the data type is float32\n",
    "# augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "# augmented_labels_tensor = torch.from_numpy(np.array(augmented_labels)).long()\n",
    "\n",
    "# # split data into train and test set\n",
    "# train_size = int(0.8 * augmented_data.shape[0])\n",
    "# test_size = augmented_data.shape[0] - train_size\n",
    "# train_data, test_data = torch.utils.data.random_split(augmented_data, [train_size, test_size])\n",
    "# train_labels, test_labels = torch.utils.data.random_split(augmented_labels, [train_size, test_size])\n",
    "\n",
    "# # create train and test dataset\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # create train and test dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # check if GPU is available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming augmented_data and augmented_labels are numpy arrays\n",
    "augmented_data = augmented_data.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "augmented_data_tensor = torch.from_numpy(augmented_data)\n",
    "augmented_labels_tensor = torch.from_numpy(augmented_labels)\n",
    "#convert labels to long\n",
    "augmented_labels_tensor = augmented_labels_tensor.long()\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(0.8 * len(augmented_data_tensor))\n",
    "test_size = len(augmented_data_tensor) - train_size\n",
    "\n",
    "# Creating datasets\n",
    "dataset = TensorDataset(augmented_data_tensor, augmented_labels_tensor)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Function to extract tensors from Subset\n",
    "def extract_subset_data(subset, dataset):\n",
    "    return dataset.tensors[0][subset.indices], dataset.tensors[1][subset.indices]\n",
    "\n",
    "# Extract data and labels from train and test sets\n",
    "train_data, train_labels = extract_subset_data(train_dataset, dataset)\n",
    "test_data, test_labels = extract_subset_data(test_dataset, dataset)\n",
    "\n",
    "# create train and test TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# create DataLoader for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_loader: 268\n",
      "shape of test_loader: 67\n"
     ]
    }
   ],
   "source": [
    "#  print the shape of train_loader and test_loader\n",
    "print(f\"shape of train_loader: {len(train_loader)}\")\n",
    "print(f\"shape of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    #calculate accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.transpose(1, 2)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return running_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train and test model\n",
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}.. Train Loss: {train_loss:.3f}.. Test Loss: {test_loss:.3f}.. Test Accuracy: {test_accuracy:.3f}\")\n",
    "    return train_losses, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30.. Train Loss: 1.600.. Test Loss: 1.584.. Test Accuracy: 0.345\n",
      "Epoch: 2/30.. Train Loss: 1.568.. Test Loss: 1.545.. Test Accuracy: 0.366\n",
      "Epoch: 3/30.. Train Loss: 1.516.. Test Loss: 1.475.. Test Accuracy: 0.365\n",
      "Epoch: 4/30.. Train Loss: 1.432.. Test Loss: 1.378.. Test Accuracy: 0.365\n",
      "Epoch: 5/30.. Train Loss: 1.346.. Test Loss: 1.307.. Test Accuracy: 0.375\n",
      "Epoch: 6/30.. Train Loss: 1.295.. Test Loss: 1.271.. Test Accuracy: 0.405\n",
      "Epoch: 7/30.. Train Loss: 1.268.. Test Loss: 1.251.. Test Accuracy: 0.407\n",
      "Epoch: 8/30.. Train Loss: 1.251.. Test Loss: 1.237.. Test Accuracy: 0.413\n",
      "Epoch: 9/30.. Train Loss: 1.238.. Test Loss: 1.225.. Test Accuracy: 0.429\n",
      "Epoch: 10/30.. Train Loss: 1.229.. Test Loss: 1.217.. Test Accuracy: 0.452\n",
      "Epoch: 11/30.. Train Loss: 1.221.. Test Loss: 1.211.. Test Accuracy: 0.461\n",
      "Epoch: 12/30.. Train Loss: 1.213.. Test Loss: 1.203.. Test Accuracy: 0.460\n",
      "Epoch: 13/30.. Train Loss: 1.206.. Test Loss: 1.196.. Test Accuracy: 0.462\n",
      "Epoch: 14/30.. Train Loss: 1.199.. Test Loss: 1.188.. Test Accuracy: 0.482\n",
      "Epoch: 15/30.. Train Loss: 1.193.. Test Loss: 1.181.. Test Accuracy: 0.481\n",
      "Epoch: 16/30.. Train Loss: 1.186.. Test Loss: 1.176.. Test Accuracy: 0.472\n",
      "Epoch: 17/30.. Train Loss: 1.180.. Test Loss: 1.168.. Test Accuracy: 0.475\n",
      "Epoch: 18/30.. Train Loss: 1.173.. Test Loss: 1.160.. Test Accuracy: 0.493\n",
      "Epoch: 19/30.. Train Loss: 1.166.. Test Loss: 1.154.. Test Accuracy: 0.480\n",
      "Epoch: 20/30.. Train Loss: 1.159.. Test Loss: 1.147.. Test Accuracy: 0.495\n",
      "Epoch: 21/30.. Train Loss: 1.151.. Test Loss: 1.140.. Test Accuracy: 0.501\n",
      "Epoch: 22/30.. Train Loss: 1.144.. Test Loss: 1.137.. Test Accuracy: 0.481\n",
      "Epoch: 23/30.. Train Loss: 1.137.. Test Loss: 1.122.. Test Accuracy: 0.501\n",
      "Epoch: 24/30.. Train Loss: 1.130.. Test Loss: 1.119.. Test Accuracy: 0.491\n",
      "Epoch: 25/30.. Train Loss: 1.123.. Test Loss: 1.108.. Test Accuracy: 0.497\n",
      "Epoch: 26/30.. Train Loss: 1.116.. Test Loss: 1.102.. Test Accuracy: 0.487\n",
      "Epoch: 27/30.. Train Loss: 1.110.. Test Loss: 1.097.. Test Accuracy: 0.498\n",
      "Epoch: 28/30.. Train Loss: 1.105.. Test Loss: 1.089.. Test Accuracy: 0.510\n",
      "Epoch: 29/30.. Train Loss: 1.099.. Test Loss: 1.084.. Test Accuracy: 0.520\n",
      "Epoch: 30/30.. Train Loss: 1.094.. Test Loss: 1.082.. Test Accuracy: 0.517\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = CNNFeatureExtractor(num_classes=num_classes)\n",
    "\n",
    "# move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 30\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"cnn_feature_extractor_join_2_dataset.pt\"\n",
    "model_name = f\"cnn_feature_extractor_join_2_dataset_5_augmentations.pt\"\n",
    "torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Train Loss: 1.623.. Test Loss: 1.610.. Test Accuracy: 0.200\n",
      "Epoch: 2/100.. Train Loss: 1.616.. Test Loss: 1.607.. Test Accuracy: 0.212\n",
      "Epoch: 3/100.. Train Loss: 1.613.. Test Loss: 1.604.. Test Accuracy: 0.210\n",
      "Epoch: 4/100.. Train Loss: 1.609.. Test Loss: 1.604.. Test Accuracy: 0.219\n",
      "Epoch: 5/100.. Train Loss: 1.608.. Test Loss: 1.602.. Test Accuracy: 0.231\n",
      "Epoch: 6/100.. Train Loss: 1.609.. Test Loss: 1.602.. Test Accuracy: 0.221\n",
      "Epoch: 7/100.. Train Loss: 1.606.. Test Loss: 1.601.. Test Accuracy: 0.232\n",
      "Epoch: 8/100.. Train Loss: 1.605.. Test Loss: 1.600.. Test Accuracy: 0.229\n",
      "Epoch: 9/100.. Train Loss: 1.605.. Test Loss: 1.599.. Test Accuracy: 0.236\n",
      "Epoch: 10/100.. Train Loss: 1.603.. Test Loss: 1.599.. Test Accuracy: 0.228\n",
      "Epoch: 11/100.. Train Loss: 1.602.. Test Loss: 1.598.. Test Accuracy: 0.236\n",
      "Epoch: 12/100.. Train Loss: 1.601.. Test Loss: 1.598.. Test Accuracy: 0.236\n",
      "Epoch: 13/100.. Train Loss: 1.601.. Test Loss: 1.597.. Test Accuracy: 0.233\n",
      "Epoch: 14/100.. Train Loss: 1.600.. Test Loss: 1.597.. Test Accuracy: 0.232\n",
      "Epoch: 15/100.. Train Loss: 1.599.. Test Loss: 1.596.. Test Accuracy: 0.239\n",
      "Epoch: 16/100.. Train Loss: 1.599.. Test Loss: 1.596.. Test Accuracy: 0.235\n",
      "Epoch: 17/100.. Train Loss: 1.598.. Test Loss: 1.595.. Test Accuracy: 0.236\n",
      "Epoch: 18/100.. Train Loss: 1.598.. Test Loss: 1.595.. Test Accuracy: 0.237\n",
      "Epoch: 19/100.. Train Loss: 1.598.. Test Loss: 1.595.. Test Accuracy: 0.239\n",
      "Epoch: 20/100.. Train Loss: 1.596.. Test Loss: 1.594.. Test Accuracy: 0.236\n",
      "Epoch: 21/100.. Train Loss: 1.596.. Test Loss: 1.593.. Test Accuracy: 0.237\n",
      "Epoch: 22/100.. Train Loss: 1.595.. Test Loss: 1.593.. Test Accuracy: 0.240\n",
      "Epoch: 23/100.. Train Loss: 1.595.. Test Loss: 1.593.. Test Accuracy: 0.242\n",
      "Epoch: 24/100.. Train Loss: 1.595.. Test Loss: 1.592.. Test Accuracy: 0.242\n",
      "Epoch: 25/100.. Train Loss: 1.594.. Test Loss: 1.592.. Test Accuracy: 0.243\n",
      "Epoch: 26/100.. Train Loss: 1.594.. Test Loss: 1.591.. Test Accuracy: 0.246\n",
      "Epoch: 27/100.. Train Loss: 1.594.. Test Loss: 1.591.. Test Accuracy: 0.243\n",
      "Epoch: 28/100.. Train Loss: 1.592.. Test Loss: 1.591.. Test Accuracy: 0.248\n",
      "Epoch: 29/100.. Train Loss: 1.592.. Test Loss: 1.590.. Test Accuracy: 0.249\n",
      "Epoch: 30/100.. Train Loss: 1.592.. Test Loss: 1.589.. Test Accuracy: 0.252\n",
      "Epoch: 31/100.. Train Loss: 1.591.. Test Loss: 1.588.. Test Accuracy: 0.249\n",
      "Epoch: 32/100.. Train Loss: 1.590.. Test Loss: 1.589.. Test Accuracy: 0.250\n",
      "Epoch: 33/100.. Train Loss: 1.589.. Test Loss: 1.587.. Test Accuracy: 0.254\n",
      "Epoch: 34/100.. Train Loss: 1.589.. Test Loss: 1.587.. Test Accuracy: 0.260\n",
      "Epoch: 35/100.. Train Loss: 1.588.. Test Loss: 1.586.. Test Accuracy: 0.259\n",
      "Epoch: 36/100.. Train Loss: 1.587.. Test Loss: 1.585.. Test Accuracy: 0.254\n",
      "Epoch: 37/100.. Train Loss: 1.586.. Test Loss: 1.585.. Test Accuracy: 0.254\n",
      "Epoch: 38/100.. Train Loss: 1.585.. Test Loss: 1.583.. Test Accuracy: 0.268\n",
      "Epoch: 39/100.. Train Loss: 1.584.. Test Loss: 1.582.. Test Accuracy: 0.269\n",
      "Epoch: 40/100.. Train Loss: 1.583.. Test Loss: 1.582.. Test Accuracy: 0.274\n",
      "Epoch: 41/100.. Train Loss: 1.583.. Test Loss: 1.580.. Test Accuracy: 0.273\n",
      "Epoch: 42/100.. Train Loss: 1.580.. Test Loss: 1.579.. Test Accuracy: 0.282\n",
      "Epoch: 43/100.. Train Loss: 1.579.. Test Loss: 1.576.. Test Accuracy: 0.293\n",
      "Epoch: 44/100.. Train Loss: 1.577.. Test Loss: 1.574.. Test Accuracy: 0.286\n",
      "Epoch: 45/100.. Train Loss: 1.574.. Test Loss: 1.570.. Test Accuracy: 0.314\n",
      "Epoch: 46/100.. Train Loss: 1.569.. Test Loss: 1.565.. Test Accuracy: 0.325\n",
      "Epoch: 47/100.. Train Loss: 1.563.. Test Loss: 1.557.. Test Accuracy: 0.326\n",
      "Epoch: 48/100.. Train Loss: 1.554.. Test Loss: 1.543.. Test Accuracy: 0.352\n",
      "Epoch: 49/100.. Train Loss: 1.536.. Test Loss: 1.515.. Test Accuracy: 0.366\n",
      "Epoch: 50/100.. Train Loss: 1.500.. Test Loss: 1.460.. Test Accuracy: 0.369\n",
      "Epoch: 51/100.. Train Loss: 1.430.. Test Loss: 1.366.. Test Accuracy: 0.386\n",
      "Epoch: 52/100.. Train Loss: 1.349.. Test Loss: 1.300.. Test Accuracy: 0.408\n",
      "Epoch: 53/100.. Train Loss: 1.303.. Test Loss: 1.267.. Test Accuracy: 0.412\n",
      "Epoch: 54/100.. Train Loss: 1.279.. Test Loss: 1.245.. Test Accuracy: 0.416\n",
      "Epoch: 55/100.. Train Loss: 1.261.. Test Loss: 1.231.. Test Accuracy: 0.439\n",
      "Epoch: 56/100.. Train Loss: 1.245.. Test Loss: 1.219.. Test Accuracy: 0.457\n",
      "Epoch: 57/100.. Train Loss: 1.230.. Test Loss: 1.205.. Test Accuracy: 0.472\n",
      "Epoch: 58/100.. Train Loss: 1.218.. Test Loss: 1.192.. Test Accuracy: 0.481\n",
      "Epoch: 59/100.. Train Loss: 1.204.. Test Loss: 1.180.. Test Accuracy: 0.482\n",
      "Epoch: 60/100.. Train Loss: 1.192.. Test Loss: 1.167.. Test Accuracy: 0.493\n",
      "Epoch: 61/100.. Train Loss: 1.178.. Test Loss: 1.167.. Test Accuracy: 0.505\n",
      "Epoch: 62/100.. Train Loss: 1.165.. Test Loss: 1.143.. Test Accuracy: 0.511\n",
      "Epoch: 63/100.. Train Loss: 1.150.. Test Loss: 1.134.. Test Accuracy: 0.504\n",
      "Epoch: 64/100.. Train Loss: 1.135.. Test Loss: 1.112.. Test Accuracy: 0.501\n",
      "Epoch: 65/100.. Train Loss: 1.118.. Test Loss: 1.096.. Test Accuracy: 0.509\n",
      "Epoch: 66/100.. Train Loss: 1.102.. Test Loss: 1.082.. Test Accuracy: 0.518\n",
      "Epoch: 67/100.. Train Loss: 1.087.. Test Loss: 1.062.. Test Accuracy: 0.535\n",
      "Epoch: 68/100.. Train Loss: 1.074.. Test Loss: 1.047.. Test Accuracy: 0.534\n",
      "Epoch: 69/100.. Train Loss: 1.062.. Test Loss: 1.036.. Test Accuracy: 0.539\n",
      "Epoch: 70/100.. Train Loss: 1.051.. Test Loss: 1.032.. Test Accuracy: 0.548\n",
      "Epoch: 71/100.. Train Loss: 1.044.. Test Loss: 1.021.. Test Accuracy: 0.533\n",
      "Epoch: 72/100.. Train Loss: 1.037.. Test Loss: 1.038.. Test Accuracy: 0.504\n",
      "Epoch: 73/100.. Train Loss: 1.031.. Test Loss: 1.024.. Test Accuracy: 0.527\n",
      "Epoch: 74/100.. Train Loss: 1.031.. Test Loss: 1.014.. Test Accuracy: 0.522\n",
      "Epoch: 75/100.. Train Loss: 1.026.. Test Loss: 1.004.. Test Accuracy: 0.538\n",
      "Epoch: 76/100.. Train Loss: 1.022.. Test Loss: 1.015.. Test Accuracy: 0.551\n",
      "Epoch: 77/100.. Train Loss: 1.019.. Test Loss: 1.024.. Test Accuracy: 0.543\n",
      "Epoch: 78/100.. Train Loss: 1.017.. Test Loss: 0.998.. Test Accuracy: 0.550\n",
      "Epoch: 79/100.. Train Loss: 1.013.. Test Loss: 0.998.. Test Accuracy: 0.534\n",
      "Epoch: 80/100.. Train Loss: 1.012.. Test Loss: 0.999.. Test Accuracy: 0.540\n",
      "Epoch: 81/100.. Train Loss: 1.010.. Test Loss: 0.993.. Test Accuracy: 0.550\n",
      "Epoch: 82/100.. Train Loss: 1.006.. Test Loss: 0.987.. Test Accuracy: 0.546\n",
      "Epoch: 83/100.. Train Loss: 1.003.. Test Loss: 0.985.. Test Accuracy: 0.540\n",
      "Epoch: 84/100.. Train Loss: 1.001.. Test Loss: 0.989.. Test Accuracy: 0.536\n",
      "Epoch: 85/100.. Train Loss: 1.000.. Test Loss: 0.978.. Test Accuracy: 0.557\n",
      "Epoch: 86/100.. Train Loss: 0.997.. Test Loss: 0.985.. Test Accuracy: 0.551\n",
      "Epoch: 87/100.. Train Loss: 0.996.. Test Loss: 0.977.. Test Accuracy: 0.551\n",
      "Epoch: 88/100.. Train Loss: 0.991.. Test Loss: 0.998.. Test Accuracy: 0.531\n",
      "Epoch: 89/100.. Train Loss: 0.992.. Test Loss: 0.999.. Test Accuracy: 0.529\n",
      "Epoch: 90/100.. Train Loss: 0.988.. Test Loss: 0.971.. Test Accuracy: 0.567\n",
      "Epoch: 91/100.. Train Loss: 0.989.. Test Loss: 0.970.. Test Accuracy: 0.555\n",
      "Epoch: 92/100.. Train Loss: 0.988.. Test Loss: 0.976.. Test Accuracy: 0.551\n",
      "Epoch: 93/100.. Train Loss: 0.985.. Test Loss: 0.969.. Test Accuracy: 0.554\n",
      "Epoch: 94/100.. Train Loss: 0.983.. Test Loss: 0.987.. Test Accuracy: 0.567\n",
      "Epoch: 95/100.. Train Loss: 0.980.. Test Loss: 0.962.. Test Accuracy: 0.560\n",
      "Epoch: 96/100.. Train Loss: 0.978.. Test Loss: 0.959.. Test Accuracy: 0.576\n",
      "Epoch: 97/100.. Train Loss: 0.977.. Test Loss: 0.979.. Test Accuracy: 0.552\n",
      "Epoch: 98/100.. Train Loss: 0.977.. Test Loss: 0.955.. Test Accuracy: 0.565\n",
      "Epoch: 99/100.. Train Loss: 0.975.. Test Loss: 0.952.. Test Accuracy: 0.575\n",
      "Epoch: 100/100.. Train Loss: 0.972.. Test Loss: 0.955.. Test Accuracy: 0.562\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_inputs = 3  # Assuming 3 input channels (x, y, z axes of the accelerometer)\n",
    "num_channels = [64, 128, 256]  # Example channel sizes for each layer\n",
    "kernel_size = 8  # Kernel size for temporal convolutions\n",
    "\n",
    "model = TCN(num_inputs, num_channels, kernel_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.00003)  # Replace lr with your learning rate\n",
    "\n",
    "# train and test model\n",
    "num_epochs = 100\n",
    "train_losses, test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "# model_name = f\"tcn_join_2_dataset.pt\"\n",
    "model_name = f\"tcn_join_2_dataset_5_augmentations.pt\"\n",
    "# torch.save(model.state_dict(), f\"./models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
